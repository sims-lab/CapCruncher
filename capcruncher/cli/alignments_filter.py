import os
import pandas as pd
from loguru import logger
import ibis
import tempfile


from capcruncher.api.io import parse_bam
from capcruncher.api.filter import CCSliceFilter, TriCSliceFilter, TiledCSliceFilter

SLICE_FILTERS = {
    "capture": CCSliceFilter,
    "tri": TriCSliceFilter,
    "tiled": TiledCSliceFilter,
}


def merge_annotations(slices: os.PathLike, annotations: os.PathLike) -> pd.DataFrame:
    """
    Merges a parquet file containing slice information with a parquet file containing
    annotation information.

    Args:
        slices (os.PathLike): Path to parquet file containing slice information
        annotations (os.PathLike): Path to parquet file containing annotation information

    Returns:
        pd.DataFrame: Merged dataframe
    """

    con = ibis.duckdb.connect()
    tbl_annotations = con.register(f"parquet://{annotations}", table_name="annotations")
    column_replacements = {"Chromosome": "chrom", "Start": "start", "End": "end"}
    for old, new in column_replacements.items():
        if old in tbl_annotations.columns:
            tbl_annotations = tbl_annotations.relabel({old: new})

    tbl_slices = con.register(f"parquet://{slices}", table_name="slices")

    tbl = tbl_slices.join(
        tbl_annotations, how="inner", predicates=["slice_name", "chrom", "start"]
    ).distinct(on="slice_name")

    return tbl.execute(limit=None)


def filter(
    bam: os.PathLike,
    annotations: os.PathLike,
    custom_filtering: os.PathLike = None,
    output_prefix: os.PathLike = "reporters",
    stats_prefix: os.PathLike = "",
    method: str = "capture",
    sample_name: str = "",
    read_type: str = "",
    fragments: bool = True,
    read_stats: bool = True,
    slice_stats: bool = True,
    cis_and_trans_stats: bool = True,
):
    """
    Removes unwanted aligned slices and identifies reporters.

    Parses a BAM file and merges this with a supplied annotation to identify unwanted slices.
    Filtering can be tuned for Capture-C, Tri-C and Tiled-C data to ensure optimal filtering.

    Common filters include:

    - Removal of unmapped slices
    - Removal of excluded/blacklisted slices
    - Removal of non-capture fragments
    - Removal of multi-capture fragments
    - Removal of non-reporter fragments
    - Removal of fragments with duplicated coordinates.

    For specific filtering for each of the three methods see:

    - :class:`CCSliceFilter <capcruncher.tools.filter.CCSliceFilter>`
    - :class:`TriCSliceFilter <capcruncher.tools.filter.TriCSliceFilter>`
    - :class:`TiledCSliceFilter <capcruncher.tools.filter.TiledCSliceFilter>`


    In addition to outputting valid reporter fragments and slices separated by capture probe,
    this script also provides statistics on the number of read/slices filtered at each stage,
    and the number of cis/trans reporters for each probe.

    Notes:

     Whilst the script is capable of processing any annotations in tsv format, provided
     that the correct columns are present. It is highly recomended that the "annotate"
     subcomand is used to generate this file.

     Slice filtering is currently hard coded into each filtering class. This will be
     modified in a future update to enable custom filtering orders.


    \f
    Args:
     bam (os.PathLike): Input bam file to analyse
     annotations (os.PathLike): Annotations file generated by slices-annotate
     custom_filtering (os.PathLike): Allows for custom filtering to be performed. A yaml file is used to supply this ordering.
     output_prefix (os.PathLike, optional): Output file prefix. Defaults to "reporters".
     stats_prefix (os.PathLike, optional): Output stats prefix. Defaults to "".
     method (str, optional): Analysis method. Choose from (capture|tri|tiled). Defaults to "capture".
     sample_name (str, optional): Sample being processed e.g. DOX-treated_1. Defaults to "".
     read_type (str, optional): Process combined(flashed) or non-combined reads (pe) used for statistics. Defaults to "".
     gzip (bool, optional): Compress output with gzip. Defaults to False.
     fragments (bool, optional): Enables fragments to be output. Defaults to True.
     read_stats (bool, optional): Enables read level statistics to be output. Defaults to True.
     slice_stats (bool, optional): Enables slice level statistics to be output. Defaults to True.
     cis_and_trans_stats (bool, optional): Enables cis/trans statistics to be output. Defaults to True.
    """

    with logger.catch():
        with tempfile.NamedTemporaryFile(suffix=".parquet") as tmp:
            # Read bam file and merege annotations

            logger.info("Loading bam file")
            parse_bam(bam).to_parquet(tmp.name)

            logger.info("Merging bam file with annotations")
            df_alignment = merge_annotations(tmp.name, annotations)

            if "blacklist" not in df_alignment.columns:
                df_alignment["blacklist"] = 0

        # Initialise SliceFilter
        # If no custom filtering, will use the class default.
        slice_filter_class = SLICE_FILTERS[method]
        slice_filter = slice_filter_class(
            slices=df_alignment,
            sample_name=sample_name,
            read_type=read_type,
            filter_stages=custom_filtering,
        )

        # Filter slices using the slice_filter
        logger.info(f"Filtering slices with method: {method}")
        slice_filter.filter_slices()

        if slice_stats:
            slice_stats_path = f"{stats_prefix}.slice.stats.csv"
            logger.info(f"Writing slice statistics to {slice_stats_path}")
            slice_filter.filter_stats.to_csv(slice_stats_path, index=False)

        if read_stats:
            read_stats_path = f"{stats_prefix}.read.stats.csv"
            logger.info(f"Writing read statistics to {read_stats_path}")
            slice_filter.read_stats.to_csv(read_stats_path, index=False)

        # Save reporter stats
        if cis_and_trans_stats:
            logger.info("Writing reporter statistics")
            slice_filter.cis_or_trans_stats.to_csv(
                f"{stats_prefix}.reporter.stats.csv", index=False
            )

        # Output slices filtered by viewpoint

        df_slices = slice_filter.slices
        df_slices_with_viewpoint = slice_filter.slices_with_viewpoint
        df_capture = slice_filter.captures

        if fragments:
            logger.info("Writing reporters at the fragment level")
            df_fragments = (
                slice_filter_class(df_slices)
                .fragments.join(
                    df_capture["capture"], lsuffix="_slices", rsuffix="_capture"
                )
                .rename(
                    columns={
                        "capture_slices": "capture",
                        "capture_capture": "viewpoint",
                    }
                )
                .assign(id=lambda df: df["id"].astype("int64"))  # Enforce type
            )

            df_fragments.to_parquet(
                f"{output_prefix}.fragments.parquet",
                compression="snappy",
                engine="pyarrow",
            )

        logger.info("Writing reporters slices")

        # Enforce dtype for parent_id
        df_slices_with_viewpoint = df_slices_with_viewpoint.assign(
            parent_id=lambda df: df["parent_id"].astype("int64")
        ).drop_duplicates("slice_id")

        # Convert objects to category
        to_convert = df_slices_with_viewpoint.select_dtypes(include="object").columns
        df_slices_with_viewpoint[to_convert] = df_slices_with_viewpoint[
            to_convert
        ].astype("category")

        df_slices_with_viewpoint.to_parquet(
            f"{output_prefix}.slices.parquet",
            compression="snappy",
            engine="pyarrow",
        )

        logger.info("Completed analysis of BAM file")
