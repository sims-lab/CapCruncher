{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032664,
     "end_time": "2021-06-16T13:04:50.748978",
     "exception": false,
     "start_time": "2021-06-16T13:04:50.716314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run statistics\n",
    "\n",
    "This report provides statistics for all major pre-processing and filtering steps performed by the pipeline.\n",
    "\n",
    "All charts are interactive so hovering over areas of interest will provide additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asmith/Projects/CapCruncher/capcruncher/pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat 26 Jun 10:43:09 BST 2021\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.176967,
     "end_time": "2021-06-16T13:04:55.951524",
     "exception": false,
     "start_time": "2021-06-16T13:04:50.774557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038997,
     "end_time": "2021-06-16T13:04:56.022056",
     "exception": false,
     "start_time": "2021-06-16T13:04:55.983059",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "directory = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.039091,
     "end_time": "2021-06-16T13:04:56.085283",
     "exception": false,
     "start_time": "2021-06-16T13:04:56.046192",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "directory = \"/t1-data/project/milne_group/asmith/Projects/capture_pipeline_testing/ccanalyser_statistics/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031176,
     "end_time": "2021-06-16T13:04:56.132932",
     "exception": false,
     "start_time": "2021-06-16T13:04:56.101756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fastq duplication statistics\n",
    "\n",
    "Fastq files (after partitioning) are examined for fragments (R1 + R2) that appear to be PCR duplicates.\n",
    "\n",
    "Duplicates are identified by comparing the concatenated R1 and R2 sequences and filtering out exact matches. \n",
    "\n",
    "This is only the first pass of PCR duplicate removal as single base changes will be ignored. The aim here is to remove as many duplicate fragments as possible to reduce the amount of downstream processing required.\n",
    "\n",
    "Approximately 5-20% of fragments are typically removed by this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.057901,
     "end_time": "2021-06-16T13:04:56.220996",
     "exception": false,
     "start_time": "2021-06-16T13:04:56.163095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{directory}/deduplication/deduplication.summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.029059,
     "end_time": "2021-06-16T13:04:56.269259",
     "exception": false,
     "start_time": "2021-06-16T13:04:56.240200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of samples present, used for setting chart heights and widths.\n",
    "N_SAMPLES = df['sample'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 21.328223,
     "end_time": "2021-06-16T13:05:17.615245",
     "exception": false,
     "start_time": "2021-06-16T13:04:56.287022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame=df.query('stat_type != \"reads_total\"'),\n",
    "       x='stat',\n",
    "       y='sample',\n",
    "       color='stat_type',\n",
    "       template='simple_white',\n",
    "       category_orders={'sample': sorted(df['sample'].unique()),\n",
    "                        'stat_type': ('reads_unique', 'reads_removed')},\n",
    "       color_discrete_sequence=['#1f77b4', 'grey'])\n",
    "fig.for_each_trace(lambda t: t.update(name=' '.join(t.name.split('_'))))\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.update_yaxes(title='Sample')\n",
    "fig.update_xaxes(title='Number of Reads')\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.083076,
     "end_time": "2021-06-16T13:05:17.802669",
     "exception": false,
     "start_time": "2021-06-16T13:05:17.719593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trimming \n",
    "\n",
    "Following initial PCR duplicate removal fastq files are trimmed to remove sequencing adapters.\n",
    "\n",
    "These plots provide a brief summary of the number of adapters identified and removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.129353,
     "end_time": "2021-06-16T13:05:18.021623",
     "exception": false,
     "start_time": "2021-06-16T13:05:17.892270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/trimming/trimming.summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.133485,
     "end_time": "2021-06-16T13:05:18.233492",
     "exception": false,
     "start_time": "2021-06-16T13:05:18.100007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = df['sample'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.71175,
     "end_time": "2021-06-16T13:05:19.034630",
     "exception": false,
     "start_time": "2021-06-16T13:05:18.322880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = df.query('stat_type == \"adapters_removed\" or stat_type == \"reads_total\"').sort_values(['sample', 'read_number'])\n",
    "subplot_specs = specs = [[{'type': 'pie'} for i in range(2)] for j in range(N_SAMPLES)]\n",
    "fig = make_subplots(rows=N_SAMPLES,\n",
    "                    cols=2,\n",
    "                    specs=specs,\n",
    "                    row_titles=sorted(df_summary['sample'].str.replace('_', ' ').unique()),\n",
    "                    column_titles=['Read 1', 'Read 2'])\n",
    "\n",
    "for ii, (sample, df_sample) in enumerate(df_summary.groupby('sample')):\n",
    "    for jj in range(0,2):\n",
    "\n",
    "        df_read_number = df_sample.query(f'read_number == {jj+1}')\n",
    "        \n",
    "        fig.add_trace(go.Pie(labels=df_read_number['stat_type'].str.replace('_', ' ').str.title(), \n",
    "                             values=df_read_number['stat'],\n",
    "                             name=f'{sample} {read_number}',\n",
    "                             domain={'row':1, },),\n",
    "                      row=ii+1, \n",
    "                      col=jj+1)\n",
    "\n",
    "fig.update_layout(width=750, height=(250 * N_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.12295,
     "end_time": "2021-06-16T13:05:19.260300",
     "exception": false,
     "start_time": "2021-06-16T13:05:19.137350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read pair combination statistics (FLASh)\n",
    "\n",
    "After the removal of adapters read pairs are combined (if any overlap exists) using FLASh to generate combined fragments (refered to as flashed). Non-combined read pairs that do not have a sufficient overlap  (refered to as paired-end or pe) are maintained as read pairs in separate fastq files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.112802,
     "end_time": "2021-06-16T13:05:19.486260",
     "exception": false,
     "start_time": "2021-06-16T13:05:19.373458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/run_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.427253,
     "end_time": "2021-06-16T13:05:20.088619",
     "exception": false,
     "start_time": "2021-06-16T13:05:19.661366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = (df.loc[df['stage'].isin(['digestion'])]\n",
    "   .loc[lambda df: df['stat_type'] == 'unfiltered']\n",
    "   .assign(read_type=lambda df: df['read_type'].replace('flashed', 'Flashed').replace('pe', 'PE'))\n",
    "   .groupby(['sample', 'stage', 'stat_type', 'read_type'])\n",
    "   ['stat']\n",
    "   .mean()\n",
    "   .reset_index())\n",
    "\n",
    "fig = px.bar(data_frame=df_summary,\n",
    "             x='stat',\n",
    "             y='sample',\n",
    "             color='read_type',\n",
    "             template='simple_white',\n",
    "             category_orders={'sample': sorted(df['sample']), 'read_type': ['Flashed', 'PE']})\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.update_yaxes(title='Sample')\n",
    "fig.update_xaxes(title='Number of Read Pairs')\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102563,
     "end_time": "2021-06-16T13:05:20.308256",
     "exception": false,
     "start_time": "2021-06-16T13:05:20.205693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fastq *in silico* digestion statistics\n",
    "\n",
    "Following read pair combination, the combined or non-combined fragments are examined for recognition sites of the restriction enzyme used for the assay. A valid digesion of a fragment (above the minimum threshold set) results in one or more restriction fragments, refered to as slices.\n",
    "\n",
    "Flashed read pairs are treated differently from paired-end read pairs as we expect to observe the ligation junction in the flashed fragment. Therefore, if no recognition sites are identified, the fragment is marked as invalid and is discarded. Non-combined (paired-end) reads are unlikely to contain the ligation junction and therefore if no restriction sites are identified, the individual read pairs are not discarded.\n",
    "\n",
    "All identified slices must be longer than the minimum length specified (default 18 bp) to be considered valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.136354,
     "end_time": "2021-06-16T13:05:20.547578",
     "exception": false,
     "start_time": "2021-06-16T13:05:20.411224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hist = pd.read_csv(directory + '/digestion/digestion.histogram.csv')\n",
    "df_reads = pd.read_csv(directory + '/digestion/digestion.reads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.126282,
     "end_time": "2021-06-16T13:05:20.769465",
     "exception": false,
     "start_time": "2021-06-16T13:05:20.643183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The number of read pairs with at least one valid slice\n",
    "\n",
    "Unfiltered read pairs = The number of read pairs containing at least one restriction site\n",
    "\n",
    "Filtered read pairs = The number of read pairs containing at least one restriction site and at least one slices is above the minimum length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.511684,
     "end_time": "2021-06-16T13:05:21.383861",
     "exception": false,
     "start_time": "2021-06-16T13:05:20.872177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_reads.query(\"read_number != 2\").assign(\n",
    "    read_type=lambda df: df[\"read_type\"].replace(\"flashed\", \"Flashed\").replace(\"pe\", \"PE\"),\n",
    "    stat_type=lambda df: df['stat_type'].replace('unfiltered', 'All Read Pairs').replace('filtered', 'Reads with slices'),\n",
    "    sample=lambda df: df['sample'].str.replace('_', ' ')\n",
    "    \n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame=df,\n",
    "    x=\"stat\",\n",
    "    y=\"stat_type\",\n",
    "    color=\"read_type\",\n",
    "    facet_row=\"sample\",\n",
    "    template=\"simple_white\",\n",
    "    height=500 * N_SAMPLES,\n",
    "    width=750,\n",
    "    category_orders={\"sample\": sorted(df[\"sample\"]), \"read_type\": [\"Flashed\", \"PE\"]},\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"\",\n",
    "    margin={\"b\": 10},\n",
    ")\n",
    "fig.update_yaxes(title=\"\", autorange=\"reversed\")\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.layout[\"xaxis\"][\"title\"][\"text\"] = \"Number of Slices (Reads with RE sites)\"\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.14292,
     "end_time": "2021-06-16T13:05:21.631267",
     "exception": false,
     "start_time": "2021-06-16T13:05:21.488347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Histogram of digested slices\n",
    "\n",
    "This plot shows the number of valid slices identified per fragment, separated by flashed status. For the PE reads, an undigested read is considered valid therefore all PE reads with > 1 slice contain a recognition site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.866044,
     "end_time": "2021-06-16T13:05:22.593995",
     "exception": false,
     "start_time": "2021-06-16T13:05:21.727951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_hist.assign(\n",
    "        read_number=lambda df: df[\"read_number\"].map(\n",
    "            {0: \"Flashed\", 1: \"PE R1\", 2: \"PE R2\"}\n",
    "        )\n",
    "    ),\n",
    "    x=\"n_slices\",\n",
    "    y=\"n_reads\",\n",
    "    color=\"read_number\",\n",
    "    facet_row=\"sample\",\n",
    "    template=\"simple_white\",\n",
    "    barmode=\"group\",\n",
    "    height=500 * N_SAMPLES,\n",
    "    width=750,\n",
    "    hover_data=[\"n_reads\"],\n",
    "    category_orders={\"read_number\": [\"Flashed\", \"PE R1\", \"PE R2\"]},\n",
    ")\n",
    "\n",
    "fig.update_layout(legend_title_text=\"\")\n",
    "fig.update_yaxes(title=\"Frequency\", matches=None, showticklabels=True)\n",
    "fig.update_xaxes(dtick=1, showticklabels=True)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.110329,
     "end_time": "2021-06-16T13:05:22.809786",
     "exception": false,
     "start_time": "2021-06-16T13:05:22.699457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Alignment filtering statistics\n",
    "\n",
    "After alignment to the reference genome and annotation with capture probes, excluded regions and restriction fragments. Aligned slices are filtered and all fragments that do not contain one capture slice and one or more reporter slice(s) (i.e. slices that are not captured or appear in excluded regions) are removed.\n",
    "\n",
    "This chart shows the number of read pairs removed at each stage of the filtering, split by flashed/pe status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.138909,
     "end_time": "2021-06-16T13:05:23.053922",
     "exception": false,
     "start_time": "2021-06-16T13:05:22.915013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reads = pd.read_csv(directory + '/reporters/reporters.reads.csv')\n",
    "df_slices = pd.read_csv(directory + '/reporters/reporters.slices.csv')\n",
    "df_reporters = pd.read_csv(directory + '/reporters/reporters.reporters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.353055,
     "end_time": "2021-06-16T13:05:23.527152",
     "exception": false,
     "start_time": "2021-06-16T13:05:23.174097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reads = (\n",
    "    df_reads.sort_values(\"stat\", ascending=False)\n",
    "    .query('stat_type != \"not-deduplicated\"')\n",
    "    .replace(\"duplicate_filtered\", \"partial_duplicate_removal\")\n",
    "    .replace(\"deduplicated\", \"full_PCR_duplicate_removal\")\n",
    "    .assign(\n",
    "        stat_type=lambda df: df[\"stat_type\"].str.replace(\"_\", \" \").str.title().str.replace('Pcr', 'PCR'),\n",
    "        read_type=lambda df: df[\"read_type\"].replace(\"flashed\", \"Flashed\").replace(\"pe\", \"PE\"),\n",
    "        sample=lambda df: df['sample'].str.replace('_', ' '),\n",
    "\n",
    "    )\n",
    ")\n",
    "\n",
    "N_SAMPLES = len(df_reads['sample'].unique())\n",
    "\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame=df_reads.sort_values(\"stat\", ascending=False),\n",
    "    x=\"stat\",\n",
    "    y=\"stat_type\",\n",
    "    template=\"simple_white\",\n",
    "    color=\"read_type\",\n",
    "    facet_row=\"sample\",\n",
    "    category_orders={\n",
    "        \"stat_type\": df_reads[\"stat_type\"].unique(),\n",
    "        \"read_type\": [\"Flashed\", \"PE\"],\n",
    "        \"sample\": sorted(df_reads['sample'].unique())\n",
    "    },\n",
    "    height=(1000 * N_SAMPLES),\n",
    ")\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.update_yaxes(title=\"\")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.update_layout(legend_title_text=\"\")\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.117644,
     "end_time": "2021-06-16T13:05:23.763576",
     "exception": false,
     "start_time": "2021-06-16T13:05:23.645932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Capture and reporter statistics\n",
    "\n",
    "This chart shows the number of cis (same chromosome as capture) or trans (different chromosome to capture) reporters identified. This is separated by capture probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.47429,
     "end_time": "2021-06-16T13:05:24.352777",
     "exception": false,
     "start_time": "2021-06-16T13:05:23.878487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_probes = df_reporters[\"capture\"].nunique()\n",
    "fig = px.bar(\n",
    "    data_frame=df_reporters.groupby(['sample', 'capture', 'cis/trans']).agg({'count': 'sum'}).reset_index().assign(sample=lambda df: df['sample'].str.replace('_', ' ')),\n",
    "    x=\"count\",\n",
    "    y=\"capture\",\n",
    "    color=\"cis/trans\",\n",
    "    facet_row=\"sample\",\n",
    "    barmode=\"group\",\n",
    "    template=\"simple_white\",\n",
    "    category_orders={\n",
    "        \"cis/trans\": [\"trans\", \"cis\"],\n",
    "        \"capture\": sorted(df_reporters[\"capture\"].unique()),\n",
    "        \"sample\": sorted(df_reporters[\"sample\"].unique()),\n",
    "    },\n",
    "    height=250 + (N_SAMPLES * n_probes * 75),\n",
    "    width=1000,\n",
    "    labels={\"count\": \"Number of reporters\"},\n",
    ")\n",
    "fig.update_yaxes(title_text=\"\")\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.split(\"=\")[0]))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.update_layout(legend={\"traceorder\": \"reversed\", \"title\": \"\"})\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.118664,
     "end_time": "2021-06-16T13:05:24.612434",
     "exception": false,
     "start_time": "2021-06-16T13:05:24.493770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overall stats\n",
    "\n",
    "This chart displays the combined statistics from the entire pipeline run summarised at the read pair level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.156075,
     "end_time": "2021-06-16T13:05:24.892552",
     "exception": false,
     "start_time": "2021-06-16T13:05:24.736477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + 'run_statistics.csv').sort_values('stat', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.139145,
     "end_time": "2021-06-16T13:05:25.158543",
     "exception": false,
     "start_time": "2021-06-16T13:05:25.019398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stat_type_mapping = {'reads_total':  'Total Reads',\n",
    "                     'reads_unique': 'PCR Duplicate Filtered (1st pass)',\n",
    "                     'unfiltered':   'Passed Trimming and Combining',\n",
    "                     'filtered':     'Passed restriction site filter.',\n",
    "                     'mapped':       'Mapped to reference genome',\n",
    "                     'contains_single_capture': 'Contains a Capture Slice',\n",
    "                     'contains_capture_and_reporter': 'Contains a Capture and Reporter Slice',\n",
    "                     'duplicate_filtered': 'PCR Duplicate Filtered (2nd pass, partial)',\n",
    "                     'deduplicated': 'PCR Duplicate Filtered (final pass)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.411481,
     "end_time": "2021-06-16T13:05:25.707240",
     "exception": false,
     "start_time": "2021-06-16T13:05:25.295759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (df.assign(stat_type=lambda df: df['stat_type'].map(stat_type_mapping),\n",
    "               read_type=lambda df: df['read_type'].replace('flashed', 'Flashed').replace('pe', 'PE'),\n",
    "               sample=lambda df: df['sample'].str.replace('_', ' '))\n",
    "     )\n",
    "      \n",
    "fig = px.bar(df.query('(read_number != 2) and (stage == stage) '),\n",
    "             x='stat',\n",
    "             y='stat_type',\n",
    "             color='read_type',\n",
    "             template='simple_white',\n",
    "             facet_row='sample',\n",
    "             height=500 * N_SAMPLES,\n",
    "             width=1000,\n",
    "             category_orders={'stat_type': df['stat_type'].unique(), 'sample': sorted(df['sample'].unique()), 'read_type': ['Flashed', 'PE']})\n",
    "fig.update_yaxes(title_text='')\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.for_each_annotation(lambda a: a.update(text=f'{a.text.split(\"=\")[1]}'))\n",
    "fig.layout['xaxis']['title_text'] = 'Number of Read Pairs'\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "760e182274aae879a0c5666d4a8d6de3482593fa65a3c9ce2da0e707648cbd7b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('cc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "papermill": {
   "duration": 47.748806,
   "end_time": "2021-06-16T13:05:27.234520",
   "environment_variables": {},
   "exception": null,
   "input_path": "/t1-data/project/milne_group/asmith/Software/miniconda3/envs/cc/lib/python3.9/site-packages/ccanalyser/visualise_statistics.ipynb",
   "output_path": "ccanalyser_statistics/visualise_statistics.ipynb",
   "parameters": {
    "directory": "/t1-data/project/milne_group/asmith/Projects/capture_pipeline_testing/ccanalyser_statistics/"
   },
   "start_time": "2021-06-16T13:04:39.485714",
   "version": "2.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}