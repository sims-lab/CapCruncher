---
title: "CapCruncher Run Report"
author: "CapCruncher"
date: today
format:
  html:
    toc: true
    theme: cosmo
    embed-resources: true
execute:
  echo: false
jupyter: python3
---


```{python}

# | tags: [parameters]
fastq_deduplication_path = ""
fastq_trimming_path = ""
fastq_flash_path = ""
fastq_digestion_path = ""
reporter_filtering_path = ""
reporter_deduplication_path = ""
reporter_cis_trans_path = ""
run_stats_path = ""

```

```{python}
import os
import sys
import warnings

warnings.filterwarnings("ignore")

import pandas as pd
import pathlib
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
import json
import panel as pn


from capcruncher.api.statistics import (
    FastqDeduplicationStatistics,
    FastqTrimmingStatistics,
    FlashStats,
    DigestionStats,
    SliceFilterStatsList,
    AlignmentDeduplicationStats,
    CisOrTransStats,
)


pn.extension('tabulator')
pn.extension('plotly')


```

```{python}
def load_json(p):
    with open(p) as f:
        return json.load(f)


```


# FASTQ PCR Duplicate Removal:

Fastq files (after partitioning) are examined for fragments (R1 + R2) that appear to be PCR duplicates.
Duplicates are identified by comparing the concatenated R1 and R2 sequences and filtering out exact matches.

This is only the first pass of PCR duplicate removal as single base changes will be ignored. The aim here is to remove as many duplicate fragments as possible to reduce the amount of downstream processing required.

Approximately 5-20% of fragments are typically removed by this step.

```{python}

dd_jsons = [p for p in pathlib.Path(fastq_deduplication_path).glob("*.json")]

stats = [FastqDeduplicationStatistics(**load_json(p)) for p in dd_jsons]

df_dedup_stats = (
    pd.DataFrame([s.model_dump() for s in stats]).groupby("sample").sum().reset_index()
)

tabulator_formatters = {
    'Percentage Duplicated': {'type': 'progress', 'max': 100, "legend": True},
}

tabulator_filters = {
    'Sample Name': {'type': 'input', 'func': 'like', 'placeholder': 'Enter Sample'},
}


table = pn.widgets.Tabulator(
    df_dedup_stats[["sample", "total", "unique", "duplicates", "percentage"]]
    .assign(percentage=lambda df: df["percentage"].astype(float).round(2))
    .rename(
        columns={
            "sample": "Sample Name",
            "total": "Total Reads",
            "unique": "Unique Reads",
            "duplicates": "Duplicate Reads",
            "percentage": "Percentage Duplicated",
        }
    ),
    formatters=tabulator_formatters,
    header_filters=tabulator_filters,
    theme='midnight',
)

df = df_dedup_stats[["sample", "duplicates", "unique"]].melt(
    id_vars="sample", var_name="read_type", value_name="count"
)
fig = px.bar(
    df,
    x="count",
    y="sample",
    color="read_type",
    template="plotly_white",
    category_orders={
        "sample": sorted(df["sample"].unique()),
        "read_type": ["unique", "duplicates"],
    },
    color_discrete_sequence=["#F9A65A", "grey"],
)

fig.update_layout(legend_title_text="")
fig.update_yaxes(title="")
fig.update_xaxes(title="Number of Reads")
fig.update_traces(marker_line_width=0)

table = table
bar_chart = pn.pane.Plotly(fig)


pn.Tabs(("Table", table), ("Bar Chart", bar_chart)).servable()

```

# Trimming:

Following initial PCR duplicate removal fastq files are trimmed to remove sequencing adapters.

```{python}

trimming_stats = [
    FastqTrimmingStatistics(**json.loads(entry))
    for entry in load_json(fastq_trimming_path)
]

df_trim = pd.DataFrame([s.model_dump() for s in trimming_stats])

tabulator_formatters = {
    'Percentage Trimmed': {'type': 'progress', 'max': 100, "legend": True},
    "Percentage Passing Quality Filter": {
        'type': 'progress',
        'max': 100,
        "legend": True,
    },
}

tabulator_filters = {
    'Sample': {'type': 'input', 'func': 'like', 'placeholder': 'Enter Sample'},
    "Read Number": {'type': 'number', 'placeholder': 'Enter Read Number'},
}


table = pn.widgets.Tabulator(
    df_trim.rename(columns=lambda col: col.replace("_", " ").title()).round(2),
    formatters=tabulator_formatters,
    theme='midnight',
    header_filters=tabulator_filters,
)

table

```

# Read pair combination statistics (FLASh):

After the removal of adapters read pairs are combined (if any overlap exists) using `FLASh` to generate combined fragments (refered to as `flashed`). Non-combined read pairs that do not have a sufficient overlap  (refered to as `paired-end` or `pe`) are maintained as read pairs in separate fastq files.

```{python}

flash_stats = [FlashStats(**json.loads(entry)) for entry in load_json(fastq_flash_path)]
df_flash_stats = pd.DataFrame([s.model_dump() for s in flash_stats])
df_flash_stats
tabulator_formatters = {
    'Percentage Combined': {'type': 'progress', 'max': 100, "legend": True},
}

tabulator_filters = {
    'Sample': {'type': 'input', 'func': 'like', 'placeholder': 'Enter Sample'},
}


table = pn.widgets.Tabulator(
    df_flash_stats.rename(columns=lambda col: col.replace("_", " ").title()).round(2),
    formatters=tabulator_formatters,
    theme='midnight',
    header_filters=tabulator_filters,
)

table

```

# Fastq *in silico* digestion statistics (read pair level):

Following read pair combination, the combined or non-combined fragments are examined for recognition sites of the restriction enzyme used for the assay. A valid digesion of a fragment (above the minimum threshold set) results in one or more restriction fragments, refered to as `slices`.

Flashed read pairs are treated differently from paired-end read pairs as we expect to observe the ligation junction in the flashed fragment. Therefore, if no recognition sites are identified, the fragment is marked as invalid and is discarded. Non-combined (paired-end) reads are unlikely to contain the ligation junction and therefore if no restriction sites are identified, the individual read pairs are not discarded.

## Digestion statistics summarised at the read pair level.

The number of valid and invalid fragments are displayed. `slices` are considered invalid for the following reasons:

* No restriction sites identified if the read pair is combined (`flashed`)
* The fragment is shorter than the minimum length specified (default 18 bp)

The histogram displays the number of `slices` identified per fragment, split by flashed/pe status and pre/post filtering.

```{python}


digestion_stats = [
    DigestionStats(**load_json(f))
    for f in pathlib.Path(fastq_digestion_path).glob("*.json")
]
df_digestion = pd.DataFrame([s.model_dump() for s in digestion_stats])
df_digestion
unfiltered = (
    pd.DataFrame([s['unfiltered'] for s in df_digestion['read_stats']])
    .fillna(0)
    .add_suffix("_unfiltered")
)
filtered = (
    pd.DataFrame([s['filtered'] for s in df_digestion['read_stats']])
    .fillna(0)
    .add_suffix("_filtered")
)
df_digestion_read = (
    pd.concat([df_digestion[["sample", "read_type"]], unfiltered, filtered], axis=1)
    .groupby(["sample", "read_type"])
    .sum()
    .reset_index()
    .melt(id_vars=["sample", "read_type"], var_name="read_stat", value_name="count")
    .assign(
        filtered=lambda df: df['read_stat'].str.split("_").str[1],
        read_number=lambda df: df['read_stat'].str.split("_").str[0],
    )
    .drop(columns=["read_stat"])
    .replace("filtered", "post-digestion")
    .replace("unfiltered", "pre-digestion")
)


df_digestion_read_tbl = (
    df_digestion_read.pivot(
        columns='filtered', index=["sample", "read_type", "read_number"], values="count"
    )[["pre-digestion", "post-digestion"]]
    .assign(
        percentage_digested=lambda df: (
            df["post-digestion"] / df["pre-digestion"] * 100
        ).round(2)
    )
    .fillna(0)
    .reset_index()
    .query("read_type == 'Pe' or (read_type == 'Flashed' and read_number == 'read1')")
)

tabulator_formatters = {
    'Percentage Digested': {'type': 'progress', 'max': 100, "legend": True},
}

tabulator_filters = {
    'Sample': {'type': 'input', 'func': 'like', 'placeholder': 'Enter Sample'},
    'Read Type': {'type': 'input', 'func': 'like', 'placeholder': 'Enter Read Type'},
    'Read Number': {
        'type': 'input',
        'func': 'like',
        'placeholder': 'Enter Read Number',
    },
}


table = pn.widgets.Tabulator(
    df_digestion_read_tbl.rename(
        columns=lambda col: col.title().replace("_", " ")
    ).round(2),
    formatters=tabulator_formatters,
    theme='midnight',
    header_filters=tabulator_filters,
)


fig = px.line(
    df_digestion_read,
    x="filtered",
    y="count",
    color="read_number",
    line_dash="read_type",
    animation_frame="sample",
    markers=True,
    range_y=(
        0,
        df_digestion_read["count"].max() + df_digestion_read["count"].max() * 0.1,
    ),
    template="plotly_white",
)

fig.update_xaxes(title="Quality Filter Status")
fig.update_yaxes(title="Number of Reads")
fig.update_layout(legend_title_text="")


try:
    fig["layout"]["updatemenus"] = None
    fig["layout"]["sliders"][0]["pad"] = {"r": 0, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

fig_line = pn.pane.Plotly(fig)


h = list()

for s in digestion_stats:
    hist = s.histograms.lengths.to_dataframe()
    hist['sample'] = s.sample
    hist['read_type'] = s.read_type
    h.append(hist)


df_hist_filtered = (
    pd.concat(h).groupby(["sample", "read_type", "read_number"]).sum().reset_index()
)

fig = px.histogram(
    df_hist_filtered.sort_values(["sample"]),
    x='slice_length',
    pattern_shape="read_number",
    facet_col="read_type",
    color='read_type',
    animation_frame='sample',
    range_x=(0, df_hist_filtered['slice_length'].max() + 1),
    barmode="group",
    template="plotly_white",
    color_discrete_sequence=["#599AD3", "#9E66AB"],
)
fig.update_xaxes(title="Length of <i>in silico</i> digested fragments")
fig.update_layout(legend_title_text="")
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))

try:
    fig["layout"]["updatemenus"] = None
    # fig["layout"]["updatemenus"][0].update(dict(y=1, pad={"b": 10, "t": 0}, x=0))
    fig["layout"]["sliders"][0]["pad"] = {"r": 10, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

fig_hist = pn.pane.Plotly(fig)

pn.Tabs(
    ("Table", table),
    ("Digestion Filtering Plot", fig_line),
    ("Slice Length Histogram", fig_hist),
).servable()
```

# Alignment filtering statistics:

After alignment to the reference genome and annotation with viewpoint probes, excluded regions and restriction fragments. Aligned `slices` are filtered and all fragments that do not contain one viewpoint slice and one or more reporter slice(s) (i.e. `slices` that are not viewpoint or appear in excluded regions) are removed.

This chart shows the number of read pairs removed at each stage of the filtering, split by `flashed`/`pe` status.

```{python}

filter_stats = [
    SliceFilterStatsList(**load_json(p))
    for p in pathlib.Path(reporter_filtering_path).glob("*.json")
]
df_filter_stats = pd.concat(
    [
        pd.Series(s.model_dump())
        for sample in filter_stats
        for s in sample.stats
    ],
    axis=1,
).T
stage_order_mapping = {
    v: k
    for k, v in df_filter_stats["stage"]
    .drop_duplicates()
    .reset_index(drop=True)
    .to_dict()
    .items()
}
df_filter_stats = (
    df_filter_stats.groupby(["sample", "stage", "read_type"])
    .sum()
    .reset_index()
    .assign(stage_order=lambda df: df["stage"].map(stage_order_mapping))
    .sort_values(["sample", "stage_order"])
)

# Need to append the final deduplication stats
aln_dedup_stats = [
    AlignmentDeduplicationStats(**load_json(p))
    for p in pathlib.Path(reporter_deduplication_path).glob("*.json")
]
df_align_dedup = pd.DataFrame(
    [s.model_dump() for s in aln_dedup_stats]
)
df_filter_stats = pd.concat(
    [
        df_filter_stats,
        df_align_dedup[
            ["sample", "read_type", "n_unique_reads", "n_unique_slices"]
        ]
        .rename(
            columns={
                "n_unique_reads": "n_fragments",
                "n_unique_slices": "n_slices",
            }
        )
        .assign(
            stage="final_duplicate_removal",
            stage_order=df_filter_stats["stage_order"].max() + 1,
        ),
    ],
    axis=0,
).sort_values(["sample", "stage_order"])

fig = px.line(
    df_filter_stats.assign(
        stage=lambda df: df["stage"].str.replace("_", " ").str.title()
    ),
    x="stage",
    y="n_fragments",
    color="read_type",
    animation_frame="sample",
    markers=True,
    range_y=(0, df_filter_stats["n_fragments"].max() + 1),
    template="plotly_white",
)
fig.update_xaxes(title="Filtering Stage")
fig.update_layout(legend_title_text="")
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))

try:
    fig["layout"]["updatemenus"] = None
    # fig["layout"]["updatemenus"][0].update(dict(y=1, pad={"b": 10, "t": 0}, x=0))
    fig["layout"]["sliders"][0]["pad"] = {"r": 10, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

fig_fragments = pn.pane.Plotly(fig)

fig = px.line(
    df_filter_stats.assign(
        stage=lambda df: df["stage"].str.replace("_", " ").str.title()
    ),
    x="stage",
    y="n_slices",
    color="read_type",
    animation_frame="sample",
    markers=True,
    range_y=(0, df_filter_stats["n_slices"].max() + 1),
    template="plotly_white",
)
fig.update_xaxes(title="Filtering Stage")
fig.update_layout(legend_title_text="")
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))

try:
    fig["layout"]["updatemenus"] = None
    # fig["layout"]["updatemenus"][0].update(dict(y=1, pad={"b": 10, "t": 0}, x=0))
    fig["layout"]["sliders"][0]["pad"] = {"r": 10, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

fig_slices = pn.pane.Plotly(fig)

pn.Tabs(("Fragments", fig_fragments), ("Slices", fig_slices)).servable()



```

# Cis/Trans statistics:

`slices` from the same read fragment as a viewpoint `slices` are termed "reporters", these are used to determine interations with the viewpoint restriction fragment.

This chart displays the number of `cis` (same chromosome as viewpoint) or `trans` (different chromosome to viewpoint) reporters identified, separated by viewpoint.

```{python}

cis_or_trans_stats = [CisOrTransStats(**load_json(p)) for p in pathlib.Path(reporter_cis_trans_path).glob("*.json")]

df = pd.DataFrame([s.model_dump() for stat in cis_or_trans_stats for s in stat.stats])

df_cis_or_trans = df.groupby(["sample", "read_type", "cis_or_trans", "viewpoint"]).sum().reset_index()


fig = px.bar(df_cis_or_trans, x="count", y="viewpoint", color="cis_or_trans", pattern_shape="read_type", animation_frame="sample", facet_row="cis_or_trans",
       template="plotly_white", color_discrete_sequence=["#599AD3", "#9E66AB"], range_x=(0, df_cis_or_trans["count"].max()))

fig.update_xaxes(title="")
fig.update_layout(legend_title_text="")
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))

try:
    fig["layout"]["updatemenus"] = None
    fig["layout"]["sliders"][0]["pad"] = {"r": 10, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

pn.pane.Plotly(fig)

```


# Pipeline run statistics:

This chart displays the combined statistics from the entire pipeline run summarised at the read pair level.

```{python}
raw = df_dedup_stats[['sample', 'total']].rename(columns={'total': 'n_reads'}).assign(stage="raw", stage_order=0)
fastq_dedup = df_dedup_stats[['sample', 'unique']].rename(columns={'unique': 'n_reads'}).assign(stage="fastq_deduplication", stage_order=1)
fastq_trim = df_trim[['sample', 'reads_output']].drop_duplicates().rename(columns={'reads_output': 'n_reads'}).assign(stage="fastq_trimming", stage_order=2)
fastq_digest = df_digestion_read.query("filtered == 'post-digestion' and read_number == 'read1'").groupby("sample")['count'].sum().reset_index().rename(columns={'count': 'n_reads'}).assign(stage="fastq_digestion", stage_order=3)
aln_filter = df_filter_stats.groupby(['sample', 'stage', 'stage_order'])['n_fragments'].sum().reset_index().rename(columns={'n_fragments': 'n_reads'}).assign(stage_order=lambda df: df['stage_order'] + 3)
df_stats = pd.concat([raw, fastq_dedup, fastq_trim, fastq_digest, aln_filter], axis=0).assign(stage=lambda df: df['stage'].str.replace('_', ' ').str.title()).sort_values(['sample', 'stage_order'])
fig = px.line(df_stats, x="stage", y="n_reads", animation_frame="sample", template="plotly_white", range_y=(0, df_stats['n_reads'].max() + 0.1 * df_stats['n_reads'].max()), markers=True)

fig.update_xaxes(title="")
fig.update_layout(legend_title_text="")
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))

try:
    fig["layout"]["updatemenus"] = None
    # fig["layout"]["updatemenus"][0].update(dict(y=1, pad={"b": 10, "t": 0}, x=0))
    fig["layout"]["sliders"][0]["pad"] = {"r": 10, "b": 5, "t": 10}
    fig["layout"]["sliders"][0]["x"] = 0
    fig["layout"]["sliders"][0]["len"] = 1
except (KeyError, IndexError):
    pass

pn.pane.Plotly(fig)

```
