{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [18]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run statistics\n",
    "\n",
    "This report provides statistics for all major pre-processing and filtering steps performed by the pipeline.\n",
    "\n",
    "All charts are interactive so hovering over areas of interest will provide additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.211494,
     "end_time": "2021-03-16T16:36:20.029937",
     "exception": false,
     "start_time": "2021-03-16T16:36:18.818443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015022,
     "end_time": "2021-03-16T16:36:20.062354",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.047332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021384,
     "end_time": "2021-03-16T16:36:20.098892",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.077508",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#directory = os.path.join(os.getcwd(), 'statistics/')\n",
    "#directory = '/t1-data/user/asmith/Projects/capturec_dev/capturec_test_run_nc_Hex/run_statistics/'\n",
    "directory = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021173,
     "end_time": "2021-03-16T16:36:20.134874",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.113701",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "directory = \"/t1-data/user/asmith/Projects/capturec_dev/small_test/statistics/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014946,
     "end_time": "2021-03-16T16:36:20.165517",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.150571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fastq duplication statistics\n",
    "\n",
    "Fastq files (after partitioning) are examined for fragments (R1 + R2) that appear to be PCR duplicates.\n",
    "\n",
    "Duplicates are identified by comparing the concatenated R1 and R2 sequences and filtering out exact matches. \n",
    "\n",
    "This is only the first pass of PCR duplicate removal as single base changes will be ignored. The aim here is to remove as many duplicate fragments as possible to reduce the amount of downstream processing required.\n",
    "\n",
    "Approximately 5-20% of fragments are typically removed by this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.04786,
     "end_time": "2021-03-16T16:36:20.228721",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.180861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{directory}/deduplication/deduplication.summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023525,
     "end_time": "2021-03-16T16:36:20.268020",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.244495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of samples present, used for setting chart heights and widths.\n",
    "N_SAMPLES = df['sample'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.084513,
     "end_time": "2021-03-16T16:36:22.368735",
     "exception": false,
     "start_time": "2021-03-16T16:36:20.284222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame=df.query('stat_type != \"reads_total\"'),\n",
    "       x='stat',\n",
    "       y='sample',\n",
    "       color='stat_type',\n",
    "       template='simple_white',\n",
    "       category_orders={'sample': sorted(df['sample'].unique()),\n",
    "                        'stat': ['Read_pairs_unique', 'Read_pairs_removed']})\n",
    "fig.for_each_trace(lambda t: t.update(name=' '.join(t.name.split('_'))))\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.update_yaxes(title='Sample')\n",
    "fig.update_xaxes(title='Number of Reads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.083282,
     "end_time": "2021-03-16T16:36:22.538633",
     "exception": false,
     "start_time": "2021-03-16T16:36:22.455351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trimming \n",
    "\n",
    "Following initial PCR duplicate removal fastq files are trimmed to remove sequencing adapters.\n",
    "\n",
    "These plots provide a brief summary of the number of adapters identified and removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.089752,
     "end_time": "2021-03-16T16:36:22.708673",
     "exception": false,
     "start_time": "2021-03-16T16:36:22.618921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/trimming/trimming.summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.086132,
     "end_time": "2021-03-16T16:36:22.874781",
     "exception": false,
     "start_time": "2021-03-16T16:36:22.788649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = df['sample'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.168432,
     "end_time": "2021-03-16T16:36:23.127126",
     "exception": false,
     "start_time": "2021-03-16T16:36:22.958694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = df.query('stat_type == \"adapters_removed\" or stat_type == \"reads_total\"')\n",
    "subplot_specs = specs = [[{'type': 'pie'} for i in range(2)] for j in range(N_SAMPLES)]\n",
    "fig = make_subplots(rows=N_SAMPLES,\n",
    "                    cols=2,\n",
    "                    specs=specs,\n",
    "                    horizontal_spacing=0.2,\n",
    "                    vertical_spacing=0.1,\n",
    "                    row_titles=sorted(df_summary['sample'].str.replace('_', ' ').to_list()),\n",
    "                    column_titles=['Read 1', 'Read 2'])\n",
    "\n",
    "for ii, (sample, df_sample) in enumerate(df_summary.groupby('sample')):\n",
    "    for jj, (read_number, df_read_number) in enumerate(df_sample.groupby('read_number')):\n",
    "        \n",
    "        fig.add_trace(go.Pie(labels=df_read_number['stat_type'], \n",
    "                             values=df_read_number['stat'],\n",
    "                             name=f'{sample} {read_number}',\n",
    "                             domain={'row':1, },),\n",
    "                      row=ii+1, \n",
    "                      col=jj+1)\n",
    "\n",
    "fig.update_layout(width=500, height=(500 * N_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.193055,
     "end_time": "2021-03-16T16:36:23.678056",
     "exception": false,
     "start_time": "2021-03-16T16:36:23.485001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read pair combination statistics (FLASh)\n",
    "\n",
    "After the removal of adapters read pairs are combined (if any overlap exists) using FLASh to generate combined fragments (refered to as flashed). Non-combined read pairs that do not have a sufficient overlap  (refered to as paired-end or pe) are maintained as read pairs in separate fastq files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.098173,
     "end_time": "2021-03-16T16:36:23.860073",
     "exception": false,
     "start_time": "2021-03-16T16:36:23.761900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/run_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.147288,
     "end_time": "2021-03-16T16:36:24.091151",
     "exception": false,
     "start_time": "2021-03-16T16:36:23.943863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = (df.loc[df['stage'].isin(['digestion'])]\n",
    "   .loc[lambda df: df['stat_type'] == 'unfiltered']\n",
    "   .groupby(['sample', 'stage', 'stat_type', 'read_type'])\n",
    "   ['stat']\n",
    "   .mean()\n",
    "   .reset_index())\n",
    "\n",
    "fig = px.bar(data_frame=df_summary,\n",
    "             x='stat',\n",
    "             y='sample',\n",
    "             color='read_type',\n",
    "             template='simple_white',\n",
    "             category_orders={'sample': sorted(df['sample'])})\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.update_yaxes(title='Sample')\n",
    "fig.update_xaxes(title='Number of Read Pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.087764,
     "end_time": "2021-03-16T16:36:24.305465",
     "exception": false,
     "start_time": "2021-03-16T16:36:24.217701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fastq *in silico* digestion statistics\n",
    "\n",
    "Following read pair combination, the combined or non-combined fragments are examined for recognition sites of the restriction enzyme used for the assay. A valid digesion of a fragment (above the minimum threshold set) results in one or more restriction fragments, refered to as slices.\n",
    "\n",
    "Flashed read pairs are treated differently from paired-end read pairs as we expect to observe the ligation junction in the flashed fragment. Therefore, if no recognition sites are identified, the fragment is marked as invalid and is discarded. Non-combined (paired-end) reads are unlikely to contain the ligation junction and therefore if no restriction sites are identified, the individual read pairs are not discarded.\n",
    "\n",
    "All identified slices must be longer than the minimum length specified (default 18 bp) to be considered valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.102014,
     "end_time": "2021-03-16T16:36:24.495930",
     "exception": false,
     "start_time": "2021-03-16T16:36:24.393916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hist = pd.read_csv(directory + '/digestion/digestion.histogram.csv')\n",
    "df_reads = pd.read_csv(directory + '/digestion/digestion.reads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.114265,
     "end_time": "2021-03-16T16:36:24.699930",
     "exception": false,
     "start_time": "2021-03-16T16:36:24.585665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The number of read pairs with at least one valid slice\n",
    "\n",
    "Unfiltered read pairs = The number of read pairs containing at least one restriction site\\\n",
    "Filtered read pairs = The number of read pairs containing at least one restriction site and at least one slices is above the minimum length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.163286,
     "end_time": "2021-03-16T16:36:25.311507",
     "exception": false,
     "start_time": "2021-03-16T16:36:25.148221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame=df_reads.query('read_number != 2'),\n",
    "             x='stat',\n",
    "             y='stat_type',\n",
    "             color='read_type',\n",
    "             facet_row='sample',\n",
    "             template='simple_white',\n",
    "             height=500*N_SAMPLES,\n",
    "             width=750)\n",
    "fig.update_layout(legend_title_text='', \n",
    "                  margin={'b': 10},\n",
    "                  )\n",
    "fig.update_yaxes(title='', autorange='reversed')\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[1]))\n",
    "fig.layout['xaxis']['title']['text'] = 'Number of Slices (Reads with RE sites)'\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.099971,
     "end_time": "2021-03-16T16:36:25.510004",
     "exception": false,
     "start_time": "2021-03-16T16:36:25.410033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Histogram of digested slices\n",
    "\n",
    "This plot shows the number of valid slices identified per fragment, separated by flashed status. For the PE reads, an undigested read is considered valid therefore all PE reads with > 1 slice contain a recognition site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.217528,
     "end_time": "2021-03-16T16:36:26.028777",
     "exception": false,
     "start_time": "2021-03-16T16:36:25.811249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(data_frame=df_hist,\n",
    "             x='n_slices',\n",
    "             y='n_reads',\n",
    "             color='read_number',\n",
    "             facet_row='sample',\n",
    "             template='simple_white',\n",
    "             barmode='group',\n",
    "             height=500 * N_SAMPLES,\n",
    "             width=750,\n",
    "             hover_data=['n_reads'])\n",
    "\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.update_yaxes(title='Frequency', matches=None, showticklabels=True)\n",
    "fig.update_xaxes(dtick=1, showticklabels=True)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.117483,
     "end_time": "2021-03-16T16:36:26.372483",
     "exception": false,
     "start_time": "2021-03-16T16:36:26.255000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Alignment filtering statistics\n",
    "\n",
    "After alignment to the reference genome and annotation with capture probes, excluded regions and restriction fragments. Aligned slices are filtered and all fragments that do not contain one capture slice and one or more reporter slice(s) (i.e. slices that are not captured or appear in excluded regions) are removed.\n",
    "\n",
    "This chart shows the number of read pairs removed at each stage of the filtering, split by flashed/pe status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.151307,
     "end_time": "2021-03-16T16:36:27.621969",
     "exception": true,
     "start_time": "2021-03-16T16:36:26.470662",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reads = pd.read_csv(directory + '/reporters/reporters.reads.csv')\n",
    "df_slices = pd.read_csv(directory + '/reporters/reporters.slices.csv')\n",
    "df_reporters = pd.read_csv(directory + '/reporters/reporters.reporters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reads = (df_reads.sort_values('stat', ascending=False)\n",
    "                    .query('stat_type != \"not-deduplicated\"')\n",
    "                    .replace('duplicate_filtered', 'partial_PCR_duplicate_removal')\n",
    "                    .replace('deduplicated', 'full_PCR_duplicate_removal')\n",
    "                    .assign(stat_type=lambda df: df['stat_type'].str.replace('_', ' ').str.title()))\n",
    "\n",
    "px.bar(data_frame=df_reads.sort_values('stat', ascending=False),\n",
    "             x='stat',\n",
    "             y='stat_type',\n",
    "             template='simple_white',\n",
    "             color='read_type',\n",
    "             facet_row='sample', \n",
    "             category_orders={'stat_type': df_reads['stat_type'].unique()},\n",
    "             height= (50 * N_SAMPLES * df['stat_type'].nunique()),\n",
    "             width=750,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Capture and reporter statistics\n",
    "\n",
    "This chart shows the number of cis (same chromosome as capture) or trans (different chromosome to capture) reporters identified. This is separated by capture probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_probes = df_reporters['capture'].nunique()\n",
    "fig = px.bar(data_frame=df_reporters,\n",
    "             x='count',\n",
    "             y='capture',\n",
    "             color='cis/trans',\n",
    "             facet_row='sample',\n",
    "             barmode='group',\n",
    "             template='simple_white',\n",
    "             category_orders={'cis_or_trans': ['trans', 'cis'],\n",
    "                              'capture': sorted(df_reporters['capture'].unique()),\n",
    "                              'sample': sorted(df_reporters['sample'].unique())},\n",
    "             height= 250 + (N_SAMPLES * n_probes * 75),\n",
    "             width=1000,\n",
    "             labels={'count': 'Number of reporters'})\n",
    "fig.update_yaxes(title_text='')\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.split('=')[0]))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[1]))\n",
    "fig.update_layout(legend={'traceorder':'reversed', 'title': ''})\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Overall stats\n",
    "\n",
    "This chart displays the combined statistics from the entire pipeline run summarised at the read pair level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + 'run_statistics.csv').sort_values('stat', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stat_type_mapping = {'reads_total':  'Total Reads',\n",
    "                     'reads_unique': 'PCR Duplicate Filtered (1st pass)',\n",
    "                     'unfiltered':   'Passed Trimming and Combining',\n",
    "                     'filtered':     'Passed restriction site filter.',\n",
    "                     'mapped':       'Mapped to reference genome',\n",
    "                     'contains_single_capture': 'Contains a Capture Slice',\n",
    "                     'contains_capture_and_reporter': 'Contains a Capture and Reporter Slice',\n",
    "                     'duplicate_filtered': 'PCR Duplicate Filtered (2nd pass, partial)',\n",
    "                     'deduplicated': 'PCR Duplicate Filtered (final pass)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.assign(stat_type=lambda df: df['stat_type'].map(stat_type_mapping))\n",
    "fig = px.bar(df.query('read_number != 2'),\n",
    "             x='stat',\n",
    "             y='stat_type',\n",
    "             color='read_type',\n",
    "             template='simple_white',\n",
    "             facet_row='sample',\n",
    "             height=500 * N_SAMPLES,\n",
    "             width=1000,\n",
    "             category_orders={'stat_type': df['stat_type'].unique()})\n",
    "fig.update_yaxes(title_text='')\n",
    "fig.update_xaxes(matches=None, showticklabels=True)\n",
    "fig.update_layout(legend_title_text='')\n",
    "fig.for_each_annotation(lambda a: a.update(text=f'{a.text.split(\"=\")[1]}'))\n",
    "fig.layout['xaxis']['title_text'] = 'Number of Read Pairs'\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "papermill": {
   "duration": 12.639803,
   "end_time": "2021-03-16T16:36:28.165947",
   "environment_variables": {},
   "exception": true,
   "input_path": "/t1-data/user/asmith/Projects/capture-c/ccanalyser/visualise_capture-c_stats.ipynb",
   "output_path": "statistics/visualise_statistics.ipynb",
   "parameters": {
    "directory": "/t1-data/user/asmith/Projects/capturec_dev/small_test/statistics/"
   },
   "start_time": "2021-03-16T16:36:15.526144",
   "version": "2.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}