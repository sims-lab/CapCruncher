{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CapCruncher is a package explicitly designed for processing Capture-C, Tri-C and Tiled-C data. Unlike other pipelines that are designed to process Hi-C or Capture-HiC data, the filtering steps in CapCruncher are specifically optimised for these datasets.</p> <p>The package consists of a configurable data processing pipeline and a supporting command line interface to enable fine-grained control over the analysis.</p> <p>The pipeline is fast, robust and scales from a single workstation to a large HPC cluster. The pipeline is designed to be run on a HPC cluster and can be configured to use a variety of package management systems e.g. conda and singularity.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Warning</p> <p>CapCruncher is currently only availible for linux. MacOS support is planned for the future.</p> <p>CapCruncher is available on conda and PyPI. To install the latest version, run:</p> <p>It is highly recommended to install CapCruncher in a conda environment. If you do not have conda installed, please follow the instructions here to install mambaforge.</p> <pre><code>pip install capcruncher\n</code></pre> <p>or</p> <pre><code>mamba install -c bioconda capcruncher\n</code></pre> <p>See the installation guide for more detailed instructions.</p>"},{"location":"#usage","title":"Usage","text":"<p>CapCruncher commands are run using the <code>capcruncher</code> command. To see a list of available commands, run:</p> <pre><code>capcruncher --help\n</code></pre> <p>To see a list of available options for a command, run:</p> <pre><code>capcruncher &lt;command&gt; --help\n</code></pre> <p>See the usage guide for more detailed instructions.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<p>The CapCruncher pipeline handles the processing of raw data from the sequencer to the generation of a contact matrix, generation of plots and production of a UCSC genome browser track hub.</p> <p>See the pipeline guide for more detailed instructions including how to configure the pipeline to run on HPC clusters and using various package management systems e.g. conda and singularity.</p>"},{"location":"#pipeline-configuration","title":"Pipeline Configuration","text":"<p>The pipeline is configured using a YAML file. It is strongly recommended to use the <code>capcruncher pipeline-config</code> command to generate a template configuration file. This command will generate a template configuration file with all available options and descriptions of each option.</p> <pre><code>capcruncher pipeline-config --help\n</code></pre>"},{"location":"#running-the-pipeline","title":"Running the pipeline","text":"<p>The pipeline is run using the <code>capcruncher pipeline</code> command. Ensure that you have a configuration file and the fastq files to process are in the current working directory.</p> <pre><code>capcruncher pipeline --cores &lt;NUMBER OF CORES TO USE&gt;\n</code></pre>"},{"location":"cli/","title":"CLI Reference","text":"<p>This page provides documentation for CapCruncher command line tools.</p>"},{"location":"cli/#capcruncher","title":"capcruncher","text":"<p>An end to end solution for processing: Capture-C, Tri-C and Tiled-C data.</p> <p>Usage:</p> <pre><code>capcruncher [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --version  Show the version and exit.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"cli/#alignments","title":"alignments","text":"<p>Alignment annotation, identification and deduplication.</p> <p>Usage:</p> <pre><code>capcruncher alignments [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#annotate","title":"annotate","text":"<p>Annotates a bed file with other bed files using bedtools intersect.</p> <p>Whilst bedtools intersect allows for interval names and counts to be used for annotating intervals, this command provides the ability to annotate intervals with both interval names and counts at the same time. As the pipeline allows for empty bed files, this command has built in support to deal with blank/malformed bed files and will return default N/A values.</p> <p>Prior to interval annotation, the bed file to be intersected is validated and duplicate entries/multimapping reads are removed to ensure consistent annotations and prevent issues with reporter identification.</p> <p>Usage:</p> <pre><code>capcruncher alignments annotate [OPTIONS] SLICES\n</code></pre> <p>Options:</p> <pre><code>  -a, --actions [get|count]       Determines if the overlaps are counted or if\n                                  the name should just be reported\n  -b, --bed_files TEXT            Bed file(s) to intersect with slices\n  -n, --names TEXT                Names to use as column names for the output\n                                  tsv file.\n  -f, --overlap_fractions FLOAT   The minimum overlap required for an\n                                  intersection between two intervals to be\n                                  reported.\n  -t, --dtypes TEXT               Data type for column\n  -o, --output TEXT               Path for the annotated slices to be output.\n  --duplicates [remove]           Method to use for reconciling duplicate\n                                  slices (i.e. multimapping). Currently only\n                                  'remove' is supported.\n  -p, --n_cores INTEGER           Intersections are performed in parallel, set\n                                  this to the number of intersections required\n  --invalid_bed_action [ignore|error]\n                                  Method to deal with invalid bed files e.g.\n                                  blank or incorrectly formatted. Setting this\n                                  to 'ignore' will report default N/A values\n                                  (either '.' or 0) for invalid files\n  --blacklist TEXT                Regions to remove from the BAM file prior to\n                                  annotation\n  --prioritize-cis-slices         Attempts to prevent slices on the most\n                                  common chromosome in a fragment (ideally cis\n                                  to the viewpoint) being removed by\n                                  deduplication\n  --priority-chroms TEXT          A comma separated list of chromosomes to\n                                  prioritize during deduplication\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#filter","title":"filter","text":"<p>Removes unwanted aligned slices and identifies reporters.</p> <p>Parses a BAM file and merges this with a supplied annotation to identify unwanted slices. Filtering can be tuned for Capture-C, Tri-C and Tiled-C data to ensure optimal filtering.</p> <p>Usage:</p> <pre><code>capcruncher alignments filter [OPTIONS] {capture|tri|tiled}\n</code></pre> <p>Options:</p> <pre><code>  -b, --bam TEXT                Bam file to process  [required]\n  -a, --annotations TEXT        Annotations for the bam file that must contain\n                                the required columns, see description.\n                                [required]\n  --custom-filtering TEXT       Custom filtering to be used. This must be\n                                supplied as a path to a yaml file.\n  -o, --output_prefix TEXT      Output prefix for deduplicated fastq file(s)\n  --statistics TEXT             Output path for stats file\n  --sample-name TEXT            Name of sample e.g. DOX_treated_1\n  --read-type [flashed|pe]      Type of read\n  --fragments / --no-fragments  Determines if read fragment aggregations are\n                                produced\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"cli/#fastq","title":"fastq","text":"<p>Fastq splitting, deduplication and digestion.</p> <p>Usage:</p> <pre><code>capcruncher fastq [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#deduplicate","title":"deduplicate","text":"<p>Identifies PCR duplicate fragments from Fastq files.</p> <p>PCR duplicates are very commonly present in Capture-C/Tri-C/Tiled-C data and must be removed for accurate analysis. These commands attempt to identify and remove duplicate reads/fragments from fastq file(s) to speed up downstream analysis.</p> <p>Usage:</p> <pre><code>capcruncher fastq deduplicate [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -1, --fastq1 TEXT         Read 1 FASTQ files  [required]\n  -2, --fastq2 TEXT         Read 2 FASTQ files  [required]\n  -o, --output-prefix TEXT  Output prefix for deduplicated FASTQ files\n  --sample-name TEXT        Name of sample e.g. DOX_treated_1\n  -s, --statistics TEXT     Statistics output file name\n  --shuffle                 Shuffle reads before deduplication\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"cli/#digest","title":"digest","text":"<p>Performs in silico digestion of one or a pair of fastq files.</p> <p>Usage:</p> <pre><code>capcruncher fastq digest [OPTIONS] FASTQS...\n</code></pre> <p>Options:</p> <pre><code>  -r, --restriction_enzyme TEXT   Restriction enzyme name or sequence to use\n                                  for in silico digestion.  [required]\n  -m, --mode [flashed|pe]         Digestion mode. Combined (Flashed) or non-\n                                  combined (PE) read pairs.  [required]\n  -o, --output_file TEXT\n  --minimum_slice_length INTEGER\n  --statistics TEXT               Output path for stats file\n  --sample-name TEXT              Name of sample e.g. DOX_treated_1. Required\n                                  for correct statistics.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#split","title":"split","text":"<p>Splits fastq file(s) into equal chunks of n reads.</p> <p>Usage:</p> <pre><code>capcruncher fastq split [OPTIONS] INPUT_FILES...\n</code></pre> <p>Options:</p> <pre><code>  -m, --method [python|unix]   Method to use for splitting\n  -o, --output_prefix TEXT     Output prefix for deduplicated fastq file(s)\n  --compression_level INTEGER  Level of compression for output files\n  -n, --n_reads INTEGER        Number of reads per fastq file\n  --gzip / --no-gzip           Determines if files are gziped or not\n  -p, --n_cores INTEGER\n  -s, --suffix TEXT            Suffix to add to output files (ignore\n                               {read_number}.fastq as this is added\n                               automatically)\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cli/#genome","title":"genome","text":"<p>Genome wide methods digestion.</p> <p>Usage:</p> <pre><code>capcruncher genome [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#digest_1","title":"digest","text":"<p>Performs in silico digestion of a genome in fasta format.</p> <p>Digests the supplied genome fasta file and generates a bed file containing the locations of all restriction fragments produced by the supplied restriction enzyme.</p> <p>A log file recording the number of restriction fragments for the suplied genome is also generated.</p> <p>Usage:</p> <pre><code>capcruncher genome digest [OPTIONS] INPUT_FASTA\n</code></pre> <p>Options:</p> <pre><code>  -r, --recognition_site TEXT  Recognition enzyme or sequence  [required]\n  -l, --logfile TEXT           Path for digestion log file\n  -o, --output_file TEXT       Output file path\n  --remove_cutsite BOOLEAN     Exclude the recognition sequence from the\n                               output\n  --sort                       Sorts the output bed file by chromosome and\n                               start coord.\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cli/#interactions","title":"interactions","text":"<p>Reporter counting, storing, comparison and pileups</p> <p>Usage:</p> <pre><code>capcruncher interactions [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#compare","title":"compare","text":"<p>Compare bedgraphs and CapCruncher cooler files.</p> <p>These commands allow for specific viewpoints to be extracted from CapCruncher HDF5 files and perform:</p> <pre><code>1. User defined groupby aggregations.\n\n2. Comparisons between conditions.\n\n3. Identification of differential interactions between conditions.\n</code></pre> <p>See subcommands for details.</p> <p>Usage:</p> <pre><code>capcruncher interactions compare [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#concat","title":"concat","text":"<p>Usage:</p> <pre><code>capcruncher interactions compare concat [OPTIONS] INFILES...\n</code></pre> <p>Options:</p> <pre><code>  -f, --format [auto|bedgraph|cooler]\n                                  Input file format\n  -o, --output TEXT               Output file name\n  -v, --viewpoint TEXT            Viewpoint to extract\n  -r, --resolution TEXT           Resolution to extract\n  --region TEXT                   Limit to specific coordinates in the format\n                                  chrom:start-end\n  --normalisation [raw|n_cis|region]\n                                  Method to use interaction normalisation\n  --normalisation-regions TEXT    Regions to use for interaction\n                                  normalisation. The --normalisation method\n                                  MUST be 'region'\n  --scale_factor INTEGER          Scale factor to use for bedgraph\n                                  normalisation\n  -p, --n_cores INTEGER           Number of cores to use for extracting\n                                  bedgraphs\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#differential","title":"differential","text":"<p>Perform differential testing on CapCruncher HDF5 files.</p> <p>This command performs differential testing on CapCruncher HDF5 files. It requires a design matrix and a contrast to test. The design matrix should be a tab separated file with the first column containing the sample names and the remaining columns containing the conditions. The contrast should specify the name of the column in the design matrix to test. The output is a tab separated bedgraph.</p> <p>Usage:</p> <pre><code>capcruncher interactions compare differential [OPTIONS] INTERACTION_FILES...\n</code></pre> <p>Options:</p> <pre><code>  -o, --output-prefix TEXT        Output file prefix\n  -v, --viewpoint TEXT            Viewpoint to extract  [required]\n  -d, --design-matrix TEXT        Design matrix file  [required]\n  -c, --contrast TEXT             Contrast to test\n  -r, --regions-of-interest TEXT  Regions of interest to test for differential\n                                  interactions\n  --viewpoint-distance INTEGER    Distance from viewpoint to test for\n                                  differential interactions\n  --threshold-count INTEGER       Minimum number of interactions to test for\n                                  differential interactions\n  --threshold-q FLOAT             Minimum q-value to test for differential\n                                  interactions\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#summarise","title":"summarise","text":"<p>Usage:</p> <pre><code>capcruncher interactions compare summarise [OPTIONS] INFILE\n</code></pre> <p>Options:</p> <pre><code>  -d, --design-matrix TEXT        Design matrix file, should be formatted as a\n                                  tab separated file with the first column\n                                  containing the sample names and the other\n                                  column containing the conditions.\n  -o, --output-prefix TEXT        Output file prefix\n  -f, --output-format [bedgraph|tsv]\n  -m, --summary-methods TEXT      Summary methods to use for aggregation. Can\n                                  be any method in numpy or scipy.stats\n  -n, --group-names TEXT          Group names for aggregation\n  -c, --group-columns TEXT        Column names/numbers (0 indexed, the first\n                                  column after the end coordinate counts as 0)\n                                  for aggregation.\n  --subtraction                   Perform subtration between aggregated groups\n  --suffix TEXT                   Add a suffix before the file extension\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#count","title":"count","text":"<p>Determines the number of captured restriction fragment interactions genome wide.</p> <p>Counts the number of interactions between each restriction fragment and all other restriction fragments in the fragment.</p> <p>The output is a cooler formatted HDF5 file containing a single group containing the interactions between restriction fragments.</p> <p>See <code>https://cooler.readthedocs.io/en/latest/</code> for further details.</p> <p>Usage:</p> <pre><code>capcruncher interactions count [OPTIONS] REPORTERS\n</code></pre> <p>Options:</p> <pre><code>  -o, --output TEXT            Name of output file\n  --remove_exclusions          Prevents analysis of fragments marked as\n                               proximity exclusions\n  --remove_capture             Prevents analysis of capture fragment\n                               interactions\n  --subsample FLOAT            Subsamples reporters before analysis of\n                               interactions\n  -f, --fragment-map TEXT      Path to digested genome bed file\n  -v, --viewpoint-path TEXT    Path to viewpoints file\n  -p, --n-cores INTEGER        Number of cores to use for counting.\n  --assay [capture|tri|tiled]\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cli/#counts-to-cooler","title":"counts-to-cooler","text":"<p>Stores restriction fragment interaction combinations at the restriction fragment level.</p> <p>Parses reporter restriction fragment interaction counts produced by \"capcruncher reporters count\" and gerates a cooler formatted group in an HDF5 File. See <code>https://cooler.readthedocs.io/en/latest/</code> for further details.</p> <p>Usage:</p> <pre><code>capcruncher interactions counts-to-cooler [OPTIONS] COUNTS\n</code></pre> <p>Options:</p> <pre><code>  -f, --fragment-map TEXT    Path to digested genome bed file  [required]\n  -v, --viewpoint-path TEXT  Path to viewpoints file  [required]\n  -n, --viewpoint-name TEXT  Name of viewpoint to store\n  -g, --genome TEXT          Name of genome\n  --suffix TEXT              Suffix to append after the capture name for the\n                             output file\n  -o, --output TEXT          Name of output file. (Cooler formatted hdf5 file)\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli/#deduplicate_1","title":"deduplicate","text":"<p>Identifies and removes duplicated aligned fragments.</p> <p>PCR duplicates are very commonly present in Capture-C/Tri-C/Tiled-C data and must be removed for accurate analysis. Unlike fastq deduplicate, this command removes fragments with identical genomic coordinates.</p> <p>Non-combined (pe) and combined (flashed) reads are treated slightly differently due to the increased confidence that the ligation junction has been captured for the flashed reads.</p> <p>Usage:</p> <pre><code>capcruncher interactions deduplicate [OPTIONS] SLICES\n</code></pre> <p>Options:</p> <pre><code>  -o, --output TEXT         Output prefix for directory of deduplicated slices\n  --statistics TEXT         Output prefix for stats file(s)\n  --sample-name TEXT        Name of sample e.g. DOX_treated_1\n  --read-type [flashed|pe]  Type of read\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"cli/#fragments-to-bins","title":"fragments-to-bins","text":"<p>Convert a cooler group containing restriction fragments to constant genomic windows</p> <p>Parses a cooler group and aggregates restriction fragment interaction counts into genomic bins of a specified size. If the normalise option is selected, columns containing normalised counts are added to the pixels table of the output</p> <p>Usage:</p> <pre><code>capcruncher interactions fragments-to-bins [OPTIONS] COOLER_PATH\n</code></pre> <p>Options:</p> <pre><code>  -b, --binsizes INTEGER       Binsizes to use for windowing\n  --normalise                  Enables normalisation of interaction counts\n                               during windowing\n  --overlap_fraction FLOAT     Minimum overlap between genomic bins and\n                               restriction fragments for overlap\n  -p, --n_cores INTEGER        Number of cores used for binning\n  --scale-factor INTEGER       Scaling factor used for normalisation\n  --conversion_tables TEXT     Pickle file containing pre-computed fragment -&gt;\n                               bin conversions.\n  -o, --output TEXT            Name of output file. (Cooler formatted hdf5\n                               file)\n  --assay [capture|tri|tiled]\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cli/#merge","title":"merge","text":"<p>Merges capcruncher HDF5 files together.</p> <p>Produces a unified cooler with both restriction fragment and genomic bins whilst reducing the storage space required by hard linking the \"bins\" tables to prevent duplication.</p> <p>Usage:</p> <pre><code>capcruncher interactions merge [OPTIONS] COOLERS...\n</code></pre> <p>Options:</p> <pre><code>  -o, --output TEXT  Output file name\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli/#pileup","title":"pileup","text":"<p>Extracts reporters from a capture experiment and generates a bedgraph file.</p> <p>Identifies reporters for a single probe (if a probe name is supplied) or all capture probes present in a capture experiment HDF5 file.</p> <p>The bedgraph generated can be normalised by the number of cis interactions for inter experiment comparisons and/or extract pilups binned into even genomic windows.</p> <p>Usage:</p> <pre><code>capcruncher interactions pileup [OPTIONS] URI\n</code></pre> <p>Options:</p> <pre><code>  -n, --viewpoint_names TEXT      Viewpoint to extract and convert to\n                                  bedgraph, if not provided will transform\n                                  all.\n  -o, --output_prefix TEXT        Output prefix for bedgraphs\n  --normalisation [raw|n_cis|region]\n                                  Method to use interaction normalisation\n  --normalisation-regions TEXT    Regions to use for interaction\n                                  normalisation. The --normalisation method\n                                  MUST be 'region'\n  --binsize INTEGER               Binsize to use for converting bedgraph to\n                                  evenly sized genomic bins\n  --gzip                          Compress output using gzip\n  --scale-factor INTEGER          Scale factor to use for bedgraph\n                                  normalisation\n  --sparse / --dense              Produce bedgraph containing just positive\n                                  bins (sparse) or all bins (dense)\n  -f, --format [bedgraph|bigwig]  Output file format\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#pipeline","title":"pipeline","text":"<p>Runs the data processing pipeline</p> <p>Usage:</p> <pre><code>capcruncher pipeline [OPTIONS] [PIPELINE_OPTIONS]...\n</code></pre> <p>Options:</p> <pre><code>  -h, --help\n  --version\n  --logo / --no-logo  Show the capcruncher logo  [default: logo]\n  --version           Show the version and exit.\n</code></pre>"},{"location":"cli/#pipeline-config","title":"pipeline-config","text":"<p>Configures the data processing pipeline</p> <p>Usage:</p> <pre><code>capcruncher pipeline-config [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -h, --help\n  --version\n  --version          Show the version and exit.\n  -i, --input PATH\n  --generate-design\n</code></pre>"},{"location":"cli/#plot","title":"plot","text":"<p>Generates plots for the outputs produced by CapCruncher</p> <p>Usage:</p> <pre><code>capcruncher plot [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -r, --region TEXT    Genomic coordinates of the region to plot  [required]\n  -t, --template TEXT  TOML file containing the template for the plot\n                       [required]\n  -o, --output TEXT    Output file path. The file extension determines the\n                       output format.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli/#utilities","title":"utilities","text":"<p>Contains miscellaneous functions</p> <p>Usage:</p> <pre><code>capcruncher utilities [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#cis-and-trans-stats","title":"cis-and-trans-stats","text":"<p>Usage:</p> <pre><code>capcruncher utilities cis-and-trans-stats [OPTIONS] SLICES\n</code></pre> <p>Options:</p> <pre><code>  -o, --output TEXT            Output file name\n  --sample-name TEXT           Name of sample e.g. DOX_treated_1\n  --assay [capture|tri|tiled]  Assay used to generate slices\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cli/#dump","title":"dump","text":"<p>Dumps the contents of a cooler or capcruncher parquet file to a TSV file</p> <p>Args:     path (str): Path to cooler or capcruncher parquet file     viewpoint (str, optional): Viewpoint to extract. Defaults to None.     resolution (int, optional): Resolution to extract. Only used for cooler (hdf5) files. Defaults to None.     output (str, optional): Output file name. Defaults to \"capcruncher_dump.tsv\".</p> <p>Usage:</p> <pre><code>capcruncher utilities dump [OPTIONS] PATH\n</code></pre> <p>Options:</p> <pre><code>  -v, --viewpoint TEXT   Viewpoint to extract\n  -r, --resolution TEXT  Resolution to extract. Only used for cooler (hdf5)\n                         files\n  -o, --output TEXT      Output file name\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"cli/#gtf-to-bed12","title":"gtf-to-bed12","text":"<p>Converts a GTF file to a BED12 file containing only 5' UTRs, 3' UTRs, and exons.</p> <p>Args:     gtf (str): Path to the input GTF file.     output (str): Path to the output BED12 file.</p> <p>Returns:     None</p> <p>Usage:</p> <pre><code>capcruncher utilities gtf-to-bed12 [OPTIONS] GTF\n</code></pre> <p>Options:</p> <pre><code>  -o, --output TEXT  Output file name\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli/#make-chicago-maps","title":"make-chicago-maps","text":"<p>Restriction map file (.rmap) - a bed file containing coordinates of the restriction fragments. By default, 4 columns: chr, start, end, fragmentID. Bait map file (.baitmap) - a bed file containing coordinates of the baited restriction fragments, and their associated annotations. By default, 5 columns: chr, start, end, fragmentID, baitAnnotation. The regions specified in this file, including their fragmentIDs, must be an exact subset of those in the .rmap file. The baitAnnotation is a text field that is used only to annotate the output and plots.</p> <p>Usage:</p> <pre><code>capcruncher utilities make-chicago-maps [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --fragments TEXT      Path to fragments file (default: capcruncher_output/re\n                        sources/restriction_fragments/genome.digest.bed.gz)\n  --viewpoints TEXT     Path to viewpoints file used for capcruncher\n                        [required]\n  -o, --outputdir TEXT  Path to output directory  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/#regenerate-fastq","title":"regenerate-fastq","text":"<p>Regenerates a FASTQ file from a parquet file containing the required reads</p> <p>Args:     fastq1 (str): Path to the first FASTQ file     fastq2 (str): Path to the second FASTQ file     parquet_file (str, optional): Path to the parquet file from which to extract the required reads. Defaults to None.     output (str, optional): Prefix for the output file. Defaults to \"regenerated_\".</p> <p>Raises:     AssertionError: If the specified parquet file does not exist.</p> <p>Returns:     None</p> <p>Usage:</p> <pre><code>capcruncher utilities regenerate-fastq [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -1, --fastq1 TEXT         Path to FASTQ file 1  [required]\n  -2, --fastq2 TEXT         Path to FASTQ file 2  [required]\n  -p, --parquet-file TEXT   Path to parquet file from which to extract the\n                            required reads  [required]\n  -o, --output-prefix TEXT  Output file prefix\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"cli/#viewpoint-coordinates","title":"viewpoint-coordinates","text":"<p>Aligns viewpoints to a genome and returns the coordinates of the viewpoint in the genome.</p> <p>Viewpoints can be supplied as a FASTA file or a TSV file with the first column containing the name of the viewpoint and the second column containing the sequence of the viewpoint.</p> <p>Args:     viewpoints (os.PathLike): Path to viewpoints     genome (os.PathLike): Path to genome fasta file     genome_indicies (os.PathLike, optional): Path to genome bowtie2 indices. Defaults to None.     recognition_site (str, optional): Restriction site used. Defaults to \"dpnii\".     output (os.PathLike, optional): Output file name. Defaults to \"viewpoint_coordinates.bed\".</p> <p>Raises:     ValueError: If viewpoints are not supplied in the correct format     ValueError: If no bowtie2 indices are supplied</p> <p>Usage:</p> <pre><code>capcruncher utilities viewpoint-coordinates [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --viewpoints TEXT        Path to viewpoints  [required]\n  -g, --genome TEXT            Path to genome fasta file  [required]\n  -i, --genome-indicies TEXT   Path to genome bowtie2 indices  [required]\n  -r, --recognition-site TEXT  Restriction site used\n  -o, --output TEXT            Output file name\n  --help                       Show this message and exit.\n</code></pre>"},{"location":"cluster_config/","title":"Set-up a Snakemake profile","text":"<p>This is not essential but it will make running the pipeline much easier by submitting jobs to the cluster automatically and using pre-set parameters.</p> <p>Note: Cookiecutter is required for this step. This can be installed using <code>pip install cookiecutter</code>.</p>"},{"location":"cluster_config/#for-slurm-based-clusters","title":"For SLURM based clusters:","text":"<pre><code># create config directory that snakemake searches for profiles (or use something else)\nprofile_dir=\"${HOME}/.config/snakemake\"\nmkdir -p \"$profile_dir\"\n# use cookiecutter to create the profile in the config directory\ntemplate=\"gh:Snakemake-Profiles/slurm\"\ncookiecutter --output-dir \"$profile_dir\" \"$template\"\n</code></pre>"},{"location":"cluster_config/#for-sge-based-clusters","title":"For SGE based clusters:","text":"<p>Warning</p> <p>This has not been tested</p> <pre><code>mkdir -p ~/.config/snakemake\ncd ~/.config/snakemake\ncookiecutter https://github.com/Snakemake-Profiles/sge.git\n</code></pre>"},{"location":"cluster_config/#example-slurm-profile","title":"Example SLURM profile:","text":"<pre><code>/home/a/asmith/.config/snakemake/slurm/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 CookieCutter.py\n\u251c\u2500\u2500 __pycache__\n\u2502   \u251c\u2500\u2500 CookieCutter.cpython-310.pyc\n\u2502   \u251c\u2500\u2500 CookieCutter.cpython-311.pyc\n\u2502   \u251c\u2500\u2500 slurm_utils.cpython-310.pyc\n\u2502   \u2514\u2500\u2500 slurm_utils.cpython-311.pyc\n\u251c\u2500\u2500 settings.json\n\u251c\u2500\u2500 slurm-jobscript.sh\n\u251c\u2500\u2500 slurm-sidecar.py\n\u251c\u2500\u2500 slurm-status.py\n\u251c\u2500\u2500 slurm-submit.py\n\u2514\u2500\u2500 slurm_utils.py\n</code></pre> <p><code>settings.json</code>:</p> <pre><code>{\n    \"SBATCH_DEFAULTS\": \"--partition=short --time=0-01:00:00 --mem=3G\",\n    \"CLUSTER_NAME\": \"\",\n    \"CLUSTER_CONFIG\": \"\"\n}\n</code></pre> <p><code>config.yaml</code>:</p> <pre><code>cluster-sidecar: \"slurm-sidecar.py\"\ncluster-cancel: \"scancel\"\nrestart-times: \"0\"\njobscript: \"slurm-jobscript.sh\"\ncluster: \"slurm-submit.py\"\ncluster-status: \"slurm-status.py\"\nmax-jobs-per-second: \"10\"\nmax-status-checks-per-second: \"10\"\nlocal-cores: 1\nlatency-wait: \"5\"\nuse-conda: \"True\"\nuse-singularity: \"False\"\nsingularity-args: -B /ceph  -B /databank -B $TMPDIR --cleanenv\njobs: \"50\"\nprintshellcmds: \"True\"\nretries: 3\n\n# Example resource configuration\n# default-resources:\n#   - runtime=100\n#   - mem_mb=6000\n#   - disk_mb=1000000\n# # set-threads: map rule names to threads\n# set-threads:\n#   - single_core_rule=1\n#   - multi_core_rule=10\n# # set-resources: map rule names to resources in general\n# set-resources:\n#   - high_memory_rule:mem_mb=12000\n#   - long_running_rule:runtime=1200\n</code></pre> <p>Note: The singularity-args are required to mount the data directories into the container. e.g.</p> <pre><code>singularity-args: -B /ceph  -B /databank\n</code></pre> <p>Gives the container access to the <code>/ceph</code> and <code>/databank</code> directories on the cluster. The current working directory is also mounted into the container by default. You can add additional directories by adding more <code>-B</code> flags. Obviously this will be different for each cluster so you'll need your own defaults. The <code>$TMPDIR</code> is also mounted as this causes errors if not. The <code>--cleanenv</code> flag is also required to prevent the container from inheriting the environment from the host.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\n</pre> \"\"\"Generate the code reference pages.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"capcruncher/api\").glob(\"*.py\")):  #\n\n    module_path = path.relative_to(\".\").with_suffix(\"\")  #\n    doc_path = module_path.with_suffix(\".md\")  #\n    full_doc_path = Path(\"reference\", doc_path)  #\n\n    parts = list(module_path.parts)\n\n    if module_path.stem == \"__init__\":\n        continue\n    elif parts[-1] == \"__init__\":  #\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()  #\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:  #\n        identifier = \".\".join(parts)  #\n        print(\"::: \" + identifier, file=fd)  #\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)  #\n</pre> for path in sorted(Path(\"capcruncher/api\").glob(\"*.py\")):  #      module_path = path.relative_to(\".\").with_suffix(\"\")  #     doc_path = module_path.with_suffix(\".md\")  #     full_doc_path = Path(\"reference\", doc_path)  #      parts = list(module_path.parts)      if module_path.stem == \"__init__\":         continue     elif parts[-1] == \"__init__\":  #         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()  #      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:  #         identifier = \".\".join(parts)  #         print(\"::: \" + identifier, file=fd)  #      mkdocs_gen_files.set_edit_path(full_doc_path, path)  # In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:  #\n    nav_file.writelines(nav.build_literate_nav())  #\n</pre> with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:  #     nav_file.writelines(nav.build_literate_nav())  #"},{"location":"installation/","title":"Installation","text":"<p>Warning</p> <p>CapCruncher is currently only availible for linux. MacOS support is planned for the future.</p>"},{"location":"installation/#setup","title":"Setup","text":"<p>It is highly recommended to install CapCruncher in a conda environment. If you do not have conda installed, see the detailed conda installation section.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>There are two main ways to obtain the dependencies required to run CapCruncher:</p>"},{"location":"installation/#install-all-dependencies-using-conda","title":"Install all dependencies using conda","text":""},{"location":"installation/#direct-installation","title":"Direct Installation","text":"<p>The easiest way to install these dependencies is to use conda. Run the following command to install CapCruncher and all dependencies:</p> <pre><code>mamba install -c bioconda capcruncher\n</code></pre> <p>Warning</p> <p>The latest version of CapCruncher is not yet available on conda. Please install the latest version from PyPI using the command below.</p>"},{"location":"installation/#two-step-installation-using-conda-and-pip","title":"Two-step installation using conda and pip","text":"<p>Alternatively, create a new conda environment and install CapCruncher using pip (currently the recommended method):</p> <pre><code>wget https://raw.githubusercontent.com/sims-lab/CapCruncher/master/environment.yml\nconda env create -f environment.yml\nconda activate cc\n\n# Install CapCruncher using pip\npip install capcruncher\ns\n# Optional - highly recommended to install the optional dependencies\n# Installs dependencies for:\n# * plotting,\n# * differential interaction analysis\n# * speeding up the pipeline using experimental features (capcruncher-tools)\npip install capcruncher[full]\n</code></pre>"},{"location":"installation/#install-capcruncher-in-a-minimal-conda-environment-and-use-singularity-to-run-the-pipeline","title":"Install CapCruncher in a minimal conda environment and use singularity to run the pipeline","text":"<p>Note</p> <p>Singularity must be installed on your system to use this method. See the pipeline guide for more information. The pipeline will only function correctly if using the --use-singularity option. This is because the pipeline uses singularity containers to run the pipeline steps. See the pipeline guide for more information.</p> <p>Create a minimal conda environment and install CapCruncher using pip:</p> <pre><code>mamba create -n cc \"python&gt;=3.10\"\nconda activate cc\n# Optional - highly recommended to install the optional dependencies\npip install capcruncher[stats,plotting,experimental]\n</code></pre>"},{"location":"installation/#manual-installation-not-recommended","title":"Manual Installation (Not Recommended)","text":""},{"location":"installation/#install-dependencies","title":"Install Dependencies","text":"<p>See the dependencies in the environment.yml and requirements.txt files. All dependencies can be installed using conda or pip.</p>"},{"location":"installation/#install-capcruncher-from-github","title":"Install CapCruncher from GitHub","text":"<p>Clone the repository and install CapCruncher using pip:</p> <pre><code>git clone https://github.com/sims-lab/CapCruncher.git\ncd CapCruncher\npip install .\n\n# Optional - highly recommended to install the optional dependencies\npip install .[stats,plotting,experimental]\n</code></pre>"},{"location":"installation/#detailed-conda-installation-instructions","title":"Detailed Conda Installation Instructions","text":"<p>Download and install MambaForge from here for your system (You will typically need the x86_64 version for most Linux systems).</p>"},{"location":"installation/#download-and-run-the-installer-for-your-system-only-linux-is-supported-at-the-moment","title":"Download and run the installer for your system (only Linux is supported at the moment)","text":"<pre><code># Download the installer for your system\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\n\n# Allow the installer to be executed\nchmod +x Mambaforge-Linux-x86_64.sh\n\n# Run the installer\n./Mambaforge-Linux-x86_64.sh\n</code></pre> <p>Follow the instructions to install MambaForge. It is advised to install MambaForge in a location with a reasonable amount of free space (&gt;2GB) as it will be used to install all dependencies for CapCruncher.</p>"},{"location":"installation/#initialise-mambaforge-in-your-shell","title":"Initialise MambaForge in your shell","text":"<pre><code>conda init bash\n</code></pre>"},{"location":"installation/#refresh-your-shell","title":"Refresh your shell","text":"<pre><code>source ~/.bashrc\n</code></pre>"},{"location":"installation/#setup-conda-channels","title":"Setup conda channels","text":"<pre><code>conda config --set channel_priority strict\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre> <p>Now the installation installation of CapCruncher can be completed using the instructions above.</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The CapCruncher pipeline handles the processing of raw data from the sequencer to the generation of a contact matrix, generation of plots and production of a UCSC genome browser track hub.</p> <p>This pipeline is based on the Snakemake workflow management system. Snakemake is a Python-based workflow management system that allows for the creation of reproducible and scalable data analyses. All elements of the workflow have been wrapped into the CapCruncher Python package. This allows for the pipeline to be run using the <code>capcruncher pipeline</code> command rather than having to run the pipeline using Snakemake directly.</p> <p>Checkout the Hints and Tips page for some useful tips on configuring and running the pipeline.</p>"},{"location":"pipeline/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"pipeline/#configuration-file","title":"Configuration File","text":"<p>The pipeline is configured using a YAML file. It is strongly recommended to use the <code>capcruncher pipeline-config</code> command to generate a template configuration file. This command will generate a template configuration file with all available options and descriptions of each option.</p> <pre><code>capcruncher pipeline-config\n</code></pre> <p>This utility will walk through the configuration options and generate a configuration file. It will generate a new directory  and place the filled-out <code>capcruncher_config.yml</code> file in this directory. <p>For an example configuration file, see here.</p> <p>The configuration file can be edited manually if required e.g. to add a manually generated <code>design</code> file. Just ensure that the configuration file is valid YAML. A common error is to use tabs instead of spaces, this will cause the pipeline to fail while parsing the configuration file.</p> <p>All options in the configuration file are documented within the file itself. Only the required options need to be filled out. The pipeline will use default values for all other options.</p>"},{"location":"pipeline/#design-file","title":"Design File","text":"<p>The design file is a tab/comma/space-delimited file that contains the sample names and the metadata for each sample. This file is completely optional and only used for comparisons between Capture-C and Tri-C data. If it is not provided the pipeline will perform a basic sample name comparison to generate a basic design file. However, this will not be as accurate as a manually generated design file. The <code>design</code> file is a tab delimited file with the following columns:</p> <ul> <li><code>sample</code>: The name of the FASTQ file (without the _R1.fastq.gz or_2.fastq.gz suffix)</li> <li><code>condition</code>: The Group that the sample belongs to.</li> </ul> <p>Provide the path to this file in the config file under the <code>design</code> key.</p>"},{"location":"pipeline/#setting-up-the-input-directory","title":"Setting up the input directory","text":"<p>Ensure that you have the configuration file and the fastq files to process in the current working directory. Symbolic links can be used to link to the fastq files if they are stored elsewhere but please ensure that the full path to the fastq files is used to create the symbolic links. e.g.</p> <pre><code># Example 1 - Make a symbolic link to the fastq file in the current directory\nln -s /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP1_1.fastq.gz .\n\n# Example 2 - Make a symbolic link to the fastq file in the current directory with a different name\nln -s /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP1_1.fastq.gz SAMPLE-A_REP1_1.fastq.gz\n\n# Example 3 - Use realpath to get the full path to the fastq file and then create a symbolic link to it in another directory\nln -s $(realpath SAMPLE-A_REP1_1.fastq.gz) /tmp/pytest-of-asmith/pytest-current/test_dircurrent/\n</code></pre> <p>The pipeline will automatically detect the configuration file and the fastq files. For example your working directory should look like this:</p> <pre><code>2023-08-02_project_name_capture/\n|-- SAMPLE-A_REP1_1.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP1_1.fastq.gz\n|-- SAMPLE-A_REP1_2.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP1_2.fastq.gz\n|-- SAMPLE-A_REP2_1.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP2_1.fastq.gz\n|-- SAMPLE-A_REP2_2.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-A_REP2_2.fastq.gz\n|-- SAMPLE-B_REP1_1.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-B_REP1_1.fastq.gz\n|-- SAMPLE-B_REP1_2.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-B_REP1_2.fastq.gz\n|-- SAMPLE-B_REP2_1.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-B_REP2_1.fastq.gz\n|-- SAMPLE-B_REP2_2.fastq.gz -&gt; /ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/SAMPLE-B_REP2_2.fastq.gz\n`-- capcruncher_config.yml\n</code></pre>"},{"location":"pipeline/#running-the-pipeline","title":"Running the pipeline","text":""},{"location":"pipeline/#basic-usage","title":"Basic Usage","text":"<p>The pipeline is run using the <code>capcruncher pipeline</code> command.</p> <pre><code># Usage\ncapcruncher pipeline --cores &lt;NUMBER OF CORES TO USE&gt;\n\n# Example\ncapcruncher pipeline --cores 8\n</code></pre>"},{"location":"pipeline/#hpc-cluster-usage-recommended-if-available","title":"HPC Cluster Usage (Recommended if available)","text":"<p>The pipeline can also be run on HPC clusters using a number of different job schedulers. See here for a quick overview on how to configure the pipeline to run on HPC clusters.</p> <p>For further information see both the Snakemake documentation and the Snakemake profile documentation.</p> <p>This is a quick example of how to run the pipeline with a pre-generated profile. This is not a complete guide and you will need to modify the configuration to suit your cluster.</p> <pre><code>capcruncher pipeline -c &lt;NUMBER OF CORES e.g. 20&gt; --profile &lt;NAME OF PROFILE OR PATH TO PROFILE&gt;\n</code></pre>"},{"location":"pipeline/#singularity-usage-recommended-if-available","title":"Singularity Usage (Recommended if available)","text":"<p>Containers have the advantage of their contents being fixed at the time of creation. This means that the pipeline will always run with the same versions of the software and aids reliablity and reproducibility. The pipeline can be run using singularity containers. This is the recommended method of running the pipeline.</p> <p>The pipeline can be run using singularity containers using the <code>--use-singularity</code> option.</p> <pre><code># Local mode\ncapcruncher pipeline --use-singularity --cores &lt;NUMBER OF CORES TO USE&gt;\n\n# Cluster mode\ncapcruncher pipeline --use-singularity --cores &lt;NUMBER OF CORES TO USE&gt; --profile &lt;NAME OF PROFILE OR PATH TO PROFILE&gt;\n</code></pre>"},{"location":"pipeline/#avoiding-disconnection-from-the-cluster","title":"Avoiding Disconnection from the Cluster","text":"<p>In order to avoid disconnecting from the cluster, it is recommended to run the pipeline in a tmux session. Alternatively, nohup can be used to run the pipeline in the background. For example:</p> <pre><code># tmux example\ntmux new -s capcruncher\ncapcruncher pipeline --cores 8 --profile slurm --use-singularity\n\n# nohup example\nnohup capcruncher pipeline --cores 8 --profile slurm --use-singularity &amp;\n</code></pre>"},{"location":"pipeline/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"pipeline/#all-assays","title":"All Assays","text":"<p>For all assays the pipeline consists of the following steps:</p> <ol> <li>Quality Control: FastQC is used to perform quality control on the FASTQ files.</li> <li>Read Splitting: The FASTQ files are split into parts of a user-defined size (default 1 million reads per part). This is done to allow for parallel processing of the FASTQ files and to reduce the memory requirements of the pipeline.</li> <li>Remove PCR Duplicates: PCR duplicates are removed from the FASTQ files using the CapCruncher package to reduce the memory and CPU requirements of the pipeline.</li> <li>Read Trimming: Trimming of the FASTQ files is performed using Trim Galore.</li> <li>Read Combining: The trimmed FASTQ files are combined using FLASh to obtain the ligtion junctions.</li> <li>Read in silico Digestion: The combined FASTQ files are digested in silico using the restriction enzyme or site specified in the configuration file.</li> <li>Read Alignment: The digested FASTQ files are aligned to the reference genome using bowtie2.</li> <li>Alignment Annotation: The aligned reads are annotated using the CapCruncher package.</li> <li>Alignment Filtering: The aligned reads are filtered using the CapCruncher package.</li> <li>Alignment PCR Duplicate Removal: PCR duplicates are removed from the aligned reads using the CapCruncher package.</li> <li>Contact Matrix Generation: Contact matrices are generated using the CapCruncher package and stored in cooler (HDF5) format.</li> <li>Pipeline Statistics: Statistics are generated for each sample using the CapCruncher package.</li> <li>Pipeline Plots: Plots and <code>capcruncher plot</code> compatible templates (TOML format) are generated for each sample using the CapCruncher package.</li> </ol>"},{"location":"pipeline/#capture-c-and-tri-c","title":"Capture-C and Tri-C","text":"<ol> <li>Pileup Generation: BigWig files are generated for each sample using the CapCruncher package.</li> <li>Pileup Normalisation: The pileup files are normalised using the CapCruncher package.</li> <li>Pileup Comparison: Interactions are compared between samples (if two or more replicates are provided) using the CapCruncher package.</li> </ol> <p>Note</p> <p>Additional methods for comparing interactions between samples will be added in the future.</p> <ol> <li>Differenital Interaction Analysis: Differential interactions are identified between groups of samples using PyDESeq2.</li> </ol> <p>Warning</p> <p>The UCSC Genome Browser Track Hub is only generated if the <code>ucsc_genome_browser_track_hub</code> option is set to <code>True</code> in the configuration file.</p> <ol> <li>UCSC Genome Browser Track Hub: A UCSC Genome Browser track hub is generated using the CapCruncher package.</li> </ol>"},{"location":"pipeline/#tiled-c","title":"Tiled-C","text":"<ol> <li>Contact Matrix Normalisation: The contact matrices are normalised using the CapCruncher package with various third-party integrations.</li> <li>Plot Generation: Plots are generated for each sample using the CapCruncher package.</li> </ol>"},{"location":"pipeline/#pipeline-output","title":"Pipeline Output","text":"<p>The <code>capcruncher_output/results</code> directory contains the following files:</p>"},{"location":"pipeline/#general","title":"General","text":"<ul> <li><code>design_matrix.tsv</code>: the design matrix used or generated by the pipeline. This   file is used for the differential analysis and plot generation.</li> </ul>"},{"location":"pipeline/#statistics-and-qc","title":"Statistics and QC","text":"<ul> <li> <p><code>capcruncher_report.html</code>: the main report of the pipeline. It contains the   results of the analysis and the figures.</p> </li> <li> <p><code>full_qc_report.html</code>: the full QC report of the pipeline. It contains the   results of the QC analysis and the figures.</p> </li> </ul>"},{"location":"pipeline/#individual-samples","title":"Individual Samples","text":"<ul> <li> <p><code>&lt;SAMPLE_NAME&gt;</code>: the results of the analysis for each sample. The directory contains the following files:</p> </li> <li> <p><code>bigwigs</code>: the bigwig files of the sample. The files are stored in the       <code>raw</code> and <code>norm</code> directories. The <code>raw</code> directory contains the raw       bigwig files. The <code>norm</code> directory contains the normalized bigwig files.       Note: Only for Capture-C and Tri-C</p> </li> <li> <p><code>&lt;SAMPLE_NAME&gt;.bam</code>: the alignment file of the sample.</p> </li> <li> <p><code>&lt;SAMPLE_NAME&gt;.bam.bai</code>: the index of the alignment file of the sample.</p> </li> <li> <p><code>&lt;SAMPLE_NAME&gt;.hdf5</code>: Cooler formated groups within the HDF5 file per viewpoint. See Cooler documentation for more information on the format.</p> </li> <li> <p><code>&lt;SAMPLE_NAME&gt;.parquet</code>: This contains all of the data from the sample in a tabular format that can be accessed by any software that reads parquet files e.g. <code>pandas</code>, <code>polars</code>, Arrow for R.</p> </li> </ul>"},{"location":"pipeline/#comparisons-and-differential-analysis","title":"Comparisons and Differential Analysis","text":"<p>Note</p> <p>The comparisons and differential analysis are only performed if two or more replicates are provided. Currently only Capture-C and Tri-C data are supported.</p> <ul> <li> <p><code>comparisons</code>: the results of the comparisons between the different   conditions. The results are stored in the <code>bigwigs</code>   directories</p> </li> <li> <p><code>differential</code>: the results of the differential analysis</p> </li> </ul>"},{"location":"pipeline/#visualisation","title":"Visualisation","text":"<ul> <li> <p><code>figures</code>: Plots generated by the pipeline. for each viewpoint at the coordinates provided in the configuration file.     This also generates templates that can be used with the <code>capcruncher plot make-plot</code> command. See CoolBox API documentation for more parameters that can be used to customize the plots. The current plotting system is a thin wrapper over this library and any parameter specified will be passed to these classes directly. See the plotting documentation for more information.</p> </li> <li> <p>UCSC Hub (if enabled in the configuration file): the UCSC Genome Browser track hub   of the pipeline. It contains the bigwig files of the samples and the   comparisons between samples (Note: Only for Capture-C and Tri-C)</p> </li> </ul> <p>Tip</p> <pre><code>Unlike regular cooler.hdf5 files, there are multiple groups per file, one per viewpoint. You can use any cooler supported tool but you will need to specify the group name. For example, to get the matrix for the viewpoint `Slc25A37` you can use the following command:\n\n``` bash\ncooler dump -t pixels -o Slc25A37.tsv &lt;SAMPLE_NAME&gt;.hdf5::Slc25A37`\n\n```\n</code></pre>"},{"location":"plotting/","title":"Plotting CapCruncher output","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport coolbox.api as cb\nfrom capcruncher.api.plotting import CCTrack, CCFigure\nimport pyranges as pr\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import coolbox.api as cb from capcruncher.api.plotting import CCTrack, CCFigure import pyranges as pr In\u00a0[2]: Copied! <pre>viewpoints = pr.read_bed(\"alpha_tile_mm9.bed\")\nviewpoints\n</pre> viewpoints = pr.read_bed(\"alpha_tile_mm9.bed\") viewpoints Out[2]: Chromosome Start End Name 0 chr11 29902950 33226736 Alpha In\u00a0[3]: Copied! <pre>tracks = [\n    CCTrack(None, type=\"scale\"),\n    CCTrack(\n        \"capcruncher_output/results/WT_FL_S3_Replicate1/WT_FL_S3_Replicate1.hdf5\",\n        type=\"heatmap\",\n        binsize=2000,\n        title=\"Alpha Tile\",\n        viewpoint=\"Alpha\",\n        normalization=\"ice\",\n        transform=\"yes\",\n        style=\"triangular\",\n    ),\n    CCTrack(None, type=\"spacer\"),\n    CCTrack(None, type=\"spacer\"),\n    CCTrack(None, type=\"xaxis\"),\n]\n\nfig = CCFigure(tracks, auto_spacing=False)\n</pre> tracks = [     CCTrack(None, type=\"scale\"),     CCTrack(         \"capcruncher_output/results/WT_FL_S3_Replicate1/WT_FL_S3_Replicate1.hdf5\",         type=\"heatmap\",         binsize=2000,         title=\"Alpha Tile\",         viewpoint=\"Alpha\",         normalization=\"ice\",         transform=\"yes\",         style=\"triangular\",     ),     CCTrack(None, type=\"spacer\"),     CCTrack(None, type=\"spacer\"),     CCTrack(None, type=\"xaxis\"), ]  fig = CCFigure(tracks, auto_spacing=False) In\u00a0[4]: Copied! <pre>fig2 = CCFigure(tracks, auto_spacing=False)\nfig2.add_track(CCTrack(None, type=\"scale\"))\n</pre> fig2 = CCFigure(tracks, auto_spacing=False) fig2.add_track(CCTrack(None, type=\"scale\")) In\u00a0[5]: Copied! <pre># Plot a specific region if desired\nfig.plot(\"chr11:29902950-33226736\")\n</pre> # Plot a specific region if desired fig.plot(\"chr11:29902950-33226736\") Out[5]: In\u00a0[6]: Copied! <pre>fig.save(\"chr11:29902950-33226736\", output=\"test.png\")\n</pre> fig.save(\"chr11:29902950-33226736\", output=\"test.png\") In\u00a0[7]: Copied! <pre>fig.to_toml(output=\"template.toml\")\n</pre> fig.to_toml(output=\"template.toml\") <p>This will look something like this:</p> In\u00a0[21]: Copied! <pre>!head -n 15 template.toml\n</pre> !head -n 15 template.toml <pre>[\"scale 0\"]\ntype = \"scale\"\n\n[\"spacer 0\"]\ntype = \"spacer\"\n\n[\"spacer 1\"]\ntype = \"spacer\"\n\n[\"Alpha Tile\"]\ntype = \"heatmap\"\nbinsize = 2000\ntitle = \"Alpha Tile\"\nviewpoint = \"Alpha\"\nnormalization = \"ice\"\n</pre> <p>This template can be re-loaded using the CapCruncher package e.g. using the <code>CCFigure.from_toml</code> method. You can also add new tracks to the figure and re-plot it.</p> <p>See this rather contrived example of reloading the figure and adding a new scale bar to it.</p> In\u00a0[14]: Copied! <pre>fig = CCFigure.from_toml(\"template.toml\")\nfig.add_track(CCTrack(None, type=\"scale\"))\nfig.plot(\"chr11:29902950-33226736\")\n</pre> fig = CCFigure.from_toml(\"template.toml\") fig.add_track(CCTrack(None, type=\"scale\")) fig.plot(\"chr11:29902950-33226736\") Out[14]: <p>Alternatively the template can be edited and used on the commandline with <code>capcruncher plot</code> e.g.</p> In\u00a0[18]: Copied! <pre>%%bash\ncapcruncher \\\nplot \\\n--template template.toml \\\n--region chr11:29902950-33226736 \\\n-o test.png\n</pre> %%bash capcruncher \\ plot \\ --template template.toml \\ --region chr11:29902950-33226736 \\ -o test.png <p>This will generate the same figure as before:</p> <p></p>"},{"location":"plotting/#plotting-capcruncher-output","title":"Plotting CapCruncher output\u00b6","text":""},{"location":"plotting/#introduction","title":"Introduction\u00b6","text":"<p>This is a quick overview of the plotting functionality of CapCruncher. Please ensure that you have installed the optional dependencies for CapCruncher by running:</p> <pre>pip install capcruncher[full]\n</pre>"},{"location":"plotting/#imports","title":"Imports\u00b6","text":""},{"location":"plotting/#read-plotting-coordinates","title":"Read plotting coordinates\u00b6","text":""},{"location":"plotting/#plot-using-the-capcruncher-api","title":"Plot using the CapCruncher API\u00b6","text":"<p>First create a number of <code>CCTrack</code> instances supported track types:</p> <ul> <li>heatmap - a contact matrix heatmap in cool format</li> <li>bigwig  - a bigwig file containing the number of reads per bin</li> <li>bigwig_summary - a collection of bigwig files containing the number of reads per bin</li> <li>scale - a scale bar. Does not require a file to be specified</li> <li>bed  - a bed file</li> <li>xaxis  - an x-axis of genomic coordinates. Does not require a file to be specified</li> <li>genes - a gene track in bed12 format</li> <li>spacer - a spacer track. Does not require a file to be specified</li> </ul>"},{"location":"plotting/#create-new-cctrack-objects","title":"Create new <code>CCTrack</code> objects\u00b6","text":""},{"location":"plotting/#option-1-create-a-list-of-cctrack-objects-and-pass-them-to-ccfigure","title":"Option 1: Create a list of <code>CCTrack</code> objects and pass them to CCFigure\u00b6","text":""},{"location":"plotting/#option-2-create-a-ccfigure-object-and-add-tracks-to-it","title":"Option 2: Create a <code>CCFigure</code> object and add tracks to it\u00b6","text":"<p>As this would overwrite the previous figure, we will create a new figure and add the tracks to it.</p>"},{"location":"plotting/#plot-the-figure-optional","title":"Plot the figure (Optional)\u00b6","text":""},{"location":"plotting/#save-the-figure","title":"Save the figure\u00b6","text":"<p>Two options:</p> <ol> <li>Save the figure as a static image using the save method of the <code>CCFigure</code>.</li> <li>Save the <code>CCFigure</code> as a TOML file which can be edited and either reloaded into a <code>CCFigure</code> or used by the command line interface to generate a new figure using <code>capcruncher plot make-plot</code>.</li> </ol>"},{"location":"plotting/#option-1-save-the-figure-to-a-file","title":"Option 1: Save the figure to a file\u00b6","text":""},{"location":"plotting/#option-2-save-the-figure-as-a-template","title":"Option 2: Save the figure as a template\u00b6","text":""},{"location":"tips/","title":"CapCruncher Tips and Tricks","text":""},{"location":"tips/#interruptions-to-the-pipeline-eg-system-failure-manual-interruption","title":"Interruptions to the pipeline (e.g. system failure, manual interruption)","text":""},{"location":"tips/#restarting-the-pipeline-after-an-interruption","title":"Restarting the pipeline after an interruption","text":"<p>If the pipeline is interrupted, it can be restarted by simply running the pipeline command again e.g. <code>capcruncher pipeline -c 1</code>.</p> <p>CapCruncher will detect which steps have already been completed and will skip them. This is useful if the pipeline is interrupted due to a system failure or if you want to add more samples to the pipeline.</p>"},{"location":"tips/#unlocking-the-working-directory","title":"Unlocking the working directory","text":"<p>Snakemake locks the working directory during the pipeline run. If the pipeline is interrupted, the working directory will remain locked and will not restart. To unlock the working directory, run:</p> <pre><code>capcruncher pipeline --unlock\n</code></pre>"},{"location":"tips/#interuptions-to-the-pipeline-eg-error-in-pipeline","title":"Interuptions to the pipeline (e.g. error in pipeline)","text":"<p>Pipeline errors very frequently are found in a few major areas:</p>"},{"location":"tips/#no-fastq-files-found","title":"No fastq files found","text":"<p>The pipeline cannot find the fastq files to process (e.g. the files are not in the current working directory or are not named correctly) this will cause an error like this:</p> <pre><code>2023-08-03 11:56:17.857 | ERROR    | capcruncher.pipeline.utils:from_files:178 - No fastq files found.\nValueError in file /ceph/home/a/asmith/software/CapCruncher/capcruncher/pipeline/workflow/Snakefile, line 30:\nNo fastq files found.\n  File \"/ceph/home/a/asmith/software/CapCruncher/capcruncher/pipeline/workflow/Snakefile\", line 30, in &lt;module&gt;\n  File \"/ceph/home/a/asmith/software/CapCruncher/capcruncher/pipeline/utils.py\", line 179, in from_files\n</code></pre> <p>To correct this, ensure that the fastq files are in the current working directory and are named correctly. See the pipeline guide for more information on how to name the fastq files.</p>"},{"location":"tips/#indicies-not-found-or-specified-incorrectly","title":"Indicies not found or specified incorrectly","text":"<p>If the pipeline cannot find the reference genome files this will cause an error at the alignment step. Please ensure that the reference genome (<code>aligner_indicies</code>) are added to the config file.</p> <pre><code>aligner_indicies: \"/ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/chr14_bowtie2_indicies/bt2\"\n</code></pre> <p>This refers to the bowtie2 index files here:</p> <pre><code>tree \"/ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/chr14_bowtie2_indicies/\"\n/ceph/home/a/asmith/software/CapCruncher/tests/data/data_for_pipeline_run/chr14_bowtie2_indicies/\n|-- bt2.1.bt2\n|-- bt2.2.bt2\n|-- bt2.3.bt2\n|-- bt2.4.bt2\n|-- bt2.rev.1.bt2\n|-- bt2.rev.2.bt2\n</code></pre>"},{"location":"tips/#incorrect-viewpoint-coordinates","title":"Incorrect viewpoint coordinates","text":"<p>Viewpoint coordinate errors fall into two categories:</p>"},{"location":"tips/#viewpoint-names-contain-invalid-characters","title":"Viewpoint names contain invalid characters","text":"<p>Including special characters e.g. \"\\/*?\" in the viewpoint name will prevent the pipeline from running to completion. Viewpoint names should only contain alphanumeric characters (<code>A-Za-z0-9</code>), hyphens (<code>-</code>) and underscores (<code>_</code>). No other stipulations are placed on the viewpoint name.</p>"},{"location":"tips/#viewpoint-coordinates-are-incorrect-for-the-supplied-reference-genome","title":"Viewpoint coordinates are incorrect for the supplied reference genome","text":"<p>Warning</p> <p>Errors in the viewpoint coordinates can be difficult to spot as pipeline errors will occur further downstream. The initial error occurs at the filtering step but the pipeline will continue to run</p> <p>In the future the presence of valid viewpoints will be confirmed during the pipeline run but for now it is up to the user to ensure that the viewpoint coordinates are correct.</p> <p>Viewpoint coordinates are supplied in the config file as a BED file and should be checked against the reference genome.</p>"},{"location":"tips/#capture-c-and-tri-c-experiments","title":"Capture-C and Tri-C experiments","text":"<p>The viewpoint coordinates specified should be that of the restriction fragment containing the viewpoint. This is usually 1-4 kb in size but the coordinates do not have to be exact (e.g. +/- 100 bp is fine).</p>"},{"location":"tips/#tiled-c-experiments","title":"Tiled-C experiments","text":"<p>The viewpoint coordinates should contain all restriction fragments that have been captured by the tiled oligos. Again these do not have to be basepair level accurate but should be as close as possible.</p>"},{"location":"tips/#adding-additional-snakemake-options","title":"Adding additional Snakemake options","text":"<p>Additional Snakemake options can be passed to the pipeline command by just adding them to the end of the command. For example, to run the pipeline with 8 cores and prevent the pipeline from removing intermediate files, run:</p> <pre><code>capcruncher pipeline --cores 8 --notemp\n</code></pre> <p>See the Snakemake documentation for a list of available options.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>capcruncher<ul> <li>api<ul> <li>annotate</li> <li>deduplicate</li> <li>filter</li> <li>io</li> <li>pileup</li> <li>plotting</li> <li>statistics</li> <li>storage</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/capcruncher/api/annotate/","title":"annotate","text":""},{"location":"reference/capcruncher/api/annotate/#capcruncher.api.annotate.increase_cis_slice_priority","title":"<code>increase_cis_slice_priority(df, score_multiplier=2)</code>","text":"<p>Prioritizes cis slices by increasing the mapping score.</p> Source code in <code>capcruncher/api/annotate.py</code> <pre><code>def increase_cis_slice_priority(df: pd.DataFrame, score_multiplier: float = 2):\n    \"\"\"\n    Prioritizes cis slices by increasing the mapping score.\n    \"\"\"\n\n    df[\"parent_name\"] = df[\"name\"].str.split(\"|\").str[0]\n\n    df_chrom_counts = (\n        df[[\"parent_name\", \"chrom\"]].value_counts().to_frame(\"slices_per_chrom\")\n    )\n    modal_chrom = (\n        df_chrom_counts.groupby(\"parent_name\")[\"slices_per_chrom\"]\n        .transform(\"max\")\n        .reset_index()\n        .set_index(\"parent_name\")[\"chrom\"]\n        .to_dict()\n    )\n    df[\"fragment_chrom\"] = df[\"parent_name\"].map(modal_chrom)\n    df[\"score\"] = np.where(\n        df[\"chrom\"] == df[\"fragment_chrom\"],\n        df[\"score\"] * score_multiplier,\n        df[\"score\"] / score_multiplier,\n    )\n\n    return df.drop(columns=\"parent_name\")\n</code></pre>"},{"location":"reference/capcruncher/api/annotate/#capcruncher.api.annotate.remove_duplicates_from_bed","title":"<code>remove_duplicates_from_bed(bed, prioritize_cis_slices=False, chroms_to_prioritize=None)</code>","text":"<p>Removes duplicate entries from a PyRanges object.</p> <p>Parameters:</p> Name Type Description Default <code>bed</code> <code>PyRanges</code> <p>PyRanges object to be deduplicated.</p> required <code>prioritize_cis_slices</code> <code>bool</code> <p>Prioritize cis slices by increasing the mapping score. Defaults to False.</p> <code>False</code> <code>chroms_to_prioritize</code> <code>Union[list, ndarray]</code> <p>Chromosomes to prioritize. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>PyRanges</code> <p>pr.PyRanges: Deduplicated PyRanges object.</p> Source code in <code>capcruncher/api/annotate.py</code> <pre><code>def remove_duplicates_from_bed(\n    bed: pr.PyRanges,\n    prioritize_cis_slices: bool = False,\n    chroms_to_prioritize: Union[list, np.ndarray] = None,\n) -&gt; pr.PyRanges:\n    \"\"\"\n    Removes duplicate entries from a PyRanges object.\n\n    Args:\n        bed (pr.PyRanges): PyRanges object to be deduplicated.\n        prioritize_cis_slices (bool, optional): Prioritize cis slices by increasing the mapping score. Defaults to False.\n        chroms_to_prioritize (Union[list, np.ndarray], optional): Chromosomes to prioritize. Defaults to None.\n\n    Returns:\n        pr.PyRanges: Deduplicated PyRanges object.\n    \"\"\"\n\n    df = bed.df.rename(columns=lambda col: col.lower()).rename(\n        columns={\"chromosome\": \"chrom\"}\n    )\n\n    # Shuffle the dataframe to randomize the duplicate removal\n    df = df.sample(frac=1)\n\n    if prioritize_cis_slices:\n        df = increase_cis_slice_priority(df)\n\n    if \"score\" in df.columns:\n        df = df.sort_values([\"score\"], ascending=False)\n\n    if chroms_to_prioritize:\n        df[\"is_chrom_priority\"] = df[\"chrom\"].isin(chroms_to_prioritize).astype(int)\n        df = df.sort_values([\"score\", \"is_chrom_priority\"], ascending=False).drop(\n            columns=\"is_chrom_priority\"\n        )\n\n    return (\n        df.drop_duplicates(subset=\"name\", keep=\"first\")\n        .sort_values([\"chrom\", \"start\"])[[\"chrom\", \"start\", \"end\", \"name\"]]\n        .rename(columns=lambda col: col.capitalize())\n        .rename(columns={\"Chrom\": \"Chromosome\"})\n        .pipe(pr.PyRanges)\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/","title":"deduplicate","text":""},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDeduplicationParserProcess","title":"<code>ReadDeduplicationParserProcess</code>","text":"<p>               Bases: <code>Process</code></p> <p>Process subclass for parsing fastq file(s) into a hashed {id:sequence} json format.</p> <p>Attributes:</p> Name Type Description <code>inq</code> <p>Input read queue</p> <code>outq</code> <p>Output read queue (Not currently used)</p> <code>hash_seed</code> <p>Seed for xxhash64 algorithm to ensure consistency</p> <code>save_hash_dict_path</code> <p>Path to save hashed dictionary</p> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>class ReadDeduplicationParserProcess(Process):\n    \"\"\"\n    Process subclass for parsing fastq file(s) into a hashed {id:sequence} json format.\n\n    Attributes:\n     inq: Input read queue\n     outq: Output read queue (Not currently used)\n     hash_seed: Seed for xxhash64 algorithm to ensure consistency\n     save_hash_dict_path: Path to save hashed dictionary\n    \"\"\"\n\n    def __init__(\n        self,\n        inq: multiprocessing.Queue,\n        hash_seed: int = 42,\n        output_path: os.PathLike = \"parsed.json\",\n    ):\n        \"\"\"\n        Args:\n         inq (multiprocessing.SimpleQueue): Input queue for fastq reads.\n         outq (multiprocessing.SimpleQueue): Output queue for processed reads.\n                                             Only used if part of a pipeline\n         hash_seed (int, optional): Seed to use for hashing. Defaults to 42.\n         output_path (os.PathLike, optional): Path to save hashed reads.\n        \"\"\"\n\n        self.inq = inq\n        self.hash_seed = hash_seed\n        self.output_path = output_path\n\n        super(ReadDeduplicationParserProcess, self).__init__()\n\n    def run(self):\n        \"\"\"Processes fastq reads from multiple files and generates a hashed json dict.\n\n        Dictionary is hashed and in the format {(read  1 name + read 2 name): (s1 + s2)}\n\n        Output path is specified by save_hashed_dict_path.\n\n        \"\"\"\n\n        hash_seed = self.hash_seed\n        hash_function = functools.partial(xxhash.xxh64_intdigest, seed=hash_seed)\n        records = dict()\n\n        while True:\n\n            try:\n                reads = self.inq.get(block=True, timeout=0.01)\n\n                if reads:\n\n                    for read_set in reads:\n                        hash_sequence = hash_function(\n                            \"\".join([r.sequence for r in read_set])\n                        )\n                        hash_id = hash_function(\"\".join([r.name for r in read_set]))\n                        records[hash_id] = hash_sequence\n\n                else:\n                    break\n\n            except queue.Empty:\n                continue\n\n        output_format = get_file_type(self.output_path)\n        save_dict(records, self.output_path, output_format)\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDeduplicationParserProcess.__init__","title":"<code>__init__(inq, hash_seed=42, output_path='parsed.json')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inq</code> <code>SimpleQueue</code> <p>Input queue for fastq reads.</p> required <code>outq</code> <code>SimpleQueue</code> <p>Output queue for processed reads.                                    Only used if part of a pipeline</p> required <code>hash_seed</code> <code>int</code> <p>Seed to use for hashing. Defaults to 42.</p> <code>42</code> <code>output_path</code> <code>PathLike</code> <p>Path to save hashed reads.</p> <code>'parsed.json'</code> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    inq: multiprocessing.Queue,\n    hash_seed: int = 42,\n    output_path: os.PathLike = \"parsed.json\",\n):\n    \"\"\"\n    Args:\n     inq (multiprocessing.SimpleQueue): Input queue for fastq reads.\n     outq (multiprocessing.SimpleQueue): Output queue for processed reads.\n                                         Only used if part of a pipeline\n     hash_seed (int, optional): Seed to use for hashing. Defaults to 42.\n     output_path (os.PathLike, optional): Path to save hashed reads.\n    \"\"\"\n\n    self.inq = inq\n    self.hash_seed = hash_seed\n    self.output_path = output_path\n\n    super(ReadDeduplicationParserProcess, self).__init__()\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDeduplicationParserProcess.run","title":"<code>run()</code>","text":"<p>Processes fastq reads from multiple files and generates a hashed json dict.</p> <p>Dictionary is hashed and in the format {(read  1 name + read 2 name): (s1 + s2)}</p> <p>Output path is specified by save_hashed_dict_path.</p> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>def run(self):\n    \"\"\"Processes fastq reads from multiple files and generates a hashed json dict.\n\n    Dictionary is hashed and in the format {(read  1 name + read 2 name): (s1 + s2)}\n\n    Output path is specified by save_hashed_dict_path.\n\n    \"\"\"\n\n    hash_seed = self.hash_seed\n    hash_function = functools.partial(xxhash.xxh64_intdigest, seed=hash_seed)\n    records = dict()\n\n    while True:\n\n        try:\n            reads = self.inq.get(block=True, timeout=0.01)\n\n            if reads:\n\n                for read_set in reads:\n                    hash_sequence = hash_function(\n                        \"\".join([r.sequence for r in read_set])\n                    )\n                    hash_id = hash_function(\"\".join([r.name for r in read_set]))\n                    records[hash_id] = hash_sequence\n\n            else:\n                break\n\n        except queue.Empty:\n            continue\n\n    output_format = get_file_type(self.output_path)\n    save_dict(records, self.output_path, output_format)\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDuplicateRemovalProcess","title":"<code>ReadDuplicateRemovalProcess</code>","text":"<p>               Bases: <code>Process</code></p> <p>Process subclass for parsing fastq file(s) and removing identified duplicates.</p> <p>Attributes:</p> Name Type Description <code>inq</code> <p>Input read queue</p> <code>outq</code> <p>Output queue for deduplicated reads.</p> <code>duplicated_ids</code> <p>Concatenated read ids to remove from input fastq files.</p> <code>statq</code> <p>Output queue for statistics.</p> <code>reads_total</code> <p>Number of fastq reads processed.</p> <code>reads_unique</code> <p>Number of non-duplicated reads output.</p> <code>hash_seed</code> <p>Seed for xxhash algorithm. Same as ReadDuplicationParserProcess.</p> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>class ReadDuplicateRemovalProcess(Process):\n    \"\"\"\n    Process subclass for parsing fastq file(s) and removing identified duplicates.\n\n    Attributes:\n     inq: Input read queue\n     outq: Output queue for deduplicated reads.\n     duplicated_ids: Concatenated read ids to remove from input fastq files.\n     statq: Output queue for statistics.\n     reads_total: Number of fastq reads processed.\n     reads_unique: Number of non-duplicated reads output.\n     hash_seed: Seed for xxhash algorithm. Same as ReadDuplicationParserProcess.\n    \"\"\"\n\n    def __init__(\n        self,\n        inq: multiprocessing.Queue,\n        outq: multiprocessing.Queue,\n        stats_tx: multiprocessing.Pipe,\n        duplicated_ids: set,\n        hash_seed: int = 42,\n        hash_read_name: bool = True,\n    ):\n        \"\"\"\n        Args:\n         inq (multiprocessing.SimpleQueue): Input queue for reads to be deduplicated.\n         outq (multiprocessing.SimpleQueue): Output queue for deduplicated reads.\n         duplicated_ids (set): Hashed read ids to be removed if encountered.\n         statq (multiprocessing.Queue, optional): Output queue for statistics.\n         hash_seed (int, optional): Seed for xxhash algorithm. Defaults to 42.\n        \"\"\"\n\n        self.inq = inq\n        self.outq = outq\n        self.hash_seed = hash_seed\n        self.duplicated_ids = duplicated_ids\n\n        # Misc\n        self.hash_read_name = hash_read_name\n\n        # Stats\n        self.stats_tx = stats_tx\n        self.reads_total = 0\n        self.reads_unique = 0\n\n        super(ReadDuplicateRemovalProcess, self).__init__()\n\n    def run(self):\n\n        \"\"\"Performs read deduplication based on sequence.\n\n        Unique reads are placed on outq and deduplication stats are placed on statq.\n\n        \"\"\"\n\n        hash_seed = self.hash_seed\n        hash_read_name = self.hash_read_name\n        hash_function = functools.partial(xxhash.xxh64_intdigest, seed=hash_seed)\n        duplicated_ids = self.duplicated_ids\n        reads_unique = list()\n\n        while True:\n\n            try:\n                reads = self.inq.get(block=True, timeout=0.01)\n\n                if reads:\n                    for read_glob in reads:\n\n                        hash_id = hash_function(\"\".join([r.name for r in read_glob]))\n\n                        if hash_id not in duplicated_ids:\n                            if hash_read_name:\n                                for r in read_glob:\n                                    r.name = str(hash_function(r.name))\n\n                            reads_unique.append(read_glob)\n\n                    self.reads_total += len(reads)\n                    self.reads_unique += len(reads_unique)\n                    self.outq.put(reads_unique.copy())\n                    reads_unique.clear()\n\n                else:\n                    break\n\n            except queue.Empty:\n                continue\n\n        stats = RemovalStatistics(\n            self.reads_total, self.reads_unique, self.reads_total - self.reads_unique\n        )\n        self.stats_tx.send(stats)\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDuplicateRemovalProcess.__init__","title":"<code>__init__(inq, outq, stats_tx, duplicated_ids, hash_seed=42, hash_read_name=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inq</code> <code>SimpleQueue</code> <p>Input queue for reads to be deduplicated.</p> required <code>outq</code> <code>SimpleQueue</code> <p>Output queue for deduplicated reads.</p> required <code>duplicated_ids</code> <code>set</code> <p>Hashed read ids to be removed if encountered.</p> required <code>statq</code> <code>Queue</code> <p>Output queue for statistics.</p> required <code>hash_seed</code> <code>int</code> <p>Seed for xxhash algorithm. Defaults to 42.</p> <code>42</code> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    inq: multiprocessing.Queue,\n    outq: multiprocessing.Queue,\n    stats_tx: multiprocessing.Pipe,\n    duplicated_ids: set,\n    hash_seed: int = 42,\n    hash_read_name: bool = True,\n):\n    \"\"\"\n    Args:\n     inq (multiprocessing.SimpleQueue): Input queue for reads to be deduplicated.\n     outq (multiprocessing.SimpleQueue): Output queue for deduplicated reads.\n     duplicated_ids (set): Hashed read ids to be removed if encountered.\n     statq (multiprocessing.Queue, optional): Output queue for statistics.\n     hash_seed (int, optional): Seed for xxhash algorithm. Defaults to 42.\n    \"\"\"\n\n    self.inq = inq\n    self.outq = outq\n    self.hash_seed = hash_seed\n    self.duplicated_ids = duplicated_ids\n\n    # Misc\n    self.hash_read_name = hash_read_name\n\n    # Stats\n    self.stats_tx = stats_tx\n    self.reads_total = 0\n    self.reads_unique = 0\n\n    super(ReadDuplicateRemovalProcess, self).__init__()\n</code></pre>"},{"location":"reference/capcruncher/api/deduplicate/#capcruncher.api.deduplicate.ReadDuplicateRemovalProcess.run","title":"<code>run()</code>","text":"<p>Performs read deduplication based on sequence.</p> <p>Unique reads are placed on outq and deduplication stats are placed on statq.</p> Source code in <code>capcruncher/api/deduplicate.py</code> <pre><code>def run(self):\n\n    \"\"\"Performs read deduplication based on sequence.\n\n    Unique reads are placed on outq and deduplication stats are placed on statq.\n\n    \"\"\"\n\n    hash_seed = self.hash_seed\n    hash_read_name = self.hash_read_name\n    hash_function = functools.partial(xxhash.xxh64_intdigest, seed=hash_seed)\n    duplicated_ids = self.duplicated_ids\n    reads_unique = list()\n\n    while True:\n\n        try:\n            reads = self.inq.get(block=True, timeout=0.01)\n\n            if reads:\n                for read_glob in reads:\n\n                    hash_id = hash_function(\"\".join([r.name for r in read_glob]))\n\n                    if hash_id not in duplicated_ids:\n                        if hash_read_name:\n                            for r in read_glob:\n                                r.name = str(hash_function(r.name))\n\n                        reads_unique.append(read_glob)\n\n                self.reads_total += len(reads)\n                self.reads_unique += len(reads_unique)\n                self.outq.put(reads_unique.copy())\n                reads_unique.clear()\n\n            else:\n                break\n\n        except queue.Empty:\n            continue\n\n    stats = RemovalStatistics(\n        self.reads_total, self.reads_unique, self.reads_total - self.reads_unique\n    )\n    self.stats_tx.send(stats)\n</code></pre>"},{"location":"reference/capcruncher/api/filter/","title":"filter","text":""},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter","title":"<code>CCSliceFilter</code>","text":"<p>               Bases: <code>SliceFilter</code></p> <p>Perform Capture-C slice filtering (inplace) and reporter identification.</p> <p>SliceFilter tuned specifically for Capture-C data. This class has addtional methods to remove common artifacts in Capture-C data i.e. multi-capture fragments, non-reporter fragments, multi-capture reporters. The default filter order is as follows:</p> <ul> <li>remove_unmapped_slices</li> <li>remove_orphan_slices</li> <li>remove_multi_capture_fragments</li> <li>remove_excluded_slices</li> <li>remove_blacklisted_slices</li> <li>remove_non_reporter_fragments</li> <li>remove_viewpoint_adjacent_restriction_fragments</li> <li>remove_slices_without_re_frag_assigned</li> <li>remove_duplicate_re_frags</li> <li>remove_duplicate_slices</li> <li>remove_duplicate_slices_pe</li> <li>remove_non_reporter_fragments</li> </ul> <p>See the individual methods for further details.</p> <p>Attributes:</p> Name Type Description <code>slices</code> <code>DataFrame</code> <p>Annotated slices dataframe.</p> <code>fragments</code> <code>DataFrame</code> <p>Slices dataframe aggregated by parental read.</p> <code>reporters</code> <code>DataFrame</code> <p>Slices identified as reporters.</p> <code>filter_stages</code> <code>dict</code> <p>Dictionary containg stages and a list of class methods (str) required to get to this stage.</p> <code>slice_stats</code> <code>DataFrame</code> <p>Provides slice level statistics.</p> <code>read_stats</code> <code>DataFrame</code> <p>Provides statistics of slice filtering at the parental read level.</p> <code>filter_stats</code> <code>DataFrame</code> <p>Provides statistics of read filtering.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>class CCSliceFilter(SliceFilter):\n    \"\"\"\n    Perform Capture-C slice filtering (inplace) and reporter identification.\n\n    SliceFilter tuned specifically for Capture-C data. This class has addtional methods\n    to remove common artifacts in Capture-C data i.e. multi-capture fragments,\n    non-reporter fragments, multi-capture reporters. The default filter order is as follows:\n\n     - remove_unmapped_slices\n     - remove_orphan_slices\n     - remove_multi_capture_fragments\n     - remove_excluded_slices\n     - remove_blacklisted_slices\n     - remove_non_reporter_fragments\n     - remove_viewpoint_adjacent_restriction_fragments\n     - remove_slices_without_re_frag_assigned\n     - remove_duplicate_re_frags\n     - remove_duplicate_slices\n     - remove_duplicate_slices_pe\n     - remove_non_reporter_fragments\n\n    See the individual methods for further details.\n\n    Attributes:\n     slices (pd.DataFrame): Annotated slices dataframe.\n     fragments (pd.DataFrame): Slices dataframe aggregated by parental read.\n     reporters (pd.DataFrame): Slices identified as reporters.\n     filter_stages (dict): Dictionary containg stages and a list of class methods (str) required to get to this stage.\n     slice_stats (pd.DataFrame): Provides slice level statistics.\n     read_stats (pd.DataFrame): Provides statistics of slice filtering at the parental read level.\n     filter_stats (pd.DataFrame): Provides statistics of read filtering.\n\n    \"\"\"\n\n    def __init__(self, slices, filter_stages=None, **sample_kwargs):\n        if not filter_stages:\n            filter_stages = {\n                \"pre-filtering\": [\n                    \"get_unfiltered_slices\",\n                ],\n                \"mapped\": [\n                    \"remove_unmapped_slices\",\n                ],\n                \"contains_single_capture\": [\n                    \"remove_orphan_slices\",\n                    \"remove_multi_capture_fragments\",\n                ],\n                \"contains_capture_and_reporter\": [\n                    \"remove_excluded_slices\",\n                    \"remove_blacklisted_slices\",\n                    \"remove_non_reporter_fragments\",\n                    \"remove_viewpoint_adjacent_restriction_fragments\",\n                ],\n                \"duplicate_filtered\": [\n                    \"remove_slices_without_re_frag_assigned\",\n                    \"remove_duplicate_re_frags\",\n                    \"remove_duplicate_slices\",\n                    \"remove_duplicate_slices_pe\",\n                    \"remove_non_reporter_fragments\",\n                ],\n            }\n\n        super(CCSliceFilter, self).__init__(slices, filter_stages, **sample_kwargs)\n\n    @property\n    def fragments(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Summarises slices at the fragment level.\n\n        Uses pandas groupby to aggregate slices by their parental read name\n        (shared by all slices from the same fragment). Also determines the\n        number of reporter slices for each fragment.\n\n        Returns:\n         pd.DataFrame: Slices aggregated by parental read name.\n\n        \"\"\"\n\n        df = (\n            self.slices.sort_values([\"parent_read\", \"chrom\", \"start\"])\n            .groupby(\"parent_read\", as_index=False, sort=False)\n            .agg(\n                unique_slices=(\"slice\", \"nunique\"),\n                pe=(\"pe\", \"first\"),\n                mapped=(\"mapped\", \"sum\"),\n                multimapped=(\"multimapped\", \"sum\"),\n                unique_capture_sites=(\"capture\", \"nunique\"),\n                capture_count=(\"capture_count\", \"sum\"),\n                unique_exclusions=(\"exclusion\", \"nunique\"),\n                exclusion_count=(\"exclusion_count\", \"sum\"),\n                unique_restriction_fragments=(\"restriction_fragment\", \"nunique\"),\n                blacklist=(\"blacklist\", \"sum\"),\n                coordinates=(\"coordinates\", \"|\".join),\n            )\n        )\n\n        # Add the number of reporters to the dataframe.\n        # Only consider a reporter if at least one capture slice is present\n        # in the fragment.\n        df[\"reporter_count\"] = np.where(\n            df[\"capture_count\"] &gt; 0,\n            df[\"mapped\"]\n            - (df[\"exclusion_count\"] + df[\"capture_count\"] + df[\"blacklist\"]),\n            0,\n        )\n\n        return df\n\n    @property\n    def slice_stats(self) -&gt; SliceFilterStats:\n        slices = self.slices.copy()\n        if slices.empty:  # Deal with empty dataframe i.e. no valid slices\n            for col in slices:\n                slices[col] = np.zeros((10,))\n\n        stats_df = slices.agg(\n            {\n                \"slice_name\": \"nunique\",\n                \"parent_read\": \"nunique\",\n                \"mapped\": \"sum\",\n                \"multimapped\": \"sum\",\n                \"capture_count\": lambda col: (col &gt; 0).sum(),\n                \"exclusion_count\": lambda col: (col &gt; 0).sum(),\n                \"blacklist\": \"sum\",\n            }\n        )\n\n        stats_df = stats_df.rename(\n            {\n                \"slice_name\": \"unique_slices\",\n                \"parent_read\": \"unique_fragments\",\n                \"multimapped\": \"multimapping_slices\",\n                \"capture_count\": \"number_of_capture_slices\",\n                \"exclusion_count\": \"number_of_slices_in_exclusion_region\",\n                \"blacklist\": \"number_of_slices_in_blacklisted_region\",\n            }\n        )\n\n        return SliceFilterStats.from_slice_stats_dataframe(\n            stats_df,\n            stage=self.current_stage,\n            sample=self.sample_name,\n            read_type=self.read_type,\n        )\n\n    @property\n    def frag_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Statistics aggregated at the fragment level.\n\n        As this involves slice aggregation it can be rather slow\n        for large datasets. It is recomended to only use this\n        property if it is required.\n\n\n        Returns:\n         pd.DataFrame: Fragment level statistics\n        \"\"\"\n\n        return self.fragments.agg(\n            {\n                \"parent_read\": \"nunique\",\n                \"mapped\": lambda col: (col &gt; 1).sum(),\n                \"multimapped\": lambda col: (col &gt; 0).sum(),\n                \"capture_count\": lambda col: (col &gt; 0).sum(),\n                \"exclusion_count\": lambda col: (col &gt; 0).sum(),\n                \"blacklisted_slices\": lambda col: (col &gt; 0).sum(),\n                \"reporter_count\": lambda col: (col &gt; 0).sum(),\n            }\n        ).rename(\n            {\n                \"parent_read\": \"unique_fragments\",\n                \"multimapped\": \"fragments_with_multimapping_slices\",\n                \"capture_count\": \"fragments_with_capture_sites\",\n                \"exclusion_count\": \"fragments_with_excluded_regions\",\n                \"blacklisted_slices\": \"fragments_with_blacklisted_regions\",\n                \"reporter_count\": \"fragments_with_reporter_slices\",\n            }\n        )\n\n    @property\n    def reporters(self) -&gt; pd.DataFrame:\n        # Return any slice with a  N/A value\n        return self.slices.query(\"capture_count &lt; 1\")\n\n    @property\n    def captures(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts capture slices from slices dataframe\n\n        i.e. slices that do not have a null capture name\n\n        Returns:\n         pd.DataFrame: Capture slices\n\n        \"\"\"\n        # Return any slice with a non N/A capture value\n        return self.slices.query(\"capture_count == 1\")\n\n    @property\n    def capture_site_stats(self) -&gt; pd.Series:\n        \"\"\"Extracts the number of unique capture sites.\"\"\"\n        return self.captures[\"capture\"].value_counts()\n\n    @property\n    def merged_captures_and_reporters(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Merges captures and reporters sharing the same parental id.\n\n        Capture slices and reporter slices with the same parental read id are\n        merged together. The prefixes 'capture' and 'reporter' are used to\n        identify slices marked as either captures or reporters.\n\n        Returns:\n         pd.DataFrame: Merged capture and reporter slices\n        \"\"\"\n\n        captures = (\n            self.captures.set_index(\"parent_read\")\n            .add_prefix(\"capture_\")\n            .rename(columns={\"capture_capture\": \"capture\"})\n        )\n\n        reporters = self.reporters.set_index(\"parent_read\").add_prefix(\"reporter_\")\n\n        # Join reporters to captures using the parent read name\n        captures_and_reporters = captures.join(reporters).reset_index()\n\n        return captures_and_reporters\n\n    @property\n    def cis_or_trans_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts reporter cis/trans statistics from slices.\n\n        Returns:\n         pd.DataFrame: Reporter cis/trans statistics\n        \"\"\"\n        cap_and_rep = self.merged_captures_and_reporters.copy()\n\n        cap_and_rep[\"cis/trans\"] = np.where(\n            cap_and_rep[\"capture_chrom\"] == cap_and_rep[\"reporter_chrom\"],\n            \"cis\",\n            \"trans\",\n        )\n\n        # Aggregate by capture site for reporting\n\n        return (\n            cap_and_rep.groupby([\"capture\", \"cis/trans\"])\n            .size()\n            .reset_index()\n            .rename(columns={\"capture\": \"viewpoint\", 0: \"count\"})\n            .assign(sample=self.sample_name, read_type=self.read_type)\n        )\n\n    def remove_non_reporter_fragments(self):\n        \"\"\"\n        Removes the fragment if it has no reporter slices present (Common)\n\n        \"\"\"\n        fragments_partial = self.slices.groupby(\"parent_id\").agg(\n            n_capture=(\"capture_count\", \"sum\"),\n            n_mapped=(\"mapped\", \"sum\"),\n            n_blacklist=(\"blacklist\", \"sum\"),\n            n_exclusions=(\"exclusion_count\", lambda ser: ser.sum()),\n        )\n\n        fragments_with_reporters = fragments_partial.query(\n            \"(n_mapped - n_capture - n_blacklist - n_exclusions) &gt; 0\"\n        )\n\n        self.slices = (\n            self.slices.set_index(\"parent_id\")\n            .loc[fragments_with_reporters.index]\n            .reset_index()\n        )\n\n    def remove_multi_capture_fragments(self):\n        \"\"\"\n        Removes double capture fragments.\n\n        All slices (i.e. the entire fragment) are removed if more than\n        one capture probe is present i.e. a double capture (V. Common)\n\n        \"\"\"\n        fragments_n_captures = self.slices.groupby(\"parent_id\")[\"capture\"].nunique()\n        single_capture_fragments = fragments_n_captures[fragments_n_captures == 1]\n\n        self.slices = (\n            self.slices.set_index(\"parent_id\")\n            .loc[single_capture_fragments.index]\n            .reset_index()\n        )\n\n    def remove_viewpoint_adjacent_restriction_fragments(self, n_adjacent: int = 1):\n        \"\"\"\n        Deals with an odd situation in which a reporter spanning two adjacent capture sites is not removed.\n\n        Example:\n         ------Capture 1----/------Capture 2------\\\n                  -----REP--------\n\n        In this case the \"reporter\" slice is not considered either a capture or exclusion.\n\n        These cases are dealt with by explicitly removing reporters on restriction fragments\n        adjacent to capture sites.\n\n        Args:\n         n_adjacent: Number of adjacent restriction fragments to remove\n\n        \"\"\"\n\n        slices_with_viewpoint = self.slices_with_viewpoint[\n            [\n                \"restriction_fragment\",\n                \"capture\",\n                \"capture_count\",\n                \"viewpoint\",\n                \"parent_id\",\n            ]\n        ]\n\n        # Create a per viewpoint dataframe of adjacent fragment ranges\n        restriction_fragments_viewpoint = (\n            self.captures.set_index(\"capture\")[\"restriction_fragment\"]\n            .drop_duplicates()\n            .reset_index()\n            .assign(\n                exclusion_start=lambda df: df[\"restriction_fragment\"] - n_adjacent,\n                exclusion_end=lambda df: df[\"restriction_fragment\"] + n_adjacent,\n            )\n        )\n\n        slices_with_viewpoint = slices_with_viewpoint.merge(\n            restriction_fragments_viewpoint[\n                [\"capture\", \"exclusion_start\", \"exclusion_end\"]\n            ],\n            left_on=\"viewpoint\",\n            right_on=\"capture\",\n        )\n\n        # Mark slices between the exclusion zones but ignore capture slices\n        excluded_slices = slices_with_viewpoint.query(\n            \"(exclusion_start &lt;= restriction_fragment &lt;= exclusion_end) and (capture_count == 0)\"\n        )\n\n        self.slices = self.slices.loc[\n            lambda df: ~df[\"parent_id\"].isin(excluded_slices[\"parent_id\"])\n        ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.capture_site_stats","title":"<code>capture_site_stats</code>  <code>property</code>","text":"<p>Extracts the number of unique capture sites.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.captures","title":"<code>captures</code>  <code>property</code>","text":"<p>Extracts capture slices from slices dataframe</p> <p>i.e. slices that do not have a null capture name</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Capture slices</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.cis_or_trans_stats","title":"<code>cis_or_trans_stats</code>  <code>property</code>","text":"<p>Extracts reporter cis/trans statistics from slices.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Reporter cis/trans statistics</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.frag_stats","title":"<code>frag_stats</code>  <code>property</code>","text":"<p>Statistics aggregated at the fragment level.</p> <p>As this involves slice aggregation it can be rather slow for large datasets. It is recomended to only use this property if it is required.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Fragment level statistics</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.fragments","title":"<code>fragments</code>  <code>property</code>","text":"<p>Summarises slices at the fragment level.</p> <p>Uses pandas groupby to aggregate slices by their parental read name (shared by all slices from the same fragment). Also determines the number of reporter slices for each fragment.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Slices aggregated by parental read name.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.merged_captures_and_reporters","title":"<code>merged_captures_and_reporters</code>  <code>property</code>","text":"<p>Merges captures and reporters sharing the same parental id.</p> <p>Capture slices and reporter slices with the same parental read id are merged together. The prefixes 'capture' and 'reporter' are used to identify slices marked as either captures or reporters.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Merged capture and reporter slices</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.remove_multi_capture_fragments","title":"<code>remove_multi_capture_fragments()</code>","text":"<p>Removes double capture fragments.</p> <p>All slices (i.e. the entire fragment) are removed if more than one capture probe is present i.e. a double capture (V. Common)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_multi_capture_fragments(self):\n    \"\"\"\n    Removes double capture fragments.\n\n    All slices (i.e. the entire fragment) are removed if more than\n    one capture probe is present i.e. a double capture (V. Common)\n\n    \"\"\"\n    fragments_n_captures = self.slices.groupby(\"parent_id\")[\"capture\"].nunique()\n    single_capture_fragments = fragments_n_captures[fragments_n_captures == 1]\n\n    self.slices = (\n        self.slices.set_index(\"parent_id\")\n        .loc[single_capture_fragments.index]\n        .reset_index()\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.remove_non_reporter_fragments","title":"<code>remove_non_reporter_fragments()</code>","text":"<p>Removes the fragment if it has no reporter slices present (Common)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_non_reporter_fragments(self):\n    \"\"\"\n    Removes the fragment if it has no reporter slices present (Common)\n\n    \"\"\"\n    fragments_partial = self.slices.groupby(\"parent_id\").agg(\n        n_capture=(\"capture_count\", \"sum\"),\n        n_mapped=(\"mapped\", \"sum\"),\n        n_blacklist=(\"blacklist\", \"sum\"),\n        n_exclusions=(\"exclusion_count\", lambda ser: ser.sum()),\n    )\n\n    fragments_with_reporters = fragments_partial.query(\n        \"(n_mapped - n_capture - n_blacklist - n_exclusions) &gt; 0\"\n    )\n\n    self.slices = (\n        self.slices.set_index(\"parent_id\")\n        .loc[fragments_with_reporters.index]\n        .reset_index()\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.CCSliceFilter.remove_viewpoint_adjacent_restriction_fragments","title":"<code>remove_viewpoint_adjacent_restriction_fragments(n_adjacent=1)</code>","text":"<p>Deals with an odd situation in which a reporter spanning two adjacent capture sites is not removed.</p> Example <p>------Capture 1----/------Capture 2------                  -----REP--------</p> <p>In this case the \"reporter\" slice is not considered either a capture or exclusion.</p> <p>These cases are dealt with by explicitly removing reporters on restriction fragments adjacent to capture sites.</p> <p>Parameters:</p> Name Type Description Default <code>n_adjacent</code> <code>int</code> <p>Number of adjacent restriction fragments to remove</p> <code>1</code> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_viewpoint_adjacent_restriction_fragments(self, n_adjacent: int = 1):\n    \"\"\"\n    Deals with an odd situation in which a reporter spanning two adjacent capture sites is not removed.\n\n    Example:\n     ------Capture 1----/------Capture 2------\\\n              -----REP--------\n\n    In this case the \"reporter\" slice is not considered either a capture or exclusion.\n\n    These cases are dealt with by explicitly removing reporters on restriction fragments\n    adjacent to capture sites.\n\n    Args:\n     n_adjacent: Number of adjacent restriction fragments to remove\n\n    \"\"\"\n\n    slices_with_viewpoint = self.slices_with_viewpoint[\n        [\n            \"restriction_fragment\",\n            \"capture\",\n            \"capture_count\",\n            \"viewpoint\",\n            \"parent_id\",\n        ]\n    ]\n\n    # Create a per viewpoint dataframe of adjacent fragment ranges\n    restriction_fragments_viewpoint = (\n        self.captures.set_index(\"capture\")[\"restriction_fragment\"]\n        .drop_duplicates()\n        .reset_index()\n        .assign(\n            exclusion_start=lambda df: df[\"restriction_fragment\"] - n_adjacent,\n            exclusion_end=lambda df: df[\"restriction_fragment\"] + n_adjacent,\n        )\n    )\n\n    slices_with_viewpoint = slices_with_viewpoint.merge(\n        restriction_fragments_viewpoint[\n            [\"capture\", \"exclusion_start\", \"exclusion_end\"]\n        ],\n        left_on=\"viewpoint\",\n        right_on=\"capture\",\n    )\n\n    # Mark slices between the exclusion zones but ignore capture slices\n    excluded_slices = slices_with_viewpoint.query(\n        \"(exclusion_start &lt;= restriction_fragment &lt;= exclusion_end) and (capture_count == 0)\"\n    )\n\n    self.slices = self.slices.loc[\n        lambda df: ~df[\"parent_id\"].isin(excluded_slices[\"parent_id\"])\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter","title":"<code>SliceFilter</code>","text":"<p>Perform slice filtering (inplace) and reporter identification.</p> <p>The SliceFilter classes e.g. CCSliceFilter, TriCSliceFilter, TiledCSliceFilter perform all of the filtering (inplace) and reporter identification whilst also providing statistics of the numbers of slices/reads removed at each stage.</p> <p>Attributes:</p> Name Type Description <code>slices</code> <code>DataFrame</code> <p>Annotated slices dataframe.</p> <code>fragments</code> <code>DataFrame</code> <p>Slices dataframe aggregated by parental read.</p> <code>reporters</code> <code>DataFrame</code> <p>Slices identified as reporters.</p> <code>filter_stages</code> <code>dict</code> <p>Dictionary containg stages and a list of class methods (str) required to get to this stage.</p> <code>slice_stats</code> <code>DataFrame</code> <p>Provides slice level statistics.</p> <code>read_stats</code> <code>DataFrame</code> <p>Provides statistics of slice filtering at the parental read level.</p> <code>filter_stats</code> <code>DataFrame</code> <p>Provides statistics of read filtering.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>class SliceFilter:\n\n    \"\"\"\n    Perform slice filtering (inplace) and reporter identification.\n\n    The SliceFilter classes e.g. CCSliceFilter, TriCSliceFilter, TiledCSliceFilter\n    perform all of the filtering (inplace) and reporter identification whilst also\n    providing statistics of the numbers of slices/reads removed at each stage.\n\n    Attributes:\n     slices (pd.DataFrame): Annotated slices dataframe.\n     fragments (pd.DataFrame): Slices dataframe aggregated by parental read.\n     reporters (pd.DataFrame): Slices identified as reporters.\n     filter_stages (dict): Dictionary containg stages and a list of class methods (str) required to get to this stage.\n     slice_stats (pd.DataFrame): Provides slice level statistics.\n     read_stats (pd.DataFrame): Provides statistics of slice filtering at the parental read level.\n     filter_stats (pd.DataFrame): Provides statistics of read filtering.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        slices: pd.DataFrame,\n        filter_stages: dict = None,\n        sample_name: str = \"\",\n        read_type: str = \"\",\n    ):\n        \"\"\"\n        Base for all slice filter objects.\n\n        Slices DataFrame must have the following columns:\n\n         - slice_name: Unique aligned read identifier (e.g. XZKG:889:11|flashed|1)\n         - parent_read: Identifier shared by slices from same fragment (e.g.XZKG:889:11)\n         - pe: Read combined by FLASh or not (i.e. \"flashed\" or \"pe\")\n         - mapped: Alignment is mapped (e.g. 0/1)\n         - multimapped: Alignment is mapped (e.g. 0/1)\n         - slice: Slice number (e.g. 0)\n         - chrom: Chromosome e.g. chr1\n         - start: Start coord\n         - end: End coord\n         - capture: Capture site intersecting slice (e.g. Slc25A37)\n         - capture_count: Number of capture probes overlapping slice (e.g. 1)\n         - exclusion: Read present in excluded region (e.g. Slc25A37)\n         - exclusion_count: Number of excluded regions overlapping slice (e.g. 1)\n         - blacklist: Read present in excluded region (e.g. 0)\n         - coordinates: Genome coordinates (e.g. chr1:1000-2000)\n\n        Filtering to be performed can be left as the default (all start with 'remove')\n        or a custom filtering order can be supplied with a yaml file. This must have the format:\n\n         FILTER_STAGE_NAME:\n             - FILTER 1\n             - FILTER 2\n         FILTER_STAGE_NAME2:\n             - FILTER 3\n             - FILTER 1\n\n\n        *All* filters present in the file must be defined within the SliceFilter class.\n\n\n        Args:\n         slices (pd.DataFrame): DatFrame containing annotated slices\n         filter_stages (dict, optional): Dictionary defining order of slice filtering. Defaults to None.\n         sample_name (str, optional): Name of sample being processed e.g. DOX-treated_1. Defaults to \"\".\n         read_type (str, optional): Combined (flashed) or not-combined (pe). Defaults to \"\".\n\n        Raises:\n         ValueError: Filter stages must be provided. This is done automatically by all subclasses\n         AttributeError: All filters must be defined in the SliceFilter.\n        \"\"\"\n\n        # Validate the slices dataframe\n        slices = DataFrame[SlicesDataFrameSchema](slices)\n\n        # Tweak format slices dataframe to be consistent\n        self.slices = slices.sort_values([\"parent_read\", \"slice\"]).assign(\n            blacklist=lambda df: df[\"blacklist\"].astype(float),\n            restriction_fragment=lambda df: df[\"restriction_fragment\"].astype(\n                pd.Int64Dtype()\n            ),\n            capture_count=lambda df: df[\"capture_count\"].fillna(0),\n            exclusion_count=lambda df: df[\"exclusion_count\"].fillna(0),\n        )\n\n        if filter_stages:\n            self.filter_stages = self._extract_filter_stages(filter_stages)\n        else:\n            raise ValueError(\"Filter stages not provided\")\n\n        self.filtering_stats = []\n        self.sample_name = sample_name\n        self.read_type = read_type\n        self.current_filter = \"\"\n        self.current_stage = \"\"\n\n    def _extract_filter_stages(self, filter_stages) -&gt; dict:\n        \"\"\"\n        Extracts filter stages from a supplied dictionary or yaml file\n\n        Checks that the filters provided are within the dictionary supplied.\n        \"\"\"\n\n        if isinstance(filter_stages, dict):\n            filters = filter_stages\n\n        elif os.path.exists(filter_stages) and (\n            \".yaml\" in filter_stages or \".yml\" in filter_stages\n        ):\n            import yaml\n\n            with open(filter_stages, \"r\") as f:\n                filters = yaml.safe_load(f)\n\n        else:\n            raise ValueError(\n                \"Provide either a path to a .yaml file or a python dictionary\"\n            )\n\n        all_filters = itertools.chain.from_iterable(filters.values())\n\n        for filt in all_filters:\n            if filt not in self.filters:\n                raise AttributeError(\n                    f\"Required filter: {filt} not present. Check for correct spelling and format.\"\n                )\n\n        return filters\n\n    @property\n    def filters(self) -&gt; list:\n        \"\"\"A list of the callable filters present within the slice filterer instance.\n\n        Returns:\n            list: All filters present in the class.\n        \"\"\"\n        filters = [attr for attr in dir(self) if \"remove_\" in attr]\n\n        # There is at least one filter not indicated by remove\n        # Need to append to the filter list.\n        filters.append(\"get_unfiltered_slices\")\n\n        return filters\n\n    @property\n    def slice_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Statistics at the slice level.\n\n        Returns:\n         pd.DataFrame: Statistics per slice.\n        \"\"\"\n        raise NotImplementedError(\"Override this method\")\n\n    @property\n    def filter_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Statistics for each filter stage.\n\n        Returns:\n         pd.DataFrame: Statistics of the number of slices removed at each stage.\n        \"\"\"\n        return (\n            self._filter_stats.transpose()\n            .reset_index()\n            .rename(columns={\"index\": \"stage\"})\n            .assign(sample=self.sample_name, read_type=self.read_type)\n        )\n\n    @property\n    def read_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Gets statistics at a read level.\n\n        Aggregates slices by parental read id and calculates stats.\n\n        Returns:\n         pd.DataFrame: Statistics of the slices/fragments removed aggregated by read id.\n        \"\"\"\n        return self.filter_stats.rename(\n            columns={\n                \"stage\": \"stat_type\",\n                \"unique_fragments\": \"stat\",\n            }\n        )[[\"stat_type\", \"stat\"]].assign(\n            stage=\"ccanalysis\",\n            read_type=self.read_type,\n            sample=self.sample_name,\n            read_number=0,\n        )\n\n    @property\n    def fragments(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Summarises slices at the fragment level.\n\n        Uses pandas groupby to aggregate slices by their parental read name\n        (shared by all slices from the same fragment). Also determines the\n        number of reporter slices for each fragment.\n\n        Returns:\n         pd.DataFrame: Slices aggregated by parental read name.\n\n        \"\"\"\n        raise NotImplementedError(\"Override this property\")\n\n    @property\n    def captures(self) -&gt; pd.DataFrame:\n        raise NotImplementedError(\"Override this property\")\n\n    @property\n    def reporters(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts reporter slices from slices dataframe i.e. non-capture slices\n\n        Returns:\n         pd.DataFrame: All non-capture slices\n\n        \"\"\"\n        raise NotImplementedError(\"Override this property\")\n\n    def filter_slices(self, output_slices=False, output_location=\".\"):\n        \"\"\"\n        Performs slice filtering.\n\n        Filters are applied to the slices dataframe in the order specified by\n        filter_stages. Filtering stats aggregated at the slice and fragment level\n        are also printed.\n\n        Args:\n         output_slices (bool, optional): Determines if slices are to be output to a specified location after each filtering step.\n                                         Useful for debugging. Defaults to False.\n         output_location (str, optional): Location to output slices at each stage. Defaults to \".\".\n        \"\"\"\n\n        for stage, filters in self.filter_stages.items():\n            self.current_stage = stage\n\n            for filt in filters:\n                try:\n                    self.current_filter = filt\n                    # Call all of the filters in the filter_stages dict in order\n                    logger.info(f\"Filtering slices: {filt}\")\n                    getattr(self, filt)()  # Gets and calls the selected method\n                    logger.info(f\"Completed: {filt}\")\n                    logger.info(f\"Number of slices: {self.slices.shape[0]}\")\n                    logger.info(\n                        f'Number of reads: {self.slices[\"parent_read\"].nunique()}'\n                    )\n                except Exception as e:\n                    logger.error(f\"Exception {e} raised during {filt} filtering\")\n                    raise e\n\n                if output_slices == \"filter\":\n                    self.slices.to_csv(os.path.join(output_location, f\"{filt}.tsv.gz\"))\n\n            if output_slices == \"stage\":\n                self.slices.to_csv(os.path.join(output_location, f\"{stage}.tsv.gz\"))\n\n            self.filtering_stats.append(self.slice_stats)\n\n    def get_unfiltered_slices(self):\n        \"\"\"\n        Does not modify slices.\n        \"\"\"\n        self.slices = self.slices\n\n    def remove_unmapped_slices(self):\n        \"\"\"\n        Removes slices marked as unmapped (Uncommon)\n        \"\"\"\n        self.slices = self.slices.query(\"mapped == 1\")\n\n    def remove_orphan_slices(self):\n        \"\"\"Remove fragments with only one aligned slice (Common)\"\"\"\n\n        # fragments = self.fragments\n        # fragments_multislice = fragments.query(\"unique_slices &gt; 1\")\n        # self.slices = self.slices[\n        #     self.slices[\"parent_read\"].isin(fragments_multislice[\"parent_read\"])\n        # ]\n\n        not_orphan = self.slices[\"parent_id\"].duplicated(keep=False)\n        self.slices = self.slices.loc[not_orphan]\n\n    def remove_duplicate_re_frags(self):\n        r\"\"\"\n        Prevent the same restriction fragment being counted more than once (Uncommon).\n\n        Example:\n\n         --RE_FRAG1--\\----Capture----\\---RE_FRAG1----\n\n        \"\"\"\n        self.slices = self.slices.drop_duplicates(\n            subset=[\"parent_read\", \"restriction_fragment\"]\n        )\n\n    def remove_slices_without_re_frag_assigned(self):\n        \"\"\"Removes slices if restriction_fragment column is N/A\"\"\"\n        self.slices = self.slices.query('restriction_fragment != \".\"')\n\n    def remove_duplicate_slices(self):\n        \"\"\"\n        Remove all slices if the slice coordinates and slice order are shared.\n\n        This method is designed to remove a fragment if it is a PCR duplicate\n        (Common).\n\n        Example:\n\n         | Frag 1:  chr1:1000-1250 chr1:1500-1750\n         | Frag 2:  chr1:1000-1250 chr1:1500-1750\n         | Frag 3:  chr1:1050-1275 chr1:1600-1755\n         | Frag 4:  chr1:1500-1750 chr1:1000-1250\n\n         Frag 2 removed. Frag 1,3,4 retained\n\n\n        \"\"\"\n        frags_deduplicated = (\n            self.slices.groupby(\"parent_id\")\n            .agg(coords=(\"coordinates\", \"|\".join))\n            .reset_index()\n            .drop_duplicates(subset=\"coords\", keep=\"first\")\n        )\n\n        self.slices = self.slices.loc[\n            self.slices[\"parent_id\"].isin(frags_deduplicated[\"parent_id\"])\n        ]\n\n    def remove_duplicate_slices_pe(self):\n        \"\"\"\n        Removes PCR duplicates from non-flashed (PE) fragments (Common).\n\n        Sequence quality is often lower at the 3' end of reads leading to variance\n        in mapping coordinates.  PCR duplicates are removed by checking that the\n        fragment start and end are not duplicated in the dataframe.\n\n        \"\"\"\n        if (\n            self.slices[\"pe\"].iloc[:100].str.contains(\"pe\").sum() &gt; 1\n        ):  # at least one un-flashed\n            fragments_partial = (\n                self.slices.groupby(\"parent_id\")\n                .agg(coords=(\"coordinates\", \"|\".join))\n                .reset_index()\n            )\n\n            fragments_partial = fragments_partial.assign(\n                read_start=lambda df: df[\"coords\"]\n                .str.split(\"|\")\n                .str[0]\n                .str.split(r\":|-\")\n                .str[1],\n                read_end=lambda df: df[\"coords\"]\n                .str.split(\"|\")\n                .str[-1]\n                .str.split(r\":|-\")\n                .str[-1],\n            )\n\n            fragments_deduplicated = fragments_partial.drop_duplicates(\n                subset=[\"read_start\", \"read_end\"]\n            )\n\n            self.slices = (\n                self.slices.set_index(\"parent_id\")\n                .loc[fragments_deduplicated[\"parent_id\"]]\n                .reset_index()\n            )\n\n    def remove_excluded_slices(self):\n        \"\"\"Removes any slices in the exclusion region (default 1kb) (V. Common)\"\"\"\n\n        slices_with_viewpoint = self.slices_with_viewpoint\n        slices_passed = slices_with_viewpoint.loc[\n            lambda df: (df[\"exclusion_count\"] &lt; 1)\n            | (df[\"exclusion\"] != df[\"viewpoint\"])\n        ]\n\n        self.slices = self.slices.loc[\n            lambda df: df[\"parent_id\"].isin(slices_passed[\"parent_id\"])\n        ]\n\n    def remove_blacklisted_slices(self):\n        \"\"\"Removes slices marked as being within blacklisted regions\"\"\"\n        self.slices = self.slices.loc[\n            lambda df: (df[\"blacklist\"] == 0) | (df[\"blacklist\"].isna())\n        ]\n\n    @property\n    def slices_with_viewpoint(self):\n        slices = self.slices.set_index(\"parent_id\")\n        captures = self.captures.set_index(\"parent_id\")\n        return (\n            slices.join(captures[\"capture\"], lsuffix=\"_slices\", rsuffix=\"_capture\")\n            .rename(\n                columns={\"capture_slices\": \"capture\", \"capture_capture\": \"viewpoint\"}\n            )\n            .reset_index()\n        )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.filter_stats","title":"<code>filter_stats</code>  <code>property</code>","text":"<p>Statistics for each filter stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Statistics of the number of slices removed at each stage.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.filters","title":"<code>filters</code>  <code>property</code>","text":"<p>A list of the callable filters present within the slice filterer instance.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>All filters present in the class.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.fragments","title":"<code>fragments</code>  <code>property</code>","text":"<p>Summarises slices at the fragment level.</p> <p>Uses pandas groupby to aggregate slices by their parental read name (shared by all slices from the same fragment). Also determines the number of reporter slices for each fragment.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Slices aggregated by parental read name.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.read_stats","title":"<code>read_stats</code>  <code>property</code>","text":"<p>Gets statistics at a read level.</p> <p>Aggregates slices by parental read id and calculates stats.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Statistics of the slices/fragments removed aggregated by read id.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.reporters","title":"<code>reporters</code>  <code>property</code>","text":"<p>Extracts reporter slices from slices dataframe i.e. non-capture slices</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: All non-capture slices</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.slice_stats","title":"<code>slice_stats</code>  <code>property</code>","text":"<p>Statistics at the slice level.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Statistics per slice.</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.__init__","title":"<code>__init__(slices, filter_stages=None, sample_name='', read_type='')</code>","text":"<p>Base for all slice filter objects.</p> <p>Slices DataFrame must have the following columns:</p> <ul> <li>slice_name: Unique aligned read identifier (e.g. XZKG:889:11|flashed|1)</li> <li>parent_read: Identifier shared by slices from same fragment (e.g.XZKG:889:11)</li> <li>pe: Read combined by FLASh or not (i.e. \"flashed\" or \"pe\")</li> <li>mapped: Alignment is mapped (e.g. 0/1)</li> <li>multimapped: Alignment is mapped (e.g. 0/1)</li> <li>slice: Slice number (e.g. 0)</li> <li>chrom: Chromosome e.g. chr1</li> <li>start: Start coord</li> <li>end: End coord</li> <li>capture: Capture site intersecting slice (e.g. Slc25A37)</li> <li>capture_count: Number of capture probes overlapping slice (e.g. 1)</li> <li>exclusion: Read present in excluded region (e.g. Slc25A37)</li> <li>exclusion_count: Number of excluded regions overlapping slice (e.g. 1)</li> <li>blacklist: Read present in excluded region (e.g. 0)</li> <li>coordinates: Genome coordinates (e.g. chr1:1000-2000)</li> </ul> <p>Filtering to be performed can be left as the default (all start with 'remove') or a custom filtering order can be supplied with a yaml file. This must have the format:</p> <p>FILTER_STAGE_NAME:      - FILTER 1      - FILTER 2  FILTER_STAGE_NAME2:      - FILTER 3      - FILTER 1</p> <p>All filters present in the file must be defined within the SliceFilter class.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>DataFrame</code> <p>DatFrame containing annotated slices</p> required <code>filter_stages</code> <code>dict</code> <p>Dictionary defining order of slice filtering. Defaults to None.</p> <code>None</code> <code>sample_name</code> <code>str</code> <p>Name of sample being processed e.g. DOX-treated_1. Defaults to \"\".</p> <code>''</code> <code>read_type</code> <code>str</code> <p>Combined (flashed) or not-combined (pe). Defaults to \"\".</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Filter stages must be provided. This is done automatically by all subclasses</p> <code>AttributeError</code> <p>All filters must be defined in the SliceFilter.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def __init__(\n    self,\n    slices: pd.DataFrame,\n    filter_stages: dict = None,\n    sample_name: str = \"\",\n    read_type: str = \"\",\n):\n    \"\"\"\n    Base for all slice filter objects.\n\n    Slices DataFrame must have the following columns:\n\n     - slice_name: Unique aligned read identifier (e.g. XZKG:889:11|flashed|1)\n     - parent_read: Identifier shared by slices from same fragment (e.g.XZKG:889:11)\n     - pe: Read combined by FLASh or not (i.e. \"flashed\" or \"pe\")\n     - mapped: Alignment is mapped (e.g. 0/1)\n     - multimapped: Alignment is mapped (e.g. 0/1)\n     - slice: Slice number (e.g. 0)\n     - chrom: Chromosome e.g. chr1\n     - start: Start coord\n     - end: End coord\n     - capture: Capture site intersecting slice (e.g. Slc25A37)\n     - capture_count: Number of capture probes overlapping slice (e.g. 1)\n     - exclusion: Read present in excluded region (e.g. Slc25A37)\n     - exclusion_count: Number of excluded regions overlapping slice (e.g. 1)\n     - blacklist: Read present in excluded region (e.g. 0)\n     - coordinates: Genome coordinates (e.g. chr1:1000-2000)\n\n    Filtering to be performed can be left as the default (all start with 'remove')\n    or a custom filtering order can be supplied with a yaml file. This must have the format:\n\n     FILTER_STAGE_NAME:\n         - FILTER 1\n         - FILTER 2\n     FILTER_STAGE_NAME2:\n         - FILTER 3\n         - FILTER 1\n\n\n    *All* filters present in the file must be defined within the SliceFilter class.\n\n\n    Args:\n     slices (pd.DataFrame): DatFrame containing annotated slices\n     filter_stages (dict, optional): Dictionary defining order of slice filtering. Defaults to None.\n     sample_name (str, optional): Name of sample being processed e.g. DOX-treated_1. Defaults to \"\".\n     read_type (str, optional): Combined (flashed) or not-combined (pe). Defaults to \"\".\n\n    Raises:\n     ValueError: Filter stages must be provided. This is done automatically by all subclasses\n     AttributeError: All filters must be defined in the SliceFilter.\n    \"\"\"\n\n    # Validate the slices dataframe\n    slices = DataFrame[SlicesDataFrameSchema](slices)\n\n    # Tweak format slices dataframe to be consistent\n    self.slices = slices.sort_values([\"parent_read\", \"slice\"]).assign(\n        blacklist=lambda df: df[\"blacklist\"].astype(float),\n        restriction_fragment=lambda df: df[\"restriction_fragment\"].astype(\n            pd.Int64Dtype()\n        ),\n        capture_count=lambda df: df[\"capture_count\"].fillna(0),\n        exclusion_count=lambda df: df[\"exclusion_count\"].fillna(0),\n    )\n\n    if filter_stages:\n        self.filter_stages = self._extract_filter_stages(filter_stages)\n    else:\n        raise ValueError(\"Filter stages not provided\")\n\n    self.filtering_stats = []\n    self.sample_name = sample_name\n    self.read_type = read_type\n    self.current_filter = \"\"\n    self.current_stage = \"\"\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter._extract_filter_stages","title":"<code>_extract_filter_stages(filter_stages)</code>","text":"<p>Extracts filter stages from a supplied dictionary or yaml file</p> <p>Checks that the filters provided are within the dictionary supplied.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def _extract_filter_stages(self, filter_stages) -&gt; dict:\n    \"\"\"\n    Extracts filter stages from a supplied dictionary or yaml file\n\n    Checks that the filters provided are within the dictionary supplied.\n    \"\"\"\n\n    if isinstance(filter_stages, dict):\n        filters = filter_stages\n\n    elif os.path.exists(filter_stages) and (\n        \".yaml\" in filter_stages or \".yml\" in filter_stages\n    ):\n        import yaml\n\n        with open(filter_stages, \"r\") as f:\n            filters = yaml.safe_load(f)\n\n    else:\n        raise ValueError(\n            \"Provide either a path to a .yaml file or a python dictionary\"\n        )\n\n    all_filters = itertools.chain.from_iterable(filters.values())\n\n    for filt in all_filters:\n        if filt not in self.filters:\n            raise AttributeError(\n                f\"Required filter: {filt} not present. Check for correct spelling and format.\"\n            )\n\n    return filters\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.filter_slices","title":"<code>filter_slices(output_slices=False, output_location='.')</code>","text":"<p>Performs slice filtering.</p> <p>Filters are applied to the slices dataframe in the order specified by filter_stages. Filtering stats aggregated at the slice and fragment level are also printed.</p> <p>Parameters:</p> Name Type Description Default <code>output_slices</code> <code>bool</code> <p>Determines if slices are to be output to a specified location after each filtering step.                                Useful for debugging. Defaults to False.</p> <code>False</code> <code>output_location</code> <code>str</code> <p>Location to output slices at each stage. Defaults to \".\".</p> <code>'.'</code> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def filter_slices(self, output_slices=False, output_location=\".\"):\n    \"\"\"\n    Performs slice filtering.\n\n    Filters are applied to the slices dataframe in the order specified by\n    filter_stages. Filtering stats aggregated at the slice and fragment level\n    are also printed.\n\n    Args:\n     output_slices (bool, optional): Determines if slices are to be output to a specified location after each filtering step.\n                                     Useful for debugging. Defaults to False.\n     output_location (str, optional): Location to output slices at each stage. Defaults to \".\".\n    \"\"\"\n\n    for stage, filters in self.filter_stages.items():\n        self.current_stage = stage\n\n        for filt in filters:\n            try:\n                self.current_filter = filt\n                # Call all of the filters in the filter_stages dict in order\n                logger.info(f\"Filtering slices: {filt}\")\n                getattr(self, filt)()  # Gets and calls the selected method\n                logger.info(f\"Completed: {filt}\")\n                logger.info(f\"Number of slices: {self.slices.shape[0]}\")\n                logger.info(\n                    f'Number of reads: {self.slices[\"parent_read\"].nunique()}'\n                )\n            except Exception as e:\n                logger.error(f\"Exception {e} raised during {filt} filtering\")\n                raise e\n\n            if output_slices == \"filter\":\n                self.slices.to_csv(os.path.join(output_location, f\"{filt}.tsv.gz\"))\n\n        if output_slices == \"stage\":\n            self.slices.to_csv(os.path.join(output_location, f\"{stage}.tsv.gz\"))\n\n        self.filtering_stats.append(self.slice_stats)\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.get_unfiltered_slices","title":"<code>get_unfiltered_slices()</code>","text":"<p>Does not modify slices.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def get_unfiltered_slices(self):\n    \"\"\"\n    Does not modify slices.\n    \"\"\"\n    self.slices = self.slices\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_blacklisted_slices","title":"<code>remove_blacklisted_slices()</code>","text":"<p>Removes slices marked as being within blacklisted regions</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_blacklisted_slices(self):\n    \"\"\"Removes slices marked as being within blacklisted regions\"\"\"\n    self.slices = self.slices.loc[\n        lambda df: (df[\"blacklist\"] == 0) | (df[\"blacklist\"].isna())\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_duplicate_re_frags","title":"<code>remove_duplicate_re_frags()</code>","text":"<p>Prevent the same restriction fragment being counted more than once (Uncommon).</p> <p>Example:</p> <p>--RE_FRAG1------Capture-------RE_FRAG1----</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_duplicate_re_frags(self):\n    r\"\"\"\n    Prevent the same restriction fragment being counted more than once (Uncommon).\n\n    Example:\n\n     --RE_FRAG1--\\----Capture----\\---RE_FRAG1----\n\n    \"\"\"\n    self.slices = self.slices.drop_duplicates(\n        subset=[\"parent_read\", \"restriction_fragment\"]\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_duplicate_slices","title":"<code>remove_duplicate_slices()</code>","text":"<p>Remove all slices if the slice coordinates and slice order are shared.</p> <p>This method is designed to remove a fragment if it is a PCR duplicate (Common).</p> <p>Example:</p> <p>| Frag 1:  chr1:1000-1250 chr1:1500-1750  | Frag 2:  chr1:1000-1250 chr1:1500-1750  | Frag 3:  chr1:1050-1275 chr1:1600-1755  | Frag 4:  chr1:1500-1750 chr1:1000-1250</p> <p>Frag 2 removed. Frag 1,3,4 retained</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_duplicate_slices(self):\n    \"\"\"\n    Remove all slices if the slice coordinates and slice order are shared.\n\n    This method is designed to remove a fragment if it is a PCR duplicate\n    (Common).\n\n    Example:\n\n     | Frag 1:  chr1:1000-1250 chr1:1500-1750\n     | Frag 2:  chr1:1000-1250 chr1:1500-1750\n     | Frag 3:  chr1:1050-1275 chr1:1600-1755\n     | Frag 4:  chr1:1500-1750 chr1:1000-1250\n\n     Frag 2 removed. Frag 1,3,4 retained\n\n\n    \"\"\"\n    frags_deduplicated = (\n        self.slices.groupby(\"parent_id\")\n        .agg(coords=(\"coordinates\", \"|\".join))\n        .reset_index()\n        .drop_duplicates(subset=\"coords\", keep=\"first\")\n    )\n\n    self.slices = self.slices.loc[\n        self.slices[\"parent_id\"].isin(frags_deduplicated[\"parent_id\"])\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_duplicate_slices_pe","title":"<code>remove_duplicate_slices_pe()</code>","text":"<p>Removes PCR duplicates from non-flashed (PE) fragments (Common).</p> <p>Sequence quality is often lower at the 3' end of reads leading to variance in mapping coordinates.  PCR duplicates are removed by checking that the fragment start and end are not duplicated in the dataframe.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_duplicate_slices_pe(self):\n    \"\"\"\n    Removes PCR duplicates from non-flashed (PE) fragments (Common).\n\n    Sequence quality is often lower at the 3' end of reads leading to variance\n    in mapping coordinates.  PCR duplicates are removed by checking that the\n    fragment start and end are not duplicated in the dataframe.\n\n    \"\"\"\n    if (\n        self.slices[\"pe\"].iloc[:100].str.contains(\"pe\").sum() &gt; 1\n    ):  # at least one un-flashed\n        fragments_partial = (\n            self.slices.groupby(\"parent_id\")\n            .agg(coords=(\"coordinates\", \"|\".join))\n            .reset_index()\n        )\n\n        fragments_partial = fragments_partial.assign(\n            read_start=lambda df: df[\"coords\"]\n            .str.split(\"|\")\n            .str[0]\n            .str.split(r\":|-\")\n            .str[1],\n            read_end=lambda df: df[\"coords\"]\n            .str.split(\"|\")\n            .str[-1]\n            .str.split(r\":|-\")\n            .str[-1],\n        )\n\n        fragments_deduplicated = fragments_partial.drop_duplicates(\n            subset=[\"read_start\", \"read_end\"]\n        )\n\n        self.slices = (\n            self.slices.set_index(\"parent_id\")\n            .loc[fragments_deduplicated[\"parent_id\"]]\n            .reset_index()\n        )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_excluded_slices","title":"<code>remove_excluded_slices()</code>","text":"<p>Removes any slices in the exclusion region (default 1kb) (V. Common)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_excluded_slices(self):\n    \"\"\"Removes any slices in the exclusion region (default 1kb) (V. Common)\"\"\"\n\n    slices_with_viewpoint = self.slices_with_viewpoint\n    slices_passed = slices_with_viewpoint.loc[\n        lambda df: (df[\"exclusion_count\"] &lt; 1)\n        | (df[\"exclusion\"] != df[\"viewpoint\"])\n    ]\n\n    self.slices = self.slices.loc[\n        lambda df: df[\"parent_id\"].isin(slices_passed[\"parent_id\"])\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_orphan_slices","title":"<code>remove_orphan_slices()</code>","text":"<p>Remove fragments with only one aligned slice (Common)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_orphan_slices(self):\n    \"\"\"Remove fragments with only one aligned slice (Common)\"\"\"\n\n    # fragments = self.fragments\n    # fragments_multislice = fragments.query(\"unique_slices &gt; 1\")\n    # self.slices = self.slices[\n    #     self.slices[\"parent_read\"].isin(fragments_multislice[\"parent_read\"])\n    # ]\n\n    not_orphan = self.slices[\"parent_id\"].duplicated(keep=False)\n    self.slices = self.slices.loc[not_orphan]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_slices_without_re_frag_assigned","title":"<code>remove_slices_without_re_frag_assigned()</code>","text":"<p>Removes slices if restriction_fragment column is N/A</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_slices_without_re_frag_assigned(self):\n    \"\"\"Removes slices if restriction_fragment column is N/A\"\"\"\n    self.slices = self.slices.query('restriction_fragment != \".\"')\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.SliceFilter.remove_unmapped_slices","title":"<code>remove_unmapped_slices()</code>","text":"<p>Removes slices marked as unmapped (Uncommon)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_unmapped_slices(self):\n    \"\"\"\n    Removes slices marked as unmapped (Uncommon)\n    \"\"\"\n    self.slices = self.slices.query(\"mapped == 1\")\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter","title":"<code>TiledCSliceFilter</code>","text":"<p>               Bases: <code>SliceFilter</code></p> <p>Perform Tiled-C slice filtering (inplace) and reporter identification.</p> <p>SliceFilter tuned specifically for Tiled-C data. This class has addtional methods to remove common artifacts in Tiled-C data i.e. non-capture fragments, multi-capture (with different tiled regions) fragments. A reporter is defined differently in a Tiled-C analysis as a reporter slice can also be a capture slice.</p> <p>The default filter order is as follows:</p> <ul> <li>remove_unmapped_slices</li> <li>remove_orphan_slices</li> <li>remove_blacklisted_slices</li> <li>remove_non_capture_fragments</li> <li>remove_dual_capture_fragments</li> <li>remove_slices_without_re_frag_assigned</li> <li>remove_duplicate_re_frags</li> <li>remove_duplicate_slices</li> <li>remove_duplicate_slices_pe</li> <li>remove_orphan_slices</li> </ul> <p>See the individual methods for further details.</p> <p>Attributes:</p> Name Type Description <code>slices</code> <code>DataFrame</code> <p>Annotated slices dataframe.</p> <code>fragments</code> <code>DataFrame</code> <p>Slices dataframe aggregated by parental read.</p> <code>reporters</code> <code>DataFrame</code> <p>Slices identified as reporters.</p> <code>filter_stages</code> <code>dict</code> <p>Dictionary containg stages and a list of class methods (str) required to get to this stage.</p> <code>slice_stats</code> <code>DataFrame</code> <p>Provides slice level statistics.</p> <code>read_stats</code> <code>DataFrame</code> <p>Provides statistics of slice filtering at the parental read level.</p> <code>filter_stats</code> <code>DataFrame</code> <p>Provides statistics of read filtering.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>class TiledCSliceFilter(SliceFilter):\n    \"\"\"\n    Perform Tiled-C slice filtering (inplace) and reporter identification.\n\n    SliceFilter tuned specifically for Tiled-C data. This class has addtional methods\n    to remove common artifacts in Tiled-C data i.e. non-capture fragments,\n    multi-capture (with different tiled regions) fragments.\n    A reporter is defined differently in a Tiled-C analysis as a reporter slice can also\n    be a capture slice.\n\n    The default filter order is as follows:\n\n     - remove_unmapped_slices\n     - remove_orphan_slices\n     - remove_blacklisted_slices\n     - remove_non_capture_fragments\n     - remove_dual_capture_fragments\n     - remove_slices_without_re_frag_assigned\n     - remove_duplicate_re_frags\n     - remove_duplicate_slices\n     - remove_duplicate_slices_pe\n     - remove_orphan_slices\n\n    See the individual methods for further details.\n\n    Attributes:\n     slices (pd.DataFrame): Annotated slices dataframe.\n     fragments (pd.DataFrame): Slices dataframe aggregated by parental read.\n     reporters (pd.DataFrame): Slices identified as reporters.\n     filter_stages (dict): Dictionary containg stages and a list of class methods (str) required to get to this stage.\n     slice_stats (pd.DataFrame): Provides slice level statistics.\n     read_stats (pd.DataFrame): Provides statistics of slice filtering at the parental read level.\n     filter_stats (pd.DataFrame): Provides statistics of read filtering.\n\n    \"\"\"\n\n    def __init__(self, slices, filter_stages=None, **sample_kwargs):\n        if not filter_stages:\n            filter_stages = {\n                \"pre-filtering\": [\n                    \"get_unfiltered_slices\",\n                ],\n                \"mapped\": [\"remove_unmapped_slices\", \"remove_orphan_slices\"],\n                \"not_blacklisted\": [\"remove_blacklisted_slices\"],\n                \"contains_capture\": [\n                    \"remove_non_capture_fragments\",\n                    \"remove_dual_capture_fragments\",\n                ],\n                \"duplicate_filtered\": [\n                    \"remove_slices_without_re_frag_assigned\",\n                    \"remove_duplicate_re_frags\",\n                    \"remove_duplicate_slices\",\n                    \"remove_duplicate_slices_pe\",\n                ],\n                \"has_reporter\": [\"remove_orphan_slices\", \"remove_religation\"],\n            }\n\n        super(TiledCSliceFilter, self).__init__(slices, filter_stages, **sample_kwargs)\n\n    @property\n    def captures(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts capture slices from slices dataframe\n\n        i.e. slices that do not have a null capture name\n\n        Returns:\n         pd.DataFrame: Capture slices\n\n        \"\"\"\n        # Return any slice with a non N/A capture value\n        return self.slices.query(\"capture_count == 1\")\n\n    @property\n    def fragments(self) -&gt; pd.DataFrame:\n        df = (\n            self.slices.sort_values([\"parent_read\", \"chrom\", \"start\"])\n            .groupby(\"parent_read\", as_index=False, sort=False)\n            .agg(\n                id=(\"parent_id\", \"first\"),\n                unique_slices=(\"slice\", \"nunique\"),\n                pe=(\"pe\", \"first\"),\n                mapped=(\"mapped\", \"sum\"),\n                multimapped=(\"multimapped\", \"sum\"),\n                capture_count=(\"capture_count\", \"sum\"),\n                unique_restriction_fragments=(\"restriction_fragment\", \"nunique\"),\n                blacklisted_slices=(\"blacklist\", \"sum\"),\n                coordinates=(\"coordinates\", \"|\".join),\n            )\n        )\n\n        return df\n\n    @property\n    def slice_stats(self):\n        slices = self.slices.copy()\n        if slices.empty:  # Deal with empty dataframe i.e. no valid slices\n            for col in slices:\n                slices[col] = np.zeros((10,))\n\n        stats_df = slices.agg(\n            {\n                \"slice_name\": \"nunique\",\n                \"parent_read\": \"nunique\",\n                \"mapped\": \"sum\",\n                \"multimapped\": \"sum\",\n                \"capture_count\": lambda col: (col &gt; 0).sum(),\n                \"blacklist\": \"sum\",\n            }\n        )\n\n        stats_df = stats_df.rename(\n            {\n                \"slice_name\": \"unique_slices\",\n                \"parent_read\": \"unique_fragments\",\n                \"multimapped\": \"multimapping_slices\",\n                \"capture_count\": \"number_of_capture_slices\",\n                \"blacklist\": \"number_of_slices_in_blacklisted_region\",\n            }\n        )\n\n        return SliceFilterStats.from_slice_stats_dataframe(\n            stats_df,\n            stage=self.current_stage,\n            sample=self.sample_name,\n            read_type=self.read_type,\n        )\n\n    @property\n    def cis_or_trans_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts reporter cis/trans statistics from slices.\n\n        Unlike Capture-C/Tri-C reporter slice can also be capture slices as\n        all slices within the capture region are considered as reporters. To extract\n        cis/trans statistics, one capture slice in each fragment is considered to be\n        the \"primary capture\" this then enables merging of this \"primary capture\" with\n        the other reporters both inside and outside of the tiled region.\n\n        Returns:\n         pd.DataFrame: Reporter cis/trans statistics\n        \"\"\"\n\n        interactions_by_capture = dict()\n\n        for capture_site, df_cap in self.slices.query(\"capture_count == 1\").groupby(\n            \"capture\"\n        ):\n            capture_chrom = df_cap.iloc[0][\"chrom\"]\n            df_primary_capture = df_cap.groupby(\n                \"parent_read\"\n            ).first()  # Artifact required as need to call one slice the \"capture\"\n            df_not_primary_capture = df_cap.loc[\n                ~(df_cap[\"slice_name\"].isin(df_primary_capture[\"slice_name\"]))\n            ]\n            df_outside_capture = self.slices.query(\"capture_count == 0\").loc[\n                lambda df_rep: df_rep[\"parent_read\"].isin(df_cap[\"parent_read\"])\n            ]\n\n            df_pseudo_reporters = pd.concat(\n                [df_not_primary_capture, df_outside_capture]\n            )\n            n_cis_interactions = df_pseudo_reporters.query(\n                f'chrom == \"{capture_chrom}\"'\n            ).shape[0]\n            n_trans_interactions = df_pseudo_reporters.shape[0] - n_cis_interactions\n\n            interactions_by_capture[capture_site] = {\n                \"cis\": n_cis_interactions,\n                \"trans\": n_trans_interactions,\n            }\n\n        return (\n            pd.DataFrame(interactions_by_capture)\n            .transpose()\n            .reset_index()\n            .rename(columns={\"index\": \"capture\"})\n            .melt(id_vars=\"capture\", var_name=\"cis/trans\", value_name=\"count\")\n            .sort_values(\"capture\")\n            .assign(sample=self.sample_name, read_type=self.read_type)\n            .rename(columns={\"capture\": \"viewpoint\"})\n        )\n\n    def remove_slices_outside_capture(self):\n        \"\"\"Removes slices outside of capture region(s)\"\"\"\n        self.slices = self.slices.query(\"capture_count != 0\")\n\n    def remove_non_capture_fragments(self):\n        \"\"\"Removes fragments without a capture assigned\"\"\"\n        fragments_with_capture = (\n            self.slices.groupby(\"parent_id\")[\"capture_count\"]\n            .sum()\n            .reset_index()\n            .query(\"capture_count &gt; 0\")\n        )\n        self.slices = self.slices[\n            self.slices[\"parent_id\"].isin(fragments_with_capture[\"parent_id\"])\n        ]\n\n    def remove_dual_capture_fragments(self):\n        \"\"\"\n        Removes a fragment with multiple different capture sites.\n\n        Modified for TiledC filtering as the fragment dataframe is generated\n        slightly differently.\n        \"\"\"\n        multicapture_fragments = (\n            self.slices.query(\"capture_count == 1\")\n            .groupby(\"parent_id\")[\"capture\"]\n            .nunique()\n            &gt; 1\n        )\n        self.slices = (\n            self.slices.set_index(\"parent_id\")\n            .loc[~multicapture_fragments]\n            .reset_index()\n        )\n\n    def remove_religation(self):\n        frag_comp = (\n            self.slices.sort_values([\"restriction_fragment\"])\n            .groupby(\"parent_id\")[\"restriction_fragment\"]\n            .transform(\"diff\")\n            .fillna(-1)\n        )\n        not_religated = (frag_comp &gt; 1) | (frag_comp == -1)\n        self.slices = self.slices.loc[not_religated]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter.captures","title":"<code>captures</code>  <code>property</code>","text":"<p>Extracts capture slices from slices dataframe</p> <p>i.e. slices that do not have a null capture name</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Capture slices</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter.cis_or_trans_stats","title":"<code>cis_or_trans_stats</code>  <code>property</code>","text":"<p>Extracts reporter cis/trans statistics from slices.</p> <p>Unlike Capture-C/Tri-C reporter slice can also be capture slices as all slices within the capture region are considered as reporters. To extract cis/trans statistics, one capture slice in each fragment is considered to be the \"primary capture\" this then enables merging of this \"primary capture\" with the other reporters both inside and outside of the tiled region.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Reporter cis/trans statistics</p>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter.remove_dual_capture_fragments","title":"<code>remove_dual_capture_fragments()</code>","text":"<p>Removes a fragment with multiple different capture sites.</p> <p>Modified for TiledC filtering as the fragment dataframe is generated slightly differently.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_dual_capture_fragments(self):\n    \"\"\"\n    Removes a fragment with multiple different capture sites.\n\n    Modified for TiledC filtering as the fragment dataframe is generated\n    slightly differently.\n    \"\"\"\n    multicapture_fragments = (\n        self.slices.query(\"capture_count == 1\")\n        .groupby(\"parent_id\")[\"capture\"]\n        .nunique()\n        &gt; 1\n    )\n    self.slices = (\n        self.slices.set_index(\"parent_id\")\n        .loc[~multicapture_fragments]\n        .reset_index()\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter.remove_non_capture_fragments","title":"<code>remove_non_capture_fragments()</code>","text":"<p>Removes fragments without a capture assigned</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_non_capture_fragments(self):\n    \"\"\"Removes fragments without a capture assigned\"\"\"\n    fragments_with_capture = (\n        self.slices.groupby(\"parent_id\")[\"capture_count\"]\n        .sum()\n        .reset_index()\n        .query(\"capture_count &gt; 0\")\n    )\n    self.slices = self.slices[\n        self.slices[\"parent_id\"].isin(fragments_with_capture[\"parent_id\"])\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TiledCSliceFilter.remove_slices_outside_capture","title":"<code>remove_slices_outside_capture()</code>","text":"<p>Removes slices outside of capture region(s)</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_slices_outside_capture(self):\n    \"\"\"Removes slices outside of capture region(s)\"\"\"\n    self.slices = self.slices.query(\"capture_count != 0\")\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TriCSliceFilter","title":"<code>TriCSliceFilter</code>","text":"<p>               Bases: <code>CCSliceFilter</code></p> <p>Perform Tri-C slice filtering (inplace) and reporter identification.</p> <p>SliceFilter tuned specifically for Tri-C data. Whilst the vast majority of filters are inherited from CCSliceFilter, this class has addtional methods for Tri-C analysis i.e. remove_slices_with_one_reporter. The default filtering order is:</p> <ul> <li>remove_unmapped_slices</li> <li>remove_slices_without_re_frag_assigned</li> <li>remove_orphan_slices</li> <li>remove_multi_capture_fragments</li> <li>remove_blacklisted_slices</li> <li>remove_non_reporter_fragments</li> <li>remove_viewpoint_adjacent_restriction_fragments</li> <li>remove_duplicate_re_frags</li> <li>remove_duplicate_slices</li> <li>remove_duplicate_slices_pe</li> <li>remove_non_reporter_fragments</li> <li>remove_slices_with_one_reporter</li> </ul> <p>See the individual methods for further details.</p> <p>Attributes:</p> Name Type Description <code>slices</code> <code>DataFrame</code> <p>Annotated slices dataframe.</p> <code>fragments</code> <code>DataFrame</code> <p>Slices dataframe aggregated by parental read.</p> <code>reporters</code> <code>DataFrame</code> <p>Slices identified as reporters.</p> <code>filter_stages</code> <code>dict</code> <p>Dictionary containg stages and a list of class methods (str) required to get to this stage.</p> <code>slice_stats</code> <code>DataFrame</code> <p>Provides slice level statistics.</p> <code>read_stats</code> <code>DataFrame</code> <p>Provides statistics of slice filtering at the parental read level.</p> <code>filter_stats</code> <code>DataFrame</code> <p>Provides statistics of read filtering.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>class TriCSliceFilter(CCSliceFilter):\n    \"\"\"\n    Perform Tri-C slice filtering (inplace) and reporter identification.\n\n    SliceFilter tuned specifically for Tri-C data. Whilst the vast majority of filters\n    are inherited from CCSliceFilter, this class has addtional methods for Tri-C analysis\n    i.e. remove_slices_with_one_reporter. The default filtering order is:\n\n     - remove_unmapped_slices\n     - remove_slices_without_re_frag_assigned\n     - remove_orphan_slices\n     - remove_multi_capture_fragments\n     - remove_blacklisted_slices\n     - remove_non_reporter_fragments\n     - remove_viewpoint_adjacent_restriction_fragments\n     - remove_duplicate_re_frags\n     - remove_duplicate_slices\n     - remove_duplicate_slices_pe\n     - remove_non_reporter_fragments\n     - remove_slices_with_one_reporter\n\n    See the individual methods for further details.\n\n    Attributes:\n     slices (pd.DataFrame): Annotated slices dataframe.\n     fragments (pd.DataFrame): Slices dataframe aggregated by parental read.\n     reporters (pd.DataFrame): Slices identified as reporters.\n     filter_stages (dict): Dictionary containg stages and a list of class methods (str) required to get to this stage.\n     slice_stats (pd.DataFrame): Provides slice level statistics.\n     read_stats (pd.DataFrame): Provides statistics of slice filtering at the parental read level.\n     filter_stats (pd.DataFrame): Provides statistics of read filtering.\"\"\"\n\n    def __init__(self, slices, filter_stages=None, **sample_kwargs):\n        if filter_stages:\n            self.filter_stages = filter_stages\n        else:\n            filter_stages = {\n                \"pre-filtering\": [\n                    \"get_unfiltered_slices\",\n                ],\n                \"mapped\": [\n                    \"remove_unmapped_slices\",\n                    \"remove_slices_without_re_frag_assigned\",\n                ],\n                \"contains_single_capture\": [\n                    \"remove_orphan_slices\",\n                    \"remove_multi_capture_fragments\",\n                ],\n                \"contains_capture_and_reporter\": [\n                    \"remove_blacklisted_slices\",\n                    \"remove_non_reporter_fragments\",\n                ],\n                \"duplicate_filtered\": [\n                    \"remove_duplicate_re_frags\",\n                    \"remove_duplicate_slices\",\n                    \"remove_duplicate_slices_pe\",\n                    \"remove_non_reporter_fragments\",\n                ],\n                \"tric_reporter\": [\"remove_slices_with_one_reporter\"],\n            }\n\n        super(TriCSliceFilter, self).__init__(slices, filter_stages, **sample_kwargs)\n\n    def remove_slices_with_one_reporter(self):\n        \"\"\"Removes fragments if they do not contain at least two reporters.\"\"\"\n        fragments_triplets = self.fragments.query(\"reporter_count &gt; 1\")\n        self.slices = self.slices.loc[\n            lambda df: df[\"parent_read\"].isin(fragments_triplets[\"parent_read\"])\n        ]\n</code></pre>"},{"location":"reference/capcruncher/api/filter/#capcruncher.api.filter.TriCSliceFilter.remove_slices_with_one_reporter","title":"<code>remove_slices_with_one_reporter()</code>","text":"<p>Removes fragments if they do not contain at least two reporters.</p> Source code in <code>capcruncher/api/filter.py</code> <pre><code>def remove_slices_with_one_reporter(self):\n    \"\"\"Removes fragments if they do not contain at least two reporters.\"\"\"\n    fragments_triplets = self.fragments.query(\"reporter_count &gt; 1\")\n    self.slices = self.slices.loc[\n        lambda df: df[\"parent_read\"].isin(fragments_triplets[\"parent_read\"])\n    ]\n</code></pre>"},{"location":"reference/capcruncher/api/io/","title":"io","text":""},{"location":"reference/capcruncher/api/io/#capcruncher.api.io.FastqReaderProcess","title":"<code>FastqReaderProcess</code>","text":"<p>               Bases: <code>Process</code></p> <p>Reads fastq file(s) in chunks and places them on a queue.</p> <p>Attributes:</p> Name Type Description <code>input_file</code> <p>Input fastq files.</p> <code>outq</code> <p>Output queue for chunked reads/read pairs.</p> <code>statq</code> <p>(Not currently used) Queue for read statistics if required.</p> <code>read_buffer</code> <p>Number of reads to process before placing them on outq</p> <code>read_counter</code> <p>(Not currently used) Can be used to sync between multiple readers.</p> <code>n_subproceses</code> <p>Number of processes running concurrently. Used to make sure enough termination signals are used.</p> Source code in <code>capcruncher/api/io.py</code> <pre><code>class FastqReaderProcess(multiprocessing.Process):\n    \"\"\"Reads fastq file(s) in chunks and places them on a queue.\n\n    Attributes:\n     input_file: Input fastq files.\n     outq: Output queue for chunked reads/read pairs.\n     statq: (Not currently used) Queue for read statistics if required.\n     read_buffer: Number of reads to process before placing them on outq\n     read_counter: (Not currently used) Can be used to sync between multiple readers.\n     n_subproceses: Number of processes running concurrently. Used to make sure enough termination signals are used.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_files: Union[str, list],\n        outq: multiprocessing.Queue,\n        read_buffer: int = 100000,\n    ) -&gt; None:\n        # Input variables\n        self.input_files = input_files\n        self._multifile = self._is_multifile(input_files)\n\n        if self._multifile:\n            self._input_files_pysam = [FastxFile(f) for f in self.input_files]\n        else:\n            self._input_files_pysam = [\n                FastxFile(self.input_files),\n            ]\n\n        # Multiprocessing variables\n        self.outq = outq\n\n        # Reader variables\n        self.read_buffer = read_buffer\n\n        super(FastqReaderProcess, self).__init__()\n\n    def _is_multifile(self, files):\n        if not isinstance(files, (str, pathlib.Path)):\n            return True\n        elif isinstance(files, (list, tuple)) and len(files &gt; 1):\n            return True\n        else:\n            return False\n\n    def run(self):\n        \"\"\"Performs reading and chunking of fastq file(s).\"\"\"\n\n        try:\n            buffer = []\n            rc = 0\n            for read_counter, read in enumerate(zip(*self._input_files_pysam)):\n                # print(f\"read_counter: {read_counter}, read: {read}, read_buffer: {self.read_buffer}\")\n                buffer.append(read)\n                if read_counter % self.read_buffer == 0 and not read_counter == 0:\n                    self.outq.put(buffer.copy())\n                    buffer.clear()\n                    logger.info(f\"{read_counter} reads parsed (batch)\")\n                    rc = read_counter\n                else:\n                    rc = read_counter\n\n            self.outq.put(buffer)  # Deal with remainder\n            self.outq.put_nowait(None)  # Poison pill to terminate queue\n            logger.info(f\"{rc} reads parsed (final)\")\n\n        except Exception as e:\n            logger.info(f\"Reader failed with exception: {e}\")\n            raise\n\n        finally:\n            for fh in self._input_files_pysam:\n                fh.close()\n</code></pre>"},{"location":"reference/capcruncher/api/io/#capcruncher.api.io.FastqReaderProcess.run","title":"<code>run()</code>","text":"<p>Performs reading and chunking of fastq file(s).</p> Source code in <code>capcruncher/api/io.py</code> <pre><code>def run(self):\n    \"\"\"Performs reading and chunking of fastq file(s).\"\"\"\n\n    try:\n        buffer = []\n        rc = 0\n        for read_counter, read in enumerate(zip(*self._input_files_pysam)):\n            # print(f\"read_counter: {read_counter}, read: {read}, read_buffer: {self.read_buffer}\")\n            buffer.append(read)\n            if read_counter % self.read_buffer == 0 and not read_counter == 0:\n                self.outq.put(buffer.copy())\n                buffer.clear()\n                logger.info(f\"{read_counter} reads parsed (batch)\")\n                rc = read_counter\n            else:\n                rc = read_counter\n\n        self.outq.put(buffer)  # Deal with remainder\n        self.outq.put_nowait(None)  # Poison pill to terminate queue\n        logger.info(f\"{rc} reads parsed (final)\")\n\n    except Exception as e:\n        logger.info(f\"Reader failed with exception: {e}\")\n        raise\n\n    finally:\n        for fh in self._input_files_pysam:\n            fh.close()\n</code></pre>"},{"location":"reference/capcruncher/api/io/#capcruncher.api.io.bam_to_parquet","title":"<code>bam_to_parquet(bam, output)</code>","text":"<p>Converts bam file to parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>bam</code> <code>Union[str, Path]</code> <p>Path to bam file.</p> required <code>output</code> <code>Union[str, Path]</code> <p>Path to output parquet file.</p> required Source code in <code>capcruncher/api/io.py</code> <pre><code>def bam_to_parquet(\n    bam: Union[str, pathlib.Path], output: Union[str, pathlib.Path]\n) -&gt; Union[str, pathlib.Path]:\n    \"\"\"Converts bam file to parquet file.\n\n    Args:\n     bam: Path to bam file.\n     output: Path to output parquet file.\n\n    \"\"\"\n    df_bam = parse_bam(bam)\n    df_bam.to_parquet(output)\n\n    return output\n</code></pre>"},{"location":"reference/capcruncher/api/io/#capcruncher.api.io.parse_alignment","title":"<code>parse_alignment(aln)</code>","text":"<p>Parses reads from a bam file into a list.</p> Extracts <p>-read name -parent reads -flashed status -slice number -mapped status -multimapping status -chromosome number (e.g. chr10) -start (e.g. 1000) -end (e.g. 2000) -coords e.g. (chr10:1000-2000)</p> <p>Parameters:</p> Name Type Description Default <code>aln</code> <code>AlignmentFile</code> <p>pysam.AlignmentFile.</p> required <p>Returns:  list: Containing the attributes extracted.</p> Source code in <code>capcruncher/api/io.py</code> <pre><code>def parse_alignment(aln: pysam.AlignmentFile) -&gt; CCAlignment:\n    \"\"\"Parses reads from a bam file into a list.\n\n    Extracts:\n     -read name\n     -parent reads\n     -flashed status\n     -slice number\n     -mapped status\n     -multimapping status\n     -chromosome number (e.g. chr10)\n     -start (e.g. 1000)\n     -end (e.g. 2000)\n     -coords e.g. (chr10:1000-2000)\n\n\n    Args:\n     aln: pysam.AlignmentFile.\n    Returns:\n     list: Containing the attributes extracted.\n\n    \"\"\"\n\n    import numpy as np\n\n    slice_name = aln.query_name\n    parent_read, pe, slice_number, uid = slice_name.split(\"|\")\n    parent_id = xxhash.xxh3_64_intdigest(parent_read, seed=42)\n    slice_id = xxhash.xxh3_64_intdigest(slice_name, seed=42)\n    ref_name = aln.reference_name\n    ref_start = aln.reference_start\n    ref_end = aln.reference_end\n    # Check if read mapped\n    if aln.is_unmapped:\n        mapped = 0\n        multimapped = 0\n        ref_name = \"\"\n        ref_start = 0\n        ref_end = 0\n        coords = \"\"\n    else:\n        mapped = 1\n        coords = f\"{ref_name}:{ref_start}-{ref_end}\"\n        # Check if multimapped\n        if aln.is_secondary:\n            multimapped = 1\n        else:\n            multimapped = 0\n\n    return CCAlignment(\n        slice_id=slice_id,\n        slice_name=slice_name,\n        parent_id=parent_id,\n        parent_read=parent_read,\n        pe=pe.lower(),\n        slice=int(slice_number),\n        uid=int(uid),\n        mapped=mapped,\n        multimapped=multimapped,\n        chrom=ref_name,\n        start=int(ref_start),\n        end=int(ref_end),\n        coordinates=coords,\n    )\n</code></pre>"},{"location":"reference/capcruncher/api/io/#capcruncher.api.io.parse_bam","title":"<code>parse_bam(bam)</code>","text":"<p>Uses parse_alignment function convert bam file to a dataframe.</p> Extracts <p>-'slice_name' -'parent_read' -'pe' -'slice' -'mapped' -'multimapped' -'chrom' -'start' -'end' -'coordinates'</p> <p>Parameters:</p> Name Type Description Default <code>bam</code> <code>Union[str, Path]</code> <p>Path to bam file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: DataFrame with the columns listed above.</p> Source code in <code>capcruncher/api/io.py</code> <pre><code>def parse_bam(bam: Union[str, pathlib.Path]) -&gt; pd.DataFrame:\n    \"\"\"Uses parse_alignment function convert bam file to a dataframe.\n\n    Extracts:\n     -'slice_name'\n     -'parent_read'\n     -'pe'\n     -'slice'\n     -'mapped'\n     -'multimapped'\n     -'chrom'\n     -'start'\n     -'end'\n     -'coordinates'\n\n    Args:\n        bam: Path to bam file.\n\n    Returns:\n     pd.Dataframe: DataFrame with the columns listed above.\n\n    \"\"\"\n\n    import numpy as np\n\n    # Load reads into dataframe\n    logger.info(\"Parsing BAM file\")\n    df_bam = pd.DataFrame(\n        [\n            parse_alignment(aln)\n            for aln in pysam.AlignmentFile(bam, \"rb\").fetch(until_eof=True)\n        ],\n    )\n    df_bam[\"bam\"] = os.path.basename(bam)\n\n    # Perform dtype conversions\n    logger.info(\"Converting dtypes\")\n    df_bam[\"chrom\"] = df_bam[\"chrom\"].astype(\"category\")\n    pe_category = pd.CategoricalDtype([\"flashed\", \"pe\"])\n    df_bam[\"pe\"] = df_bam[\"pe\"].astype(\n        pe_category\n    )  # Only the one type present so need to include both\n    df_bam[\"coordinates\"] = df_bam[\"coordinates\"].astype(\"category\")\n    df_bam[\"parent_read\"] = df_bam[\"parent_read\"].astype(\"category\")\n    df_bam[\"slice\"] = df_bam[\"slice\"].astype(np.int8)\n    df_bam[\"uid\"] = df_bam[\"uid\"].astype(np.int8)\n    df_bam[\"multimapped\"] = df_bam[\"multimapped\"].astype(bool)\n    df_bam[\"mapped\"] = df_bam[\"mapped\"].astype(bool)\n    df_bam[\"bam\"] = df_bam[\"bam\"].astype(\"category\")\n\n    logger.info(\"Finished parsing BAM file\")\n    return df_bam\n</code></pre>"},{"location":"reference/capcruncher/api/pileup/","title":"pileup","text":""},{"location":"reference/capcruncher/api/pileup/#capcruncher.api.pileup.CoolerBedGraph","title":"<code>CoolerBedGraph</code>","text":"<p>Generates a bedgraph file from a cooler file created by interactions-store.</p> <p>Attributes:</p> Name Type Description <code>cooler</code> <code>Cooler</code> <p>Cooler file to use for bedgraph production</p> <code>capture_name</code> <code>str</code> <p>Name of capture probe being processed.</p> <code>sparse</code> <code>bool</code> <p>Only output bins with interactions.</p> <code>only_cis</code> <code>bool</code> <p>Only output cis interactions.</p> Source code in <code>capcruncher/api/pileup.py</code> <pre><code>class CoolerBedGraph:\n    \"\"\"Generates a bedgraph file from a cooler file created by interactions-store.\n\n    Attributes:\n     cooler (cooler.Cooler): Cooler file to use for bedgraph production\n     capture_name (str): Name of capture probe being processed.\n     sparse (bool): Only output bins with interactions.\n     only_cis (bool): Only output cis interactions.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        sparse: bool = True,\n        only_cis: bool = False,\n        region_to_limit: str = None,\n    ):\n        \"\"\"\n        Args:\n            uri (str): Path to cooler group in hdf5 file.\n            sparse (bool, optional): Only output non-zero bins. Defaults to True.\n        \"\"\"\n        self._sparse = sparse\n        self._only_cis = only_cis\n\n        logger.info(f\"Reading {uri}\")\n        self._cooler = cooler.Cooler(uri)\n        self.viewpoint_name = self._cooler.info[\"metadata\"][\"viewpoint_name\"]\n        self._viewpoint_bins = self._cooler.info[\"metadata\"][\"viewpoint_bins\"]\n\n        if len(self._viewpoint_bins) &gt; 1:\n            self.multiple_viewpoint_bins = True\n            logger.warning(\n                f\"Viewpoint {self.viewpoint_name} has multiple bins! {self._viewpoint_bins}. Proceed with caution!\"\n            )\n        else:\n            self.multiple_viewpoint_bins = False\n\n        self.viewpoint_chroms = self._cooler.info[\"metadata\"][\"viewpoint_chrom\"]\n        self.n_cis_interactions = self._cooler.info[\"metadata\"][\"n_cis_interactions\"]\n        logger.info(f\"Processing {self.viewpoint_name}\")\n\n        if only_cis:\n            pixels = []\n            bins = []\n            for chrom in self.viewpoint_chroms:\n                _bins = self._cooler.bins().fetch(chrom)\n                viewpoint_chrom_bins = self._bins[\"name\"]\n                _pixels = (\n                    self._cooler.pixels()\n                    .fetch(self.viewpoint_chroms)\n                    .query(\n                        \"(bin1_id in @viewpoint_chrom_bins) and (bin2_id in @viewpoint_chrom_bins)\"\n                    )\n                )\n                _bins = self._cooler.bins().fetch(chrom)\n\n                pixels.append(_pixels)\n                bins.append(_bins)\n\n            self._pixels = pd.concat(pixels)\n            self._bins = pd.concat(bins)\n\n        elif region_to_limit:\n            self._pixels = self._cooler.pixels().fetch(region_to_limit)\n            self._bins = self._cooler.bins().fetch(region_to_limit)\n\n        else:\n            self._pixels = self._cooler.pixels()[:]\n            # TODO: Avoid this if possible as reading all bins into memory\n            self._bins = self._cooler.bins()[:]\n\n        # Ensure name column is present\n        self._bins = (\n            self._bins.assign(name=lambda df: df.index)\n            if \"name\" not in self._bins.columns\n            else self._bins\n        )\n        self._reporters = None\n\n    def _get_reporters(self):\n        logger.info(\"Extracting reporters\")\n        concat_ids = pd.concat([self._pixels[\"bin1_id\"], self._pixels[\"bin2_id\"]])\n        concat_ids_filt = concat_ids.loc[lambda ser: ser.isin(self._viewpoint_bins)]\n        pixels = self._pixels.loc[concat_ids_filt.index]\n\n        df_interactions = pd.DataFrame()\n        df_interactions[\"capture\"] = np.where(\n            pixels[\"bin1_id\"].isin(self._viewpoint_bins),\n            pixels[\"bin1_id\"],\n            pixels[\"bin2_id\"],\n        )\n\n        df_interactions[\"reporter\"] = np.where(\n            pixels[\"bin1_id\"].isin(self._viewpoint_bins),\n            pixels[\"bin2_id\"],\n            pixels[\"bin1_id\"],\n        )\n\n        df_interactions[\"count\"] = pixels[\"count\"].values\n\n        return df_interactions.sort_values([\"capture\", \"reporter\"]).reset_index(\n            drop=True\n        )\n\n    def extract_bedgraph(\n        self, normalisation: Literal[\"raw\", \"n_cis\", \"region\"] = \"raw\", **norm_kwargs\n    ) -&gt; pd.DataFrame:\n        logger.info(\"Generating bedgraph\")\n        df_bdg = (\n            self._bins.merge(\n                self.reporters,\n                left_on=\"name\",\n                right_on=\"reporter\",\n                how=\"inner\" if self._sparse else \"outer\",\n            )[[\"chrom\", \"start\", \"end\", \"count\"]]\n            .assign(count=lambda df: df[\"count\"].fillna(0))\n            .sort_values([\"chrom\", \"start\"])\n        )\n\n        # TODO: This is a hack to deal with multiple bins for a viewpoint\n        if self.multiple_viewpoint_bins:\n            gr_bdg = pr.PyRanges(\n                df_bdg.rename(\n                    columns={\"chrom\": \"Chromosome\", \"start\": \"Start\", \"end\": \"End\"}\n                )\n            )\n\n            df_bdg = (\n                gr_bdg.cluster()\n                .df.groupby(\"Cluster\")\n                .agg(\n                    {\n                        \"count\": \"sum\",\n                        \"Start\": \"min\",\n                        \"End\": \"max\",\n                        \"Chromosome\": \"first\",\n                    }\n                )\n                .reset_index()\n                .rename(\n                    columns={\"Start\": \"start\", \"End\": \"end\", \"Chromosome\": \"chrom\"}\n                )[[\"chrom\", \"start\", \"end\", \"count\"]]\n            )\n\n        if not normalisation == \"raw\":\n            logger.info(\"Normalising bedgraph\")\n            self._normalise_bedgraph(df_bdg, method=normalisation, **norm_kwargs)\n\n        return df_bdg\n\n    @property\n    def reporters(self) -&gt; pd.DataFrame:\n        \"\"\"Interactions with capture fragments/bins.\n\n        Returns:\n         pd.DataFrame: DataFrame containing just bins interacting with the capture probe.\n        \"\"\"\n\n        if self._reporters is not None:\n            return self._reporters\n        else:\n            self._reporters = self._get_reporters()\n            return self._reporters\n\n    def _normalise_bedgraph(\n        self, bedgraph, scale_factor=1e6, method: str = \"n_cis\", region: str = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Normalises the bedgraph (in place).\n\n        Uses the number of cis interactions to normalise the bedgraph counts.\n\n        Args:\n         scale_factor (int, optional): Scaling factor for normalisation. Defaults to 1e6.\n\n        Returns:\n         pd.DataFrame: Normalised bedgraph formatted DataFrame\n        \"\"\"\n\n        if method == \"raw\":\n            pass\n        elif method == \"n_cis\":\n            self._normalise_by_n_cis(bedgraph, scale_factor)\n        elif method == \"region\":\n            self._normalise_by_regions(bedgraph, scale_factor, region)\n\n    def _normalise_by_n_cis(self, bedgraph, scale_factor: float):\n        bedgraph[\"count\"] = (bedgraph[\"count\"] / self.n_cis_interactions) * scale_factor\n\n    def _normalise_by_regions(self, bedgraph, scale_factor: float, regions: str):\n        if not is_valid_bed(regions):\n            raise ValueError(\n                \"A valid bed file is required for region based normalisation\"\n            )\n\n        df_viewpoint_norm_regions = pd.read_csv(\n            regions, sep=\"\\t\", names=[\"chrom\", \"start\", \"end\", \"name\"]\n        )\n        df_viewpoint_norm_regions = df_viewpoint_norm_regions.loc[\n            lambda df: df[\"name\"].str.contains(self.viewpoint_name)\n        ]\n\n        counts_in_regions = []\n        for region in df_viewpoint_norm_regions.itertuples():\n            counts_in_regions.append(\n                bedgraph.query(\n                    \"(chrom == @region.chrom) and (start &gt;= @region.start) and (start &lt;= @region.end)\"\n                )\n            )\n\n        df_counts_in_regions = pd.concat(counts_in_regions)\n        total_counts_in_region = df_counts_in_regions[\"count\"].sum()\n\n        bedgraph[\"count\"] = (bedgraph[\"count\"] / total_counts_in_region) * scale_factor\n\n    def to_pyranges(\n        self, normalisation: Literal[\"raw\", \"n_cis\", \"region\"] = \"raw\", **norm_kwargs\n    ):\n        return pr.PyRanges(\n            self.extract_bedgraph(\n                normalisation=normalisation, norm_kwargs=norm_kwargs\n            ).rename(columns={\"chrom\": \"Chromosome\", \"start\": \"Start\", \"end\": \"End\"})\n        )\n</code></pre>"},{"location":"reference/capcruncher/api/pileup/#capcruncher.api.pileup.CoolerBedGraph.reporters","title":"<code>reporters</code>  <code>property</code>","text":"<p>Interactions with capture fragments/bins.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing just bins interacting with the capture probe.</p>"},{"location":"reference/capcruncher/api/pileup/#capcruncher.api.pileup.CoolerBedGraph.__init__","title":"<code>__init__(uri, sparse=True, only_cis=False, region_to_limit=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Path to cooler group in hdf5 file.</p> required <code>sparse</code> <code>bool</code> <p>Only output non-zero bins. Defaults to True.</p> <code>True</code> Source code in <code>capcruncher/api/pileup.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    sparse: bool = True,\n    only_cis: bool = False,\n    region_to_limit: str = None,\n):\n    \"\"\"\n    Args:\n        uri (str): Path to cooler group in hdf5 file.\n        sparse (bool, optional): Only output non-zero bins. Defaults to True.\n    \"\"\"\n    self._sparse = sparse\n    self._only_cis = only_cis\n\n    logger.info(f\"Reading {uri}\")\n    self._cooler = cooler.Cooler(uri)\n    self.viewpoint_name = self._cooler.info[\"metadata\"][\"viewpoint_name\"]\n    self._viewpoint_bins = self._cooler.info[\"metadata\"][\"viewpoint_bins\"]\n\n    if len(self._viewpoint_bins) &gt; 1:\n        self.multiple_viewpoint_bins = True\n        logger.warning(\n            f\"Viewpoint {self.viewpoint_name} has multiple bins! {self._viewpoint_bins}. Proceed with caution!\"\n        )\n    else:\n        self.multiple_viewpoint_bins = False\n\n    self.viewpoint_chroms = self._cooler.info[\"metadata\"][\"viewpoint_chrom\"]\n    self.n_cis_interactions = self._cooler.info[\"metadata\"][\"n_cis_interactions\"]\n    logger.info(f\"Processing {self.viewpoint_name}\")\n\n    if only_cis:\n        pixels = []\n        bins = []\n        for chrom in self.viewpoint_chroms:\n            _bins = self._cooler.bins().fetch(chrom)\n            viewpoint_chrom_bins = self._bins[\"name\"]\n            _pixels = (\n                self._cooler.pixels()\n                .fetch(self.viewpoint_chroms)\n                .query(\n                    \"(bin1_id in @viewpoint_chrom_bins) and (bin2_id in @viewpoint_chrom_bins)\"\n                )\n            )\n            _bins = self._cooler.bins().fetch(chrom)\n\n            pixels.append(_pixels)\n            bins.append(_bins)\n\n        self._pixels = pd.concat(pixels)\n        self._bins = pd.concat(bins)\n\n    elif region_to_limit:\n        self._pixels = self._cooler.pixels().fetch(region_to_limit)\n        self._bins = self._cooler.bins().fetch(region_to_limit)\n\n    else:\n        self._pixels = self._cooler.pixels()[:]\n        # TODO: Avoid this if possible as reading all bins into memory\n        self._bins = self._cooler.bins()[:]\n\n    # Ensure name column is present\n    self._bins = (\n        self._bins.assign(name=lambda df: df.index)\n        if \"name\" not in self._bins.columns\n        else self._bins\n    )\n    self._reporters = None\n</code></pre>"},{"location":"reference/capcruncher/api/pileup/#capcruncher.api.pileup.CoolerBedGraph._normalise_bedgraph","title":"<code>_normalise_bedgraph(bedgraph, scale_factor=1000000.0, method='n_cis', region=None)</code>","text":"<p>Normalises the bedgraph (in place).</p> <p>Uses the number of cis interactions to normalise the bedgraph counts.</p> <p>Parameters:</p> Name Type Description Default <code>scale_factor</code> <code>int</code> <p>Scaling factor for normalisation. Defaults to 1e6.</p> <code>1000000.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Normalised bedgraph formatted DataFrame</p> Source code in <code>capcruncher/api/pileup.py</code> <pre><code>def _normalise_bedgraph(\n    self, bedgraph, scale_factor=1e6, method: str = \"n_cis\", region: str = None\n) -&gt; pd.DataFrame:\n    \"\"\"Normalises the bedgraph (in place).\n\n    Uses the number of cis interactions to normalise the bedgraph counts.\n\n    Args:\n     scale_factor (int, optional): Scaling factor for normalisation. Defaults to 1e6.\n\n    Returns:\n     pd.DataFrame: Normalised bedgraph formatted DataFrame\n    \"\"\"\n\n    if method == \"raw\":\n        pass\n    elif method == \"n_cis\":\n        self._normalise_by_n_cis(bedgraph, scale_factor)\n    elif method == \"region\":\n        self._normalise_by_regions(bedgraph, scale_factor, region)\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/","title":"plotting","text":""},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure","title":"<code>CCFigure</code>","text":"<p>Generates a figure from a list of tracks</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>List[CCTrack]</code> <p>List of tracks to plot. Defaults to None.</p> <code>None</code> <code>auto_spacing</code> <code>bool</code> <p>Automatically add a spacer track between each track. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments to pass to the figure</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>class CCFigure:\n    \"\"\"\n    Generates a figure from a list of tracks\n\n    Args:\n        tracks (List[CCTrack], optional): List of tracks to plot. Defaults to None.\n        auto_spacing (bool, optional): Automatically add a spacer track between each track. Defaults to False.\n        **kwargs: Additional arguments to pass to the figure\n    \"\"\"\n\n    def __init__(\n        self, tracks: List[CCTrack] = None, auto_spacing: bool = False, **kwargs\n    ) -&gt; None:\n        self.frame = cb.Frame()\n        self.auto_spacing = auto_spacing\n        self.properties = dict()\n        self.properties.update(kwargs)\n\n        if tracks:\n            self.tracks = set(tracks)\n            self.add_tracks(tracks)\n        else:\n            self.tracks = set()\n\n    def add_track(self, track: CCTrack) -&gt; None:\n        \"\"\"\n        Add a track to the figure\n\n        Args:\n            track (CCTrack): Track to add\n        \"\"\"\n        self.tracks.add(track)\n        self.frame.add_track(track.get_track())\n\n    def add_tracks(self, tracks: List[CCTrack]) -&gt; None:\n        \"\"\"\n        Add a list of tracks to the figure\n\n        Args:\n            tracks (List[CCTrack]): List of tracks to add\n        \"\"\"\n        for track in tracks:\n            if self.auto_spacing:\n                spacer = CCTrack(None, file_type=\"spacer\")\n                self.add_track(spacer.get_track())\n\n            self.add_track(track)\n\n    def plot(\n        self,\n        gr: Union[str, GenomeRange],\n        gr2: Union[str, GenomeRange] = None,\n        show: bool = True,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Plot the figure\n\n        Args:\n            gr (Union[str, GenomeRange]): GenomeRange to plot\n            gr2 (Union[str, GenomeRange], optional): Second GenomeRange to plot. Defaults to None.\n            show (bool, optional): Show the figure. Defaults to True.\n            **kwargs: Additional arguments to pass to the plot\n        \"\"\"\n\n        if gr2:\n            fig = self.frame.plot(gr, gr2, **kwargs)\n        else:\n            fig = self.frame.plot(gr, **kwargs)\n        if show:\n            fig.show()\n\n        return fig\n\n    def save(\n        self,\n        gr: Union[str, GenomeRange],\n        gr2: Union[str, GenomeRange] = None,\n        output: str = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Plots the figure and saves it to a file\n\n        Args:\n            gr (Union[str, GenomeRange]): GenomeRange to plot\n            gr2 (Union[str, GenomeRange], optional): Second GenomeRange to plot. Defaults to None.\n            output (str, optional): Path to save the figure to. Defaults to None.\n            **kwargs: Additional arguments to pass to the plot\n        \"\"\"\n\n        fig = self.plot(gr, gr2, show=False, **kwargs)\n        if output:\n            fig.savefig(output, dpi=300)\n        else:\n            fig.savefig(f\"{gr.chrom}_{gr.start}_{gr.end}.png\", dpi=300)\n\n    @classmethod\n    def from_toml(cls, toml_file: os.PathLike, **kwargs) -&gt; \"CCFigure\":\n        \"\"\"\n        Instantiate a CCFigure from a toml file\n\n        Args:\n            toml_file (os.PathLike): Path to toml file\n            **kwargs: Additional arguments to pass to the figure\n        \"\"\"\n        import toml\n\n        with open(toml_file) as f:\n            config = toml.load(f)\n\n        tracks = []\n        for track_name, attr in config.items():\n            file = attr.pop(\"file\") if attr.get(\"file\") else None\n            track_name = attr.pop(\"title\") if attr.get(\"title\") else track_name\n            tracks.append(CCTrack(file, title=track_name, **attr))\n        return cls(tracks, **kwargs)\n\n    @classmethod\n    def from_frame(cls, frame: cb.Frame, **kwargs) -&gt; \"CCFigure\":\n        \"\"\"\n        Instantiate a CCFigure from a coolbox Frame\n\n        Args:\n            frame (cb.Frame): coolbox Frame to instantiate from\n            **kwargs: Additional arguments to pass to the figure\n        \"\"\"\n        tracks = []\n        for track in frame.tracks:\n            tracks.append(CCTrack(track.properties[\"file\"], **track.properties))\n\n        return cls(tracks, **kwargs)\n\n    def to_toml(self, output: str = None) -&gt; Union[None, Dict[str, Any]]:\n        \"\"\"\n        Save the CCFigure to a toml file\n\n        Args:\n            output (str, optional): Path to save the toml file to. Defaults to None.\n\n        Returns:\n            Union[None, Dict[str, Any]]: If output is not specified, returns a dict of the toml file\n\n        \"\"\"\n\n        import toml\n        from collections import OrderedDict\n\n        def _get_n_tracks_of_type(config: Dict[str, Dict], track_type: str):\n            return sum(1 for t in config.keys() if track_type in t)\n\n        config = OrderedDict()\n        for track in self.tracks:\n            # Perform conversions for file-less tracks\n            if track.properties.get(\"type\") in [\"spacer\", \"scale\", \"xaxis\"]:\n                track_type = track.properties.get(\"type\")\n                n = _get_n_tracks_of_type(config, track_type)\n                config[f\"{track_type} {n}\"] = track.properties\n                config[f\"{track_type} {n}\"][\"file\"] = None\n            elif track.properties.get(\"type\") == \"genes\":\n                track_type = track.properties.get(\"type\")\n                n = _get_n_tracks_of_type(config, track_type)\n                config[f\"{track_type} {n}\"] = track.properties\n                config[f\"{track_type} {n}\"][\"file\"] = track.path\n            else:\n                config[track.properties[\"title\"]] = track.properties\n                config[track.properties[\"title\"]][\"file\"] = track.path\n\n        outfile = output if output else \"config.toml\"\n\n        with open(outfile, \"w\") as f:\n            config_str = toml.dumps(config)\n            f.write(config_str)\n\n        if not output:\n            return config\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.add_track","title":"<code>add_track(track)</code>","text":"<p>Add a track to the figure</p> <p>Parameters:</p> Name Type Description Default <code>track</code> <code>CCTrack</code> <p>Track to add</p> required Source code in <code>capcruncher/api/plotting.py</code> <pre><code>def add_track(self, track: CCTrack) -&gt; None:\n    \"\"\"\n    Add a track to the figure\n\n    Args:\n        track (CCTrack): Track to add\n    \"\"\"\n    self.tracks.add(track)\n    self.frame.add_track(track.get_track())\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.add_tracks","title":"<code>add_tracks(tracks)</code>","text":"<p>Add a list of tracks to the figure</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>List[CCTrack]</code> <p>List of tracks to add</p> required Source code in <code>capcruncher/api/plotting.py</code> <pre><code>def add_tracks(self, tracks: List[CCTrack]) -&gt; None:\n    \"\"\"\n    Add a list of tracks to the figure\n\n    Args:\n        tracks (List[CCTrack]): List of tracks to add\n    \"\"\"\n    for track in tracks:\n        if self.auto_spacing:\n            spacer = CCTrack(None, file_type=\"spacer\")\n            self.add_track(spacer.get_track())\n\n        self.add_track(track)\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.from_frame","title":"<code>from_frame(frame, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instantiate a CCFigure from a coolbox Frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Frame</code> <p>coolbox Frame to instantiate from</p> required <code>**kwargs</code> <p>Additional arguments to pass to the figure</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>@classmethod\ndef from_frame(cls, frame: cb.Frame, **kwargs) -&gt; \"CCFigure\":\n    \"\"\"\n    Instantiate a CCFigure from a coolbox Frame\n\n    Args:\n        frame (cb.Frame): coolbox Frame to instantiate from\n        **kwargs: Additional arguments to pass to the figure\n    \"\"\"\n    tracks = []\n    for track in frame.tracks:\n        tracks.append(CCTrack(track.properties[\"file\"], **track.properties))\n\n    return cls(tracks, **kwargs)\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.from_toml","title":"<code>from_toml(toml_file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instantiate a CCFigure from a toml file</p> <p>Parameters:</p> Name Type Description Default <code>toml_file</code> <code>PathLike</code> <p>Path to toml file</p> required <code>**kwargs</code> <p>Additional arguments to pass to the figure</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>@classmethod\ndef from_toml(cls, toml_file: os.PathLike, **kwargs) -&gt; \"CCFigure\":\n    \"\"\"\n    Instantiate a CCFigure from a toml file\n\n    Args:\n        toml_file (os.PathLike): Path to toml file\n        **kwargs: Additional arguments to pass to the figure\n    \"\"\"\n    import toml\n\n    with open(toml_file) as f:\n        config = toml.load(f)\n\n    tracks = []\n    for track_name, attr in config.items():\n        file = attr.pop(\"file\") if attr.get(\"file\") else None\n        track_name = attr.pop(\"title\") if attr.get(\"title\") else track_name\n        tracks.append(CCTrack(file, title=track_name, **attr))\n    return cls(tracks, **kwargs)\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.plot","title":"<code>plot(gr, gr2=None, show=True, **kwargs)</code>","text":"<p>Plot the figure</p> <p>Parameters:</p> Name Type Description Default <code>gr</code> <code>Union[str, GenomeRange]</code> <p>GenomeRange to plot</p> required <code>gr2</code> <code>Union[str, GenomeRange]</code> <p>Second GenomeRange to plot. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Show the figure. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments to pass to the plot</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>def plot(\n    self,\n    gr: Union[str, GenomeRange],\n    gr2: Union[str, GenomeRange] = None,\n    show: bool = True,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Plot the figure\n\n    Args:\n        gr (Union[str, GenomeRange]): GenomeRange to plot\n        gr2 (Union[str, GenomeRange], optional): Second GenomeRange to plot. Defaults to None.\n        show (bool, optional): Show the figure. Defaults to True.\n        **kwargs: Additional arguments to pass to the plot\n    \"\"\"\n\n    if gr2:\n        fig = self.frame.plot(gr, gr2, **kwargs)\n    else:\n        fig = self.frame.plot(gr, **kwargs)\n    if show:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.save","title":"<code>save(gr, gr2=None, output=None, **kwargs)</code>","text":"<p>Plots the figure and saves it to a file</p> <p>Parameters:</p> Name Type Description Default <code>gr</code> <code>Union[str, GenomeRange]</code> <p>GenomeRange to plot</p> required <code>gr2</code> <code>Union[str, GenomeRange]</code> <p>Second GenomeRange to plot. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Path to save the figure to. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the plot</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>def save(\n    self,\n    gr: Union[str, GenomeRange],\n    gr2: Union[str, GenomeRange] = None,\n    output: str = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Plots the figure and saves it to a file\n\n    Args:\n        gr (Union[str, GenomeRange]): GenomeRange to plot\n        gr2 (Union[str, GenomeRange], optional): Second GenomeRange to plot. Defaults to None.\n        output (str, optional): Path to save the figure to. Defaults to None.\n        **kwargs: Additional arguments to pass to the plot\n    \"\"\"\n\n    fig = self.plot(gr, gr2, show=False, **kwargs)\n    if output:\n        fig.savefig(output, dpi=300)\n    else:\n        fig.savefig(f\"{gr.chrom}_{gr.start}_{gr.end}.png\", dpi=300)\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCFigure.to_toml","title":"<code>to_toml(output=None)</code>","text":"<p>Save the CCFigure to a toml file</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Path to save the toml file to. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[None, Dict[str, Any]]</code> <p>Union[None, Dict[str, Any]]: If output is not specified, returns a dict of the toml file</p> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>def to_toml(self, output: str = None) -&gt; Union[None, Dict[str, Any]]:\n    \"\"\"\n    Save the CCFigure to a toml file\n\n    Args:\n        output (str, optional): Path to save the toml file to. Defaults to None.\n\n    Returns:\n        Union[None, Dict[str, Any]]: If output is not specified, returns a dict of the toml file\n\n    \"\"\"\n\n    import toml\n    from collections import OrderedDict\n\n    def _get_n_tracks_of_type(config: Dict[str, Dict], track_type: str):\n        return sum(1 for t in config.keys() if track_type in t)\n\n    config = OrderedDict()\n    for track in self.tracks:\n        # Perform conversions for file-less tracks\n        if track.properties.get(\"type\") in [\"spacer\", \"scale\", \"xaxis\"]:\n            track_type = track.properties.get(\"type\")\n            n = _get_n_tracks_of_type(config, track_type)\n            config[f\"{track_type} {n}\"] = track.properties\n            config[f\"{track_type} {n}\"][\"file\"] = None\n        elif track.properties.get(\"type\") == \"genes\":\n            track_type = track.properties.get(\"type\")\n            n = _get_n_tracks_of_type(config, track_type)\n            config[f\"{track_type} {n}\"] = track.properties\n            config[f\"{track_type} {n}\"][\"file\"] = track.path\n        else:\n            config[track.properties[\"title\"]] = track.properties\n            config[track.properties[\"title\"]][\"file\"] = track.path\n\n    outfile = output if output else \"config.toml\"\n\n    with open(outfile, \"w\") as f:\n        config_str = toml.dumps(config)\n        f.write(config_str)\n\n    if not output:\n        return config\n</code></pre>"},{"location":"reference/capcruncher/api/plotting/#capcruncher.api.plotting.CCTrack","title":"<code>CCTrack</code>","text":"<p>Provides a wrapper around tracks to provide a consistent interface</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>Path to file to plot</p> required <code>file_type</code> <code>Literal['heatmap', 'bigwig', 'bigwig_summary', 'scale', 'bed', 'xaxis', 'genes', 'spacer']</code> <p>Type of file to plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the track</p> <code>{}</code> Source code in <code>capcruncher/api/plotting.py</code> <pre><code>class CCTrack:\n    \"\"\"\n    Provides a wrapper around tracks to provide a consistent interface\n\n    Args:\n        file (os.PathLike): Path to file to plot\n        file_type (Literal[\"heatmap\", \"bigwig\", \"bigwig_summary\", \"scale\", \"bed\", \"xaxis\", \"genes\", \"spacer\"], optional): Type of file to plot. Defaults to None.\n        **kwargs: Additional arguments to pass to the track\n    \"\"\"\n\n    def __init__(\n        self,\n        file,\n        file_type: Literal[\n            \"heatmap\",\n            \"heatmap_summary\",\n            \"bigwig\",\n            \"bigwig_summary\",\n            \"scale\",\n            \"bed\",\n            \"xaxis\",\n            \"genes\",\n            \"spacer\",\n        ] = None,\n        **kwargs,\n    ):\n        self.file = file\n        self.properties = dict()\n        self.properties.update(kwargs)\n\n        if file_type:\n            self.properties[\"type\"] = file_type\n        elif self.properties.get(\"type\"):\n            pass\n        else:\n            raise ValueError(\n                \"Please specify file_type as one of: heatmap, bigwig, bigwig_summary, scale, bed, xaxis, genes, spacer\"\n            )\n\n    def get_track(self):\n        match self.properties.get(\"type\"):  # noqa\n            case \"heatmap\":\n                assert (\n                    \"binsize\" in self.properties\n                ), \"Binsize must be specified for heatmap track (e.g. binsize=5000)\"\n                return CCMatrix(self.file, **self.properties)\n            case \"heatmap_summary\":\n                assert (\n                    \"binsize\" in self.properties\n                ), \"Binsize must be specified for heatmap track (e.g. binsize=5000)\"\n\n                matricies = []\n                for matrix in self.file:\n                    matricies.append(CCMatrix(matrix, **self.properties))\n\n                return CCAverageMatrix(matricies, **self.properties)\n            case \"bigwig\":\n                if self.properties.get(\"overlay\"):\n                    return cb.BigWigCoverage(self.file, **self.properties)\n                else:\n                    return CCBigWig(self.file, **self.properties)\n            case \"bigwig_summary\":\n                return CCBigWigCollection(self.file, **self.properties)\n            case \"scale\":\n                return ScaleBar(**self.properties)\n            case \"bed\":\n                return CCSimpleBed(self.file, **self.properties)\n            case \"xaxis\":\n                return CCXAxisGenomic(**self.properties)\n            case \"genes\":\n                if self.properties.get(\"title\"):\n                    del self.properties[\"title\"]\n                return cb.BED(self.file, **self.properties)\n            case \"spacer\":\n                return cb.Spacer(**self.properties)\n            case _:\n                if getattr(cb, self.properties.get(\"type\")):\n                    return getattr(cb, self.properties.get(\"type\"))(\n                        self.file, **self.properties\n                    )\n\n                else:\n                    raise ValueError(\n                        f\"Unknown track type {self.properties.get('type')}, select from: heatmap, bigwig, bigwig_summary, scale, bed, xaxis, genes, spacer\"\n                    )\n\n    @property\n    def path(self) -&gt; str:\n        if isinstance(self.file, (list, tuple, np.ndarray)):\n            return [str(pathlib.Path(f).resolve()) for f in self.file]\n        else:\n            return str(pathlib.Path(self.file).resolve())\n\n    def __repr__(self) -&gt; str:\n        return f\"CCTrack({self.properties.get('title')}, {self.properties.get('type')})\"\n</code></pre>"},{"location":"reference/capcruncher/api/statistics/","title":"statistics","text":""},{"location":"reference/capcruncher/api/statistics/#capcruncher.api.statistics.FastqDeduplicationStatistics","title":"<code>FastqDeduplicationStatistics</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Statistics for Fastq deduplication</p> Source code in <code>capcruncher/api/statistics.py</code> <pre><code>class FastqDeduplicationStatistics(BaseModel):\n    \"\"\"Statistics for Fastq deduplication\"\"\"\n\n    sample: str = \"unknown_sample\"\n    total: int\n    duplicates: int\n\n    @computed_field\n    @property\n    def percentage(self) -&gt; float:\n        return self.duplicates / self.total * 100\n\n    @computed_field\n    @property\n    def unique(self) -&gt; int:\n        return self.total - self.duplicates\n</code></pre>"},{"location":"reference/capcruncher/api/statistics/#capcruncher.api.statistics.FastqTrimmingStatistics","title":"<code>FastqTrimmingStatistics</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Statistics for Fastq trimming</p> Source code in <code>capcruncher/api/statistics.py</code> <pre><code>class FastqTrimmingStatistics(BaseModel):\n    \"\"\"Statistics for Fastq trimming\"\"\"\n\n    sample: str = \"unknown_sample\"\n    read_number: Union[int, float]\n    reads_input: int\n    reads_output: int\n    reads_with_adapter_identified: int\n\n    @computed_field\n    @property\n    def percentage_trimmed(self) -&gt; float:\n        return self.reads_with_adapter_identified / self.reads_input * 100\n\n    @computed_field\n    @property\n    def percentage_passing_quality_filter(self) -&gt; float:\n        return self.reads_output / self.reads_input * 100\n\n    @classmethod\n    def from_multiqc_entry(cls, entry: pd.Series) -&gt; \"FastqTrimmingStatistics\":\n        return cls(\n            sample=entry[\"sample\"],\n            read_number=entry[\"read_number\"],\n            reads_input=entry[\"r_processed\"],\n            reads_output=entry[\"r_written\"],\n            reads_with_adapter_identified=entry[\"r_with_adapters\"],\n        )\n\n    def __add__(self, other: \"FastqTrimmingStatistics\"):\n        return FastqTrimmingStatistics(\n            sample=self.sample,\n            reads_input=self.reads_input + other.reads_input,\n            reads_output=self.reads_output + other.reads_output,\n            reads_with_adapter_identified=self.reads_with_adapter_identified\n            + other.reads_with_adapter_identified,\n        )\n</code></pre>"},{"location":"reference/capcruncher/api/storage/","title":"storage","text":""},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.CoolerBinner","title":"<code>CoolerBinner</code>","text":"Source code in <code>capcruncher/api/storage.py</code> <pre><code>class CoolerBinner:\n    def __init__(\n        self,\n        cooler_group: os.PathLike,\n        binsize: int = None,\n        method: Union[Literal[\"overlap\"], Literal[\"midpoint\"]] = \"midpoint\",\n        minimum_overlap: float = 0.51,\n        n_cis_interaction_correction: bool = True,\n        n_rf_per_bin_correction: bool = True,\n        scale_factor: int = 1_000_000,\n        assay: Literal[\"capture\", \"tri\", \"tiled\"] = \"capture\",\n    ) -&gt; None:\n        self.cooler_group = cooler_group\n        self.binsize = binsize\n        self.method = method\n        self.minimum_overlap = minimum_overlap\n\n        if isinstance(cooler_group, str):\n            self.cooler = cooler.Cooler(cooler_group)\n        elif isinstance(cooler_group, cooler.Cooler):\n            self.cooler = cooler_group\n        else:\n            raise ValueError(\n                \"cooler_group must be a path to a cooler file or a cooler object\"\n            )\n\n        self.n_cis_interactions = self.cooler.info[\"metadata\"][\"n_cis_interactions\"]\n        self.n_cis_interaction_correction = n_cis_interaction_correction\n        self.n_restriction_fragment_correction = n_rf_per_bin_correction\n        self.scale_factor = scale_factor\n        self.assay = assay\n\n    @functools.cached_property\n    def genomic_bins(self) -&gt; pr.PyRanges:\n        return (\n            cooler.binnify(binsize=self.binsize, chromsizes=self.cooler.chromsizes)\n            .sort_values(by=[\"chrom\", \"start\", \"end\"])\n            .assign(\n                genomic_bin_id=lambda df: df.reset_index(drop=True)\n                .index.to_series()\n                .values\n            )\n            .rename(columns={\"chrom\": \"Chromosome\", \"start\": \"Start\", \"end\": \"End\"})\n            .pipe(pr.PyRanges)\n        )\n\n    @functools.cached_property\n    def fragment_bins(self):\n        return (\n            self.cooler.bins()[:]\n            .rename(\n                columns={\n                    \"chrom\": \"Chromosome\",\n                    \"start\": \"Start\",\n                    \"end\": \"End\",\n                    \"name\": \"fragment_id\",\n                }\n            )\n            .pipe(pr.PyRanges)\n        )\n\n    @functools.cached_property\n    def fragment_to_genomic_table(self) -&gt; pr.PyRanges:\n        \"\"\"\n        Translate genomic bins to fragment bins\n        \"\"\"\n\n        fragment_bins = self.fragment_bins\n\n        if self.method == \"midpoint\":\n            fragment_bins = (\n                fragment_bins.as_df()\n                .assign(\n                    Start=lambda df: df[\"Start\"] + (df[\"End\"] - df[\"Start\"]) / 2,\n                    End=lambda df: df[\"Start\"] + 1,\n                )\n                .pipe(pr.PyRanges)\n            )\n\n        pr_fragment_to_bins = self.genomic_bins.join(\n            fragment_bins, strandedness=0, how=None, report_overlap=True\n        )\n\n        if self.method == \"overlap\":\n            pr_fragment_to_bins = pr_fragment_to_bins[\n                pr_fragment_to_bins[\"Overlap\"] &gt;= self.minimum_overlap\n            ]\n\n        # Add number of fragments per bin\n        pr_fragment_to_bins = pr_fragment_to_bins.assign(\n            \"n_fragments_per_bin\",\n            lambda df: df.groupby(\"genomic_bin_id\")[\"fragment_id\"].transform(\"nunique\"),\n        )\n\n        return pr_fragment_to_bins\n\n    @functools.cached_property\n    def fragment_to_genomic_mapping(self) -&gt; Dict[int, int]:\n        \"\"\"\n        Translate genomic bins to fragment bins\n        \"\"\"\n        fragment_to_bins_mapping = (\n            self.fragment_to_genomic_table.as_df()\n            .set_index(\"fragment_id\")[\"genomic_bin_id\"]\n            .to_dict()\n        )\n        return fragment_to_bins_mapping\n\n    @functools.cached_property\n    def pixels(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Translate fragment pixels to genomic pixels\n        \"\"\"\n\n        fragment_to_bins_mapping = self.fragment_to_genomic_mapping\n\n        pixels = self.cooler.pixels()[:].assign(\n            genomic_bin1_id=lambda df: df[\"bin1_id\"].map(fragment_to_bins_mapping),\n            genomic_bin2_id=lambda df: df[\"bin2_id\"].map(fragment_to_bins_mapping),\n        )\n\n        # Sum the counts of pixels that map to the same genomic bins\n        pixels = (\n            pixels.groupby([\"genomic_bin1_id\", \"genomic_bin2_id\"])\n            .agg(\n                count=(\"count\", \"sum\"),\n            )\n            .reset_index()\n        )\n\n        # Normalize pixels if specified\n        if self.n_restriction_fragment_correction:\n            n_fragments_per_bin = (\n                self.fragment_to_genomic_table.as_df()\n                .set_index(\"genomic_bin_id\")[\"n_fragments_per_bin\"]\n                .to_dict()\n            )\n            pixels = pixels.assign(\n                n_fragments_per_bin1=lambda df: df[\"genomic_bin1_id\"].map(\n                    n_fragments_per_bin\n                ),\n                n_fragments_per_bin2=lambda df: df[\"genomic_bin2_id\"].map(\n                    n_fragments_per_bin\n                ),\n                n_fragments_per_bin_correction=lambda df: (\n                    df[\"n_fragments_per_bin1\"] + df[\"n_fragments_per_bin2\"]\n                ),\n                count_n_rf_norm=lambda df: df[\"count\"]\n                / df[\"n_fragments_per_bin_correction\"],\n            )\n\n        if self.n_cis_interaction_correction:\n            pixels = pixels.assign(\n                count_n_cis_norm=lambda df: (df[\"count\"] / self.n_cis_interactions)\n                * self.scale_factor,\n            )\n\n        if self.n_cis_interaction_correction and self.n_restriction_fragment_correction:\n            pixels = pixels.assign(\n                count_n_cis_rf_norm=lambda df: (\n                    pixels[\"count_n_rf_norm\"] / self.n_cis_interactions\n                )\n                * self.scale_factor\n            )\n\n        return pixels\n\n    @functools.cached_property\n    def viewpoint_bins(self) -&gt; List[int]:\n        \"\"\"\n        Return list of viewpoint bins\n        \"\"\"\n\n        pr_viewpoint = pr.from_dict(\n            dict(\n                zip(\n                    [\"Chromosome\", \"Start\", \"End\"],\n                    [\n                        [\n                            x,\n                        ]\n                        for x in re.split(\n                            \":|-\", self.cooler.info[\"metadata\"][\"viewpoint_coords\"][0]\n                        )\n                    ],\n                )\n            )\n        )\n\n        return pr_viewpoint.join(self.genomic_bins).df[\"genomic_bin_id\"].to_list()\n\n    def to_cooler(self, store: os.PathLike):\n        metadata = {**self.cooler.info[\"metadata\"]}\n        metadata[\"viewpoint_bins\"] = [int(x) for x in self.viewpoint_bins]\n        metadata[\"n_interactions_total\"] = int(self.cooler.pixels()[:][\"count\"].sum())\n        cooler_fn = f\"{store}::/{metadata['viewpoint_name']}/resolutions/{self.binsize}\"\n\n        pixels = (\n            self.pixels.drop(\n                columns=[\n                    \"bin1_id\",\n                    \"bin2_id\",\n                    \"n_fragments_per_bin1\",\n                    \"n_fragments_per_bin2\",\n                    \"n_fragments_per_bin_correction\",\n                ],\n                errors=\"ignore\",\n            )\n            .rename(\n                columns={\"genomic_bin1_id\": \"bin1_id\", \"genomic_bin2_id\": \"bin2_id\"}\n            )\n            .loc[:, lambda df: [\"bin1_id\", \"bin2_id\", \"count\", *df.columns[3:]]]\n            .sort_values(by=[\"bin1_id\", \"bin2_id\"])\n        )\n\n        bins = (\n            self.genomic_bins.df.rename(\n                columns={\"Chromosome\": \"chrom\", \"Start\": \"start\", \"End\": \"end\"}\n            )\n            .sort_values(\"genomic_bin_id\")\n            .assign(bin_id=lambda df: df[\"genomic_bin_id\"])\n            .set_index(\"genomic_bin_id\")\n        )\n\n        cooler.create_cooler(\n            cooler_fn,\n            bins=bins,\n            pixels=pixels,\n            metadata=metadata,\n            mode=\"w\" if not os.path.exists(store) else \"a\",\n            columns=pixels.columns[2:],\n            dtypes=dict(zip(pixels.columns[2:], [\"float32\"] * len(pixels.columns[2:]))),\n            ensure_sorted=True,\n            ordered=True,\n        )\n\n        return cooler_fn\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.CoolerBinner.fragment_to_genomic_mapping","title":"<code>fragment_to_genomic_mapping</code>  <code>cached</code> <code>property</code>","text":"<p>Translate genomic bins to fragment bins</p>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.CoolerBinner.fragment_to_genomic_table","title":"<code>fragment_to_genomic_table</code>  <code>cached</code> <code>property</code>","text":"<p>Translate genomic bins to fragment bins</p>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.CoolerBinner.pixels","title":"<code>pixels</code>  <code>cached</code> <code>property</code>","text":"<p>Translate fragment pixels to genomic pixels</p>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.CoolerBinner.viewpoint_bins","title":"<code>viewpoint_bins</code>  <code>cached</code> <code>property</code>","text":"<p>Return list of viewpoint bins</p>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.Viewpoint","title":"<code>Viewpoint</code>","text":"Source code in <code>capcruncher/api/storage.py</code> <pre><code>class Viewpoint:\n    def __init__(\n        self, coordinates: pr.PyRanges, assay: Literal[\"capture\", \"tri\", \"tiled\"]\n    ) -&gt; None:\n        self.coordinates = coordinates\n        self.assay = assay\n\n    @classmethod\n    def from_bed(\n        cls, bed: str, viewpoint: str, assay: Literal[\"capture\", \"tri\", \"tiled\"]\n    ):\n        \"\"\"\n        Creates a viewpoint object from a bed file.\n\n        Args:\n            bed (str): Path to bed file containing viewpoint coordinates.\n            viewpoint (str): Name of viewpoint to extract from bed file.\n\n        Raises:\n            IndexError: Oligo name cannot be found within viewpoints.\n\n        Returns:\n            Viewpoint: Viewpoint object.\n        \"\"\"\n        gr_viewpoints = pr.read_bed(bed)\n        df_viewpoints = gr_viewpoints.as_df()\n\n        df_viewpoints = df_viewpoints.loc[\n            lambda df: df[\"Name\"].str.contains(f\"{viewpoint}$\")\n        ]\n\n        if df_viewpoints.empty:\n            raise IndexError(\n                f\"Oligo name cannot be found within viewpoints: {viewpoint}\"\n            )\n\n        return Viewpoint(df_viewpoints.pipe(pr.PyRanges), assay=assay)\n\n    def bins(self, bins: pr.PyRanges):\n        \"\"\"\n        Returns the bins that overlap with the viewpoint.\n\n        Args:\n            bins (pr.PyRanges): PyRanges object containing all bins.\n\n        Returns:\n            pr.PyRanges: PyRanges object containing all bins that overlap with the viewpoint.\n        \"\"\"\n        return bins.join(self.coordinates)\n\n    def bin_names(self, bins: pr.PyRanges) -&gt; List[int]:\n        return self.bins(bins).df[\"Name\"].astype(int).to_list()\n\n    def bins_cis(self, bins: pr.PyRanges) -&gt; List[int]:\n        \"\"\"\n        Returns the bins that are on the same chromosome(s) as the viewpoint.\n\n        Args:\n            bins (pr.PyRanges): PyRanges object containing all bins.\n\n        Returns:\n            List[int]: List of bin names.\n        \"\"\"\n\n        # Get the chromosomes of the viewpoint\n        viewpoint_chromosomes = self.chromosomes\n\n        # Get the bins that are on the same chromosome(s) as the viewpoint\n        df_cis_bins = bins.df.loc[\n            lambda df: df[\"Chromosome\"].isin(viewpoint_chromosomes)\n        ]\n\n        # If capture or tri, remove viewpoint bins from cis bins\n        if self.assay == \"capture\" or self.assay == \"tri\":\n            df_cis_bins = df_cis_bins.loc[\n                lambda df: ~df[\"Name\"].isin(self.bin_names(bins))\n            ]\n\n        return df_cis_bins[\"Name\"].to_list()\n\n    @property\n    def chromosomes(self) -&gt; List[str]:\n        return self.coordinates.df[\"Chromosome\"].unique().tolist()\n\n    @property\n    def coords(self) -&gt; List[str]:\n        \"\"\"\n        Returns the genomic coordinates of the viewpoint.\n\n        Returns:\n            List[str]: List of genomic coordinates.\n        \"\"\"\n        _coords = []\n        for row in self.coordinates.df.itertuples():\n            _coords.append(f\"{row.Chromosome}:{row.Start}-{row.End}\")\n\n        return _coords\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.Viewpoint.coords","title":"<code>coords</code>  <code>property</code>","text":"<p>Returns the genomic coordinates of the viewpoint.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of genomic coordinates.</p>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.Viewpoint.bins","title":"<code>bins(bins)</code>","text":"<p>Returns the bins that overlap with the viewpoint.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>PyRanges</code> <p>PyRanges object containing all bins.</p> required <p>Returns:</p> Type Description <p>pr.PyRanges: PyRanges object containing all bins that overlap with the viewpoint.</p> Source code in <code>capcruncher/api/storage.py</code> <pre><code>def bins(self, bins: pr.PyRanges):\n    \"\"\"\n    Returns the bins that overlap with the viewpoint.\n\n    Args:\n        bins (pr.PyRanges): PyRanges object containing all bins.\n\n    Returns:\n        pr.PyRanges: PyRanges object containing all bins that overlap with the viewpoint.\n    \"\"\"\n    return bins.join(self.coordinates)\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.Viewpoint.bins_cis","title":"<code>bins_cis(bins)</code>","text":"<p>Returns the bins that are on the same chromosome(s) as the viewpoint.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>PyRanges</code> <p>PyRanges object containing all bins.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of bin names.</p> Source code in <code>capcruncher/api/storage.py</code> <pre><code>def bins_cis(self, bins: pr.PyRanges) -&gt; List[int]:\n    \"\"\"\n    Returns the bins that are on the same chromosome(s) as the viewpoint.\n\n    Args:\n        bins (pr.PyRanges): PyRanges object containing all bins.\n\n    Returns:\n        List[int]: List of bin names.\n    \"\"\"\n\n    # Get the chromosomes of the viewpoint\n    viewpoint_chromosomes = self.chromosomes\n\n    # Get the bins that are on the same chromosome(s) as the viewpoint\n    df_cis_bins = bins.df.loc[\n        lambda df: df[\"Chromosome\"].isin(viewpoint_chromosomes)\n    ]\n\n    # If capture or tri, remove viewpoint bins from cis bins\n    if self.assay == \"capture\" or self.assay == \"tri\":\n        df_cis_bins = df_cis_bins.loc[\n            lambda df: ~df[\"Name\"].isin(self.bin_names(bins))\n        ]\n\n    return df_cis_bins[\"Name\"].to_list()\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.Viewpoint.from_bed","title":"<code>from_bed(bed, viewpoint, assay)</code>  <code>classmethod</code>","text":"<p>Creates a viewpoint object from a bed file.</p> <p>Parameters:</p> Name Type Description Default <code>bed</code> <code>str</code> <p>Path to bed file containing viewpoint coordinates.</p> required <code>viewpoint</code> <code>str</code> <p>Name of viewpoint to extract from bed file.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>Oligo name cannot be found within viewpoints.</p> <p>Returns:</p> Name Type Description <code>Viewpoint</code> <p>Viewpoint object.</p> Source code in <code>capcruncher/api/storage.py</code> <pre><code>@classmethod\ndef from_bed(\n    cls, bed: str, viewpoint: str, assay: Literal[\"capture\", \"tri\", \"tiled\"]\n):\n    \"\"\"\n    Creates a viewpoint object from a bed file.\n\n    Args:\n        bed (str): Path to bed file containing viewpoint coordinates.\n        viewpoint (str): Name of viewpoint to extract from bed file.\n\n    Raises:\n        IndexError: Oligo name cannot be found within viewpoints.\n\n    Returns:\n        Viewpoint: Viewpoint object.\n    \"\"\"\n    gr_viewpoints = pr.read_bed(bed)\n    df_viewpoints = gr_viewpoints.as_df()\n\n    df_viewpoints = df_viewpoints.loc[\n        lambda df: df[\"Name\"].str.contains(f\"{viewpoint}$\")\n    ]\n\n    if df_viewpoints.empty:\n        raise IndexError(\n            f\"Oligo name cannot be found within viewpoints: {viewpoint}\"\n        )\n\n    return Viewpoint(df_viewpoints.pipe(pr.PyRanges), assay=assay)\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.create_cooler_cc","title":"<code>create_cooler_cc(output_prefix, bins, pixels, viewpoint_name, viewpoint_path, assay='capture', suffix=None, **cooler_kwargs)</code>","text":"<p>Creates a cooler hdf5 file or cooler formatted group within a hdf5 file.</p> <p>Parameters:</p> Name Type Description Default <code>output_prefix</code> <code>str</code> <p>Output path for hdf5 file. If this already exists, will append a new group to the file.</p> required <code>bins</code> <code>DataFrame</code> <p>DataFrame containing the genomic coordinates of all bins in the pixels table.</p> required <code>pixels</code> <code>DataFrame</code> <p>DataFrame with columns: bin1_id, bin2_id, count.</p> required <code>viewpoint_name</code> <code>str</code> <p>Name of viewpoint to store.</p> required <code>viewpoint_path</code> <code>PathLike</code> <p>Path to viewpoints used for the analysis.</p> required <code>suffix</code> <code>str</code> <p>Suffix to append before the .hdf5 file extension. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Viewpoint name must exactly match the a supplied viewpoint.</p> <p>Returns:</p> Type Description <code>PathLike</code> <p>os.PathLike: Path of cooler hdf5 file.</p> Source code in <code>capcruncher/api/storage.py</code> <pre><code>def create_cooler_cc(\n    output_prefix: str,\n    bins: pd.DataFrame,\n    pixels: pd.DataFrame,\n    viewpoint_name: str,\n    viewpoint_path: os.PathLike,\n    assay: Literal[\"capture\", \"tri\", \"tiled\"] = \"capture\",\n    suffix=None,\n    **cooler_kwargs,\n) -&gt; os.PathLike:\n    \"\"\"\n    Creates a cooler hdf5 file or cooler formatted group within a hdf5 file.\n\n    Args:\n     output_prefix (str): Output path for hdf5 file. If this already exists, will append a new group to the file.\n     bins (pd.DataFrame): DataFrame containing the genomic coordinates of all bins in the pixels table.\n     pixels (pd.DataFrame): DataFrame with columns: bin1_id, bin2_id, count.\n     viewpoint_name (str): Name of viewpoint to store.\n     viewpoint_path (os.PathLike): Path to viewpoints used for the analysis.\n     suffix (str, optional): Suffix to append before the .hdf5 file extension. Defaults to None.\n\n    Raises:\n     ValueError: Viewpoint name must exactly match the a supplied viewpoint.\n\n    Returns:\n     os.PathLike: Path of cooler hdf5 file.\n    \"\"\"\n\n    viewpoint = Viewpoint.from_bed(\n        bed=viewpoint_path, viewpoint=viewpoint_name, assay=assay\n    )\n\n    gr_bins = pr.PyRanges(\n        bins.rename(\n            columns={\n                \"chrom\": \"Chromosome\",\n                \"start\": \"Start\",\n                \"end\": \"End\",\n                \"name\": \"Name\",\n            }\n        )\n    )\n\n    # Get cis bins\n    bins_cis = viewpoint.bins_cis(gr_bins)\n\n    # Get cis pixels\n    pixels_cis = pixels.loc[\n        lambda df: (df[\"bin1_id\"].isin(bins_cis)) | (df[\"bin2_id\"].isin(bins_cis))\n    ]\n\n    # Metadata for cooler file.\n    metadata = {\n        \"viewpoint_bins\": viewpoint.bin_names(gr_bins),\n        \"viewpoint_name\": viewpoint_name,\n        \"viewpoint_chrom\": viewpoint.chromosomes,\n        \"viewpoint_coords\": viewpoint.coords,\n        \"n_cis_interactions\": int(pixels_cis[\"count\"].sum()),\n        \"n_total_interactions\": int(pixels[\"count\"].sum()),\n    }\n\n    if os.path.exists(\n        output_prefix\n    ):  # Will append to a prexisting file if one is supplied\n        append_to_file = True\n        cooler_fn = f\"{output_prefix}::/{viewpoint_name}\"\n    else:\n        append_to_file = False\n        cooler_fn = f\"{output_prefix.replace('.hdf5', '')}{'.' + suffix if suffix else ''}.hdf5::/{viewpoint_name}\"\n\n    cooler.create_cooler(\n        cooler_fn,\n        bins=bins,\n        pixels=pixels,\n        metadata=metadata,\n        mode=\"w\" if not append_to_file else \"a\",\n        **cooler_kwargs,\n    )\n\n    return cooler_fn\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.get_merged_cooler_metadata","title":"<code>get_merged_cooler_metadata(coolers)</code>","text":"<p>Merges metadata from multiple coolers.</p> Source code in <code>capcruncher/api/storage.py</code> <pre><code>def get_merged_cooler_metadata(coolers: Iterable[os.PathLike]):\n    \"\"\"\n    Merges metadata from multiple coolers.\n    \"\"\"\n    # Get metadata from all coolers and copy to the merged file\n    metadata = {}\n    for cooler_uri in coolers:\n        filepath, group = cooler_uri.split(\"::\")\n\n        with h5py.File(filepath, mode=\"r\") as src:\n            metadata_src = ujson.decode(src[group].attrs[\"metadata\"])\n\n            for metadata_key, metadata_value in metadata_src.items():\n                if isinstance(metadata_value, str):\n                    metadata[metadata_key] = metadata_value\n\n                elif isinstance(metadata_value, Iterable):\n                    if metadata_key not in metadata:\n                        metadata[metadata_key] = []\n                        metadata[metadata_key].extend(metadata_value)\n                    else:\n                        metadata[metadata_key].extend(\n                            [\n                                v\n                                for v in metadata_value\n                                if v not in metadata[metadata_key]\n                            ]\n                        )\n\n                elif isinstance(metadata_value, (int, float)):\n                    if metadata_key not in metadata:\n                        metadata[metadata_key] = metadata_value\n                    else:\n                        metadata[metadata_key] += metadata_value\n\n    return metadata\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.link_common_cooler_tables","title":"<code>link_common_cooler_tables(clr)</code>","text":"<p>Reduces cooler storage space by linking \"bins\" table.</p> <p>All of the cooler \"bins\" tables containing the genomic coordinates of each bin  are identical for all cooler files of the same resoultion. As cooler.create_cooler  generates a new bins table for each cooler, this leads to a high degree of duplication.</p> <p>This function hard links the bins tables for a given resolution to reduce the degree of duplication.</p> <p>Parameters:</p> Name Type Description Default <code>clr</code> <code>PathLike</code> <p>Path to cooler hdf5 produced by the merge command.</p> required Source code in <code>capcruncher/api/storage.py</code> <pre><code>def link_common_cooler_tables(clr: os.PathLike):\n    \"\"\"Reduces cooler storage space by linking \"bins\" table.\n\n     All of the cooler \"bins\" tables containing the genomic coordinates of each bin\n     are identical for all cooler files of the same resoultion. As cooler.create_cooler\n     generates a new bins table for each cooler, this leads to a high degree of duplication.\n\n     This function hard links the bins tables for a given resolution to reduce the degree of duplication.\n\n    Args:\n     clr (os.PathLike): Path to cooler hdf5 produced by the merge command.\n    \"\"\"\n\n    logger.info(\"Making links to common cooler tables to conserve disk space\")\n\n    with h5py.File(clr, \"a\") as f:\n        # Get all viewpoints stored\n        viewpoints = sorted(list(f.keys()))\n\n        # Get all resolutions stored\n        try:\n            resolutions = [res for res in f[viewpoints[0]][\"resolutions\"]]\n        except (KeyError, IndexError):\n            resolutions = None\n\n        for viewpoint in viewpoints[1:]:\n            try:\n                # Delete currenly stored bins group and replace with link to first viewpoint \"bins\" group\n                del f[viewpoint][\"bins\"]\n                f[viewpoint][\"bins\"] = f[viewpoints[0]][\"bins\"]\n\n                # Delete chroms table and replace with link to the first \"chroms\" group\n                del f[viewpoint][\"chroms\"]\n                f[viewpoint][\"chroms\"] = f[viewpoints[0]][\"chroms\"]\n            except KeyError:\n                pass\n\n            # Repeat for resolutions i.e. binned coolers\n            if resolutions:\n                for resolution in resolutions:\n                    del f[viewpoint][\"resolutions\"][resolution][\"bins\"]\n                    f[viewpoint][\"resolutions\"][resolution][\"bins\"] = f[viewpoints[0]][\n                        \"resolutions\"\n                    ][resolution][\"bins\"]\n\n                    del f[viewpoint][\"resolutions\"][resolution][\"chroms\"]\n                    f[viewpoint][\"resolutions\"][resolution][\"chroms\"] = f[\n                        viewpoints[0]\n                    ][\"resolutions\"][resolution][\"chroms\"]\n</code></pre>"},{"location":"reference/capcruncher/api/storage/#capcruncher.api.storage.merge_coolers","title":"<code>merge_coolers(coolers, output)</code>","text":"<p>Merges capcruncher cooler files together.</p> <p>Produces a unified cooler with both restriction fragment and genomic bins whilst reducing the storage space required by hard linking the \"bins\" tables to prevent duplication.</p> <p>Parameters:</p> Name Type Description Default <code>coolers</code> <code>Tuple</code> <p>Cooler files produced by either the fragments or bins subcommands.</p> required <code>output</code> <code>PathLike</code> <p>Path from merged cooler file.</p> required Source code in <code>capcruncher/api/storage.py</code> <pre><code>def merge_coolers(coolers: Tuple, output: os.PathLike):\n    \"\"\"\n    Merges capcruncher cooler files together.\n\n    Produces a unified cooler with both restriction fragment and genomic bins whilst\n    reducing the storage space required by hard linking the \"bins\" tables to prevent duplication.\n\n    Args:\n     coolers (Tuple): Cooler files produced by either the fragments or bins subcommands.\n     output (os.PathLike): Path from merged cooler file.\n    \"\"\"\n    from collections import defaultdict\n    import cooler\n\n    logger.info(\"Merging cooler files\")\n\n    coolers_to_merge = defaultdict(list)\n\n    # Remove output file as need to append to it.\n    if os.path.exists(output):\n        os.unlink(output)\n\n    # Extract a list of coolers to merge, grouped by viewpoint name\n    for clr in coolers:\n        with h5py.File(clr, mode=\"r\") as src:\n            viewpoints = list(src.keys())\n\n            for viewpoint in viewpoints:\n                if \"resolutions\" not in list(src[viewpoint].keys()):\n                    coolers_to_merge[viewpoint].append(f\"{clr}::/{viewpoint}\")\n                else:\n                    for resolution in src[viewpoint][\"resolutions\"].keys():\n                        coolers_to_merge[f\"{viewpoint}::{resolution}\"].append(\n                            f\"{clr}::/{viewpoint}/resolutions/{resolution}\"\n                        )\n\n    # Initial pass to perform copying for all coolers without a matching group\n    need_merging = list()\n    with h5py.File(output, mode=\"w\") as dest:\n        for ii, (viewpoint, cooler_uris) in enumerate(coolers_to_merge.items()):\n            if len(cooler_uris) &lt; 2:  # Only merge if two or more, else just copy\n                (file_path, group_path) = cooler_uris[0].split(\"::\")\n\n                with h5py.File(file_path, mode=\"r\") as src:\n                    src.copy(src[group_path], dest, group_path)\n\n            else:\n                need_merging.append(viewpoint)\n\n    # Actually merge the coolers left over that do have duplicates\n    for viewpoint in need_merging:\n        tmp = tempfile.NamedTemporaryFile().name\n        cooler_uris = coolers_to_merge[viewpoint]\n        cooler.merge_coolers(\n            f\"{tmp}::/{viewpoint.replace('::', '/resolutions/')}\",\n            cooler_uris,\n            mergebuf=int(1e6),\n        )\n\n        with h5py.File(tmp, mode=\"r\") as src:\n            with h5py.File(output, mode=\"a\") as dest:\n                dest.copy(\n                    src[viewpoint.replace(\"::\", \"/resolutions/\")], dest, viewpoint\n                )\n\n        metadata = get_merged_cooler_metadata(cooler_uris)\n\n        with h5py.File(output, mode=\"a\") as dest:\n            dest[viewpoint.replace(\"::\", \"/resolutions/\")].attrs[\n                \"metadata\"\n            ] = ujson.encode(metadata)\n\n    # Reduce space by linking common tables (bins, chroms)\n    link_common_cooler_tables(output)\n</code></pre>"}]}